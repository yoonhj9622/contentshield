"""
SNS Content Analyzer - Groq Dual Model Edition (MVP)
Llama-Guard-4-12b (ÌïÑÌÑ∞ÎßÅ) + Llama-3.1-8b-instant (Î∂ÑÏÑù)

‚úÖ v3.1.0 Î≥ÄÍ≤Ω
- scam/spam Ï†úÍ±∞ (MVP Î≤îÏúÑÏóêÏÑú Ï†úÏô∏)
- Ïπ¥ÌÖåÍ≥†Î¶¨ Ïû¨Ï†ïÏùò
- AI Î∂ÑÏÑù ÏùòÍ≤¨ÏùÑ ÏÇ¨Îûå ÎßêÌà¨ Ïû•Î¨∏ÏúºÎ°ú ÌõÑÏ≤òÎ¶¨ ÏÉùÏÑ±
"""

from fastapi import FastAPI, HTTPException, Body, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import logging
import os
from datetime import datetime
import httpx
import json
import re
import asyncio
import urllib.parse

from dotenv import load_dotenv


# ‚úÖ WindowsÏóêÏÑú .env Ïù∏ÏΩîÎî© Ïù¥Ïäà ÎåÄÎπÑ
def _safe_load_dotenv():
    env_path = os.path.join(os.path.dirname(__file__), ".env")
    for enc in ("utf-8", "utf-8-sig", "utf-16", "utf-16-le", "cp949"):
        try:
            load_dotenv(dotenv_path=env_path, override=False, encoding=enc)
            return
        except UnicodeDecodeError:
            continue
        except Exception:
            continue


_safe_load_dotenv()

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# FastAPI Ïï± ÏÉùÏÑ±
app = FastAPI(
    title="SNS Content Analyzer - Groq Dual Model (MVP)",
    description="Llama Guard 4 + Llama 3.1 ÎìÄÏñº Î™®Îç∏ ÏïÖÏÑ± ÏΩòÌÖêÏ∏† ÌÉêÏßÄ (Ïä§Ìå∏/ÏÇ¨Í∏∞ Ï†úÏô∏)",
    version="3.1.0",
)

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ ====================

class TextAnalysisRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=10000)
    language: str = Field(default="auto", description="ko/en ÎòêÎäî auto(ÏûêÎèô Í∞êÏßÄ)")  # ‚úÖ ko/enÎßå
    use_dual_model: bool = Field(default=True, description="Îëê Î™®Îç∏ Î™®Îëê ÏÇ¨Ïö© Ïó¨Î∂Ä")


class AnalysisResponse(BaseModel):
    is_malicious: bool
    toxicity_score: float
    hate_speech_score: float
    profanity_score: float
    threat_score: float
    violence_score: float
    sexual_score: float

    confidence_score: float
    category: str
    detected_keywords: List[str]

    # Guard Î™®Îç∏ Í≤∞Í≥º
    guard_result: Optional[Dict[str, Any]] = None
    guard_categories: List[str] = []

    # Llama 3.1 Í≤∞Í≥º (ÏõêÎ¨∏ reasoning ÎåÄÏã†, ÏÑúÎ≤ÑÏóêÏÑú Ïû¨ÏûëÏÑ±Ìïú Ïû•Î¨∏ ÌîºÎìúÎ∞±)
    llama_reasoning: Optional[str] = None

    ai_model_version: str
    processing_time_ms: float
    analyzed_at: str


# ==================== Groq Dual Model Analyzer ====================

class GroqDualModelAnalyzer:
    """Llama Guard 4 + Llama 3.1 ÎìÄÏñº Î™®Îç∏ Î∂ÑÏÑù ÏÑúÎπÑÏä§ (MVP: scam/spam Ï†úÏô∏)"""

    def __init__(self):
        self.model_version = "groq-dual-llama-guard4-llama3.1-mvp-v3.1.0"
        self.api_key = os.getenv("GROQ_API_KEY")
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"

        if not self.api_key:
            logger.warning("‚ö†Ô∏è GROQ_API_KEY not set")
            logger.warning("  Get FREE API key: https://console.groq.com/keys")
        else:
            logger.info("‚úì Groq API key configured")

        self.models = {
            "guard": "meta-llama/llama-guard-4-12b",
            "analysis": "llama-3.1-8b-instant",
        }

        # Guard Ïπ¥ÌÖåÍ≥†Î¶¨ Îß§Ìïë
        self.guard_categories = {
            "S1": "violent_crimes",
            "S2": "non_violent_crimes",
            "S3": "sex_related_crimes",
            "S4": "child_exploitation",
            "S5": "defamation",
            "S6": "specialized_advice",
            "S7": "privacy",
            "S8": "intellectual_property",
            "S9": "indiscriminate_weapons",
            "S10": "hate",
            "S11": "self_harm",
            "S12": "sexual_content",
            "S13": "elections",
        }

        # Í∑úÏπô Í∏∞Î∞ò Ï∞®Îã® Îã®Ïñ¥(ÏµúÏÜå ÏòàÏãú) - ko/enÎßå
        self.blocked_words = {
            "ko": ["Î∞îÎ≥¥", "Î©çÏ≤≠Ïù¥", "Î≥ëÏã†", "Í∞úÏÉàÎÅº", "Ïî®Î∞ú", "ÏßÄÎûÑ", "ÎØ∏Ïπú", "Í∫ºÏ†∏", "Îã•Ï≥ê", "„ÖÖ„ÖÇ", "„ÖÇ„ÖÖ"],
            "en": ["stupid", "idiot", "fuck", "shit", "hate", "damn"],
        }

        # ÎèôÏùº ÏûÖÎ†• Í≤∞Í≥º Í≥†Ï†ï Ï∫êÏãú
        self._result_cache: Dict[str, Dict[str, Any]] = {}
        self._cache_max_items: int = 300

        logger.info("Groq Dual Model Analyzer initialized")
        logger.info(f"  - Guard Model: {self.models['guard']}")
        logger.info(f"  - Analysis Model: {self.models['analysis']}")

    # ==================== Ïñ∏Ïñ¥ Í∞êÏßÄ ====================
    def _detect_language_simple(self, text: str) -> str:
        """‚úÖ ko/enÎßå Í∞êÏßÄ"""
        if not text:
            return "en"
        if re.search(r"[Í∞Ä-Ìû£]", text):
            return "ko"
        return "en"

    # ==================== Ï∫êÏãú ÌÇ§ ====================
    def _normalize_text_for_cache(self, text: str) -> str:
        if not text:
            return ""
        t = text.strip()
        t = re.sub(r"\s+", " ", t)
        return t

    def _make_cache_key(self, text: str, language: str, use_dual_model: bool) -> str:
        import hashlib

        norm = self._normalize_text_for_cache(text)
        raw = f"{language}||{str(use_dual_model)}||{norm}".encode("utf-8", errors="ignore")
        return hashlib.sha256(raw).hexdigest()

    # ==================== ÎßàÏä§ÌÇπ ====================
    def _mask_text_for_api(self, text: str, language: str) -> str:
        """
        ‚úÖ Groq Ï†ïÏ±Ö/ÌïÑÌÑ∞Ïóê Í±∏Î†§ APIÍ∞Ä ÏùëÎãµ ÏûêÏ≤¥Î•º Ïã§Ìå®ÌïòÎäî Í≤ΩÏö∞ Ïö∞Ìöå Î™©Ï†Å
        - blocked_wordsÎßå [MASK] Ï≤òÎ¶¨
        """
        try:
            words = self.blocked_words.get(language, [])
            if not words:
                return text

            masked = text
            for w in words:
                if not w:
                    continue
                pattern = re.compile(re.escape(w), flags=re.IGNORECASE)
                masked = pattern.sub("[MASK]", masked)

            return masked
        except Exception:
            return text

    # ==================== MVP Ïπ¥ÌÖåÍ≥†Î¶¨ Ïû¨Ï†ïÏùò ====================
    def _decide_category_mvp(
        self,
        toxicity: float,
        threat: float,
        violence: float,
        hate: float,
        sexual: float,
        protected_group: bool,
        guard_violated: List[str],
    ) -> str:
        """
        ‚úÖ MVP Î∞úÌëúÏö© Ïπ¥ÌÖåÍ≥†Î¶¨(Ïä§Ìå∏/ÏÇ¨Í∏∞ Ï†úÏô∏)
        - safe / toxic / hate_speech / threat / violence / sexual_content
        + (ÏòµÏÖò) guardÍ∞Ä Î™ÖÌôïÌûà Ï∞çÏùÄ Í≤ΩÏö∞ defamation/privacyÎèÑ Î≥¥Ïó¨Ï£ºÍ≥† Ïã∂ÏúºÎ©¥ Ïó¨Í∏∞ÏÑú Ï∂îÍ∞Ä Í∞ÄÎä•
        """
        # Guard Í∏∞Î∞ò Î≥¥Ï†ï
        if "defamation" in guard_violated:
            return "defamation"
        if "privacy" in guard_violated:
            return "privacy"

        if threat > 45:
            return "threat"
        if violence > 65:
            return "violence"
        if sexual > 75:
            return "sexual_content"
        if hate > 65 and protected_group:
            return "hate_speech"
        if toxicity > 55:
            return "toxic"
        return "safe"

    # ==================== Ïû•Î¨∏ ÌîºÎìúÎ∞± ÏÉùÏÑ±(ÏÑúÎ≤Ñ ÌõÑÏ≤òÎ¶¨) ====================
    def _make_reasoning_longform(
        self,
        language: str,
        category: str,
        text: str,
        detected_keywords: List[str],
        toxicity: float,
        threat: float,
        violence: float,
        hate: float,
        sexual: float,
        protected_group: bool,
        guard_violated: List[str],
    ) -> str:
        """
        ‚úÖ Ïà´Ïûê ÎÇòÏó¥ ÎåÄÏã†, ÏÇ¨Îûå ÎßêÌà¨ Ïû•Î¨∏ ÏÑ§Î™Ö
        ‚úÖ Î∞úÌëú/ÏãúÏó∞ÏóêÏÑú ‚ÄúÏßÑÏßú ÏÑúÎπÑÏä§Ï≤òÎüº Î≥¥Ïù¥Îäî‚Äù ÌîºÎìúÎ∞± ÌÖçÏä§Ìä∏
        """

        def level_ko(x: float) -> str:
            if x >= 75:
                return "ÎÜíÏùÄ Ìé∏"
            if x >= 55:
                return "ÍΩ§ ÏûàÎäî Ìé∏"
            if x >= 35:
                return "ÏïΩÍ∞Ñ ÏûàÎäî Ìé∏"
            return "ÌÅ¨ÏßÄ ÏïäÏùÄ Ìé∏"

        def level_en(x: float) -> str:
            if x >= 75:
                return "high"
            if x >= 55:
                return "moderate"
            if x >= 35:
                return "some"
            return "low"

        # Í∞ÑÎã®Ìïú ‚ÄúÎ∞îÍøîÏì∞Í∏∞‚Äù ÏòàÏãú ÏÉùÏÑ±(LLM ÏóÜÏù¥ ÌÖúÌîåÎ¶ø)
        def rewrite_ko(cat: str) -> List[str]:
            base = []
            if cat == "toxic":
                base = [
                    "ÌëúÌòÑÏù¥ Îã§ÏÜå Í≥µÍ≤©Ï†ÅÏúºÎ°ú Îì§Î¶¥ Ïàò ÏûàÏñ¥Ïöî. ‚ÄòÎÇòÎäî ~ÎùºÍ≥† ÎäêÍºàÎã§‚ÄôÏ≤òÎüº Í∞êÏ†ï/ÏÇ¨Ïã§ Ï§ëÏã¨ÏúºÎ°ú Î∞îÍøîÎ≥¥Î©¥ Ï¢ãÏäµÎãàÎã§.",
                    "ÏÉÅÎåÄÎ•º ÌèâÍ∞ÄÌïòÎäî Î¨∏Ïû• ÎåÄÏã†, Íµ¨Ï≤¥Ï†ÅÏù∏ ÌñâÎèô/ÏÉÅÌô©ÏùÑ ÏßöÏñ¥ ÎßêÌïòÎ©¥ Í∞àÎì±Ïù¥ Ï§ÑÏñ¥Îì≠ÎãàÎã§.",
                ]
            elif cat == "hate_speech":
                base = [
                    "ÌäπÏ†ï ÏßëÎã®/Ï†ïÏ≤¥ÏÑ±ÏùÑ ÏùºÎ∞òÌôîÌïòÎäî ÌëúÌòÑÏùÄ Ïò§Ìï¥ÏôÄ Ï∞®Î≥ÑÎ°ú Ïù¥Ïñ¥Ïßà Ïàò ÏûàÏñ¥Ïöî. ÎåÄÏÉÅ ÏßÄÏπ≠ÏùÑ ÎπºÍ≥† ‚ÄòÌï¥Îãπ ÌñâÎèô‚ÄôÏóêÎßå Ï¥àÏ†êÏùÑ ÎßûÏ∂∞Î≥¥ÏÑ∏Ïöî.",
                    "‚Äò~ÏùÄ Îã§ Í∑∏Î†áÎã§‚Äô Í∞ôÏùÄ ÏùºÎ∞òÌôî ÎåÄÏã†, ‚ÄòÏùºÎ∂Ä ÏÇ¨Î°ÄÏóêÏÑú Ïù¥Îü∞ Î¨∏Ï†úÍ∞Ä ÏûàÏóàÎã§‚ÄôÏ≤òÎüº Î≤îÏúÑÎ•º Ï¢ÅÌòÄ ÎßêÌïòÎäî Í≤å ÏïàÏ†ÑÌï©ÎãàÎã§.",
                ]
            elif cat == "threat":
                base = [
                    "ÏÉÅÎåÄÍ∞Ä ‚ÄòÌòëÎ∞ï‚ÄôÏúºÎ°ú ÎäêÎÇÑ Ïàò ÏûàÎäî Î¨∏Íµ¨Îäî ÌîºÌïòÎäî Í≤å Ï¢ãÏïÑÏöî. ÏöîÍµ¨/Í≤ΩÍ≥†Í∞Ä ÌïÑÏöîÌïòÎã§Î©¥ Í∑úÏ†ï/Ï†àÏ∞® ÏïàÎÇ¥ ÌòïÌÉúÎ°ú Î∞îÍæ∏Îäî Í±∏ Ï∂îÏ≤úÌï©ÎãàÎã§.",
                    "ÏßÅÏ†ëÏ†Å Ï†úÏû¨ Ïñ∏Í∏âÎ≥¥Îã® ‚ÄòÌïÑÏöîÌïòÎ©¥ Ïã†Í≥†/Ï∞®Îã®ÏùÑ Í≤ÄÌÜ†ÌïòÍ≤†Îã§‚ÄôÏ≤òÎüº Ï§ëÎ¶ΩÏ†ÅÏúºÎ°ú Ï†ïÎ¶¨Ìï¥Î≥¥ÏÑ∏Ïöî.",
                ]
            elif cat == "violence":
                base = [
                    "Ìè≠Î†•Ï†Å ÌëúÌòÑÏùÄ Í≥ºÏû•/ÎπÑÏú†ÎùºÎèÑ Ïò§Ìï¥ ÏÜåÏßÄÍ∞Ä Ïª§Ïöî. Í∞êÏ†ï ÌëúÌòÑÏùÄ ‚ÄòÌôîÍ∞Ä ÎÇ¨Îã§/Î∂àÏæåÌïòÎã§‚ÄôÏ≤òÎüº ÎπÑÌè≠Î†•Ï†ÅÏúºÎ°ú Î∞îÍæ∏Îäî Í≤å ÏïàÏ†ÑÌï©ÎãàÎã§.",
                ]
            elif cat == "sexual_content":
                base = [
                    "ÏÑ±Ï†Å ÎâòÏïôÏä§Îäî Îß•ÎùΩÏóê Îî∞Îùº Î∂àÏæåÍ∞êÏùÑ Ï§Ñ Ïàò ÏûàÏñ¥Ïöî. ÎÖ∏Í≥®Ï†Å/ÏïîÏãúÏ†Å ÌëúÌòÑÏùÄ Ï§ÑÏù¥Í≥† Ï†ïÎ≥¥ Ï†ÑÎã¨ ÏúÑÏ£ºÎ°ú Î∞îÍøîÎ≥¥ÏÑ∏Ïöî.",
                ]
            elif cat == "defamation":
                base = [
                    "ÌäπÏ†ï Í∞úÏù∏/Îã®Ï≤¥Î•º Îã®Ï†ïÏ†ÅÏúºÎ°ú ÎπÑÎÇúÌïòÎ©¥ Î™ÖÏòàÌõºÏÜê Ïù¥ÏäàÍ∞Ä Îê† Ïàò ÏûàÏñ¥Ïöî. ÏÇ¨Ïã§ ÌôïÏù∏ Ï†ÑÏóêÎäî ‚ÄòÏùòÏã¨ÎêúÎã§/Ï∂îÏ†ïÎêúÎã§‚Äô Í∞ôÏùÄ ÌëúÌòÑÏù¥ ÏïàÏ†ÑÌï©ÎãàÎã§.",
                    "Ïã§Î™Ö/Íµ¨Ï≤¥ Ï†ïÎ≥¥Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÎã§Î©¥ Í∞ÄÎ¶¨Îäî Í≤ÉÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.",
                ]
            elif cat == "privacy":
                base = [
                    "Í∞úÏù∏Ï†ïÎ≥¥(Ïó∞ÎùΩÏ≤ò/Ï£ºÏÜå/Ïã§Î™Ö Îì±) ÎÖ∏Ï∂úÏùÄ ÏúÑÌóòÌï¥Ïöî. Ìï¥Îãπ Ï†ïÎ≥¥Îäî ÏÇ≠Ï†úÌïòÍ±∞ÎÇò ÏùµÎ™Ö Ï≤òÎ¶¨ÌïòÎäî Í≤å ÏïàÏ†ÑÌï©ÎãàÎã§.",
                ]
            else:
                base = [
                    "ÌòÑÏû¨ Î¨∏Ïû•ÏùÄ ÌÅ∞ ÏúÑÌóò Ïã†Ìò∏Îäî ÌÅ¨ÏßÄ ÏïäÏïÑ Î≥¥ÏûÖÎãàÎã§. Îã§Îßå ÎØºÍ∞êÌïú Ï£ºÏ†úÎùºÎ©¥ Ï§ëÎ¶ΩÏ†ÅÏù∏ ÌëúÌòÑÏùÑ Ïú†ÏßÄÌïòÎ©¥ Îçî ÏïàÏ†ÑÌï©ÎãàÎã§."
                ]
            return base

        def rewrite_en(cat: str) -> List[str]:
            base = []
            if cat == "toxic":
                base = [
                    "This may come across as hostile. Consider switching to an 'I feel / I think' framing and focus on the specific behavior.",
                    "Avoid labeling the person; describe the situation and what you'd like to change.",
                ]
            elif cat == "hate_speech":
                base = [
                    "Generalizations about an identity/protected group can be perceived as discriminatory. Remove group labels and focus on the behavior.",
                    "Use narrower scope language (e.g., 'in some cases') instead of blanket statements.",
                ]
            elif cat == "threat":
                base = [
                    "This could be perceived as a threat. If you need to enforce rules, phrase it as a neutral policy/procedure notice.",
                    "Consider a calm escalation path (report/block) instead of intimidation.",
                ]
            elif cat == "violence":
                base = [
                    "Violent wording can be risky even as a metaphor. Use non-violent emotional descriptions instead.",
                ]
            elif cat == "sexual_content":
                base = [
                    "Sexual implications can cause discomfort depending on context. Keep wording informational and avoid suggestive phrasing.",
                ]
            elif cat == "defamation":
                base = [
                    "Definitive accusations can lead to defamation risk. Use cautious language and avoid sharing identifying details.",
                ]
            elif cat == "privacy":
                base = [
                    "Personal data exposure is risky. Remove or anonymize any identifying information.",
                ]
            else:
                base = [
                    "No strong risk signals detected. Keeping a neutral tone will make it safer in sensitive discussions."
                ]
            return base

        # ÌïµÏã¨ Ïã†Ìò∏ Î¨∏Ïû•Ìôî(Ïà´Ïûê ÎåÄÏã† ÎäêÎÇå Î†àÎ≤®)
        if language == "ko":
            intro = f"Î∂ÑÏÑù Í≤∞Í≥º, Ïù¥ Î¨∏Ïû•ÏùÄ **{category}** ÏÑ±Í≤©ÏúºÎ°ú Î∂ÑÎ•òÎêòÏóàÏäµÎãàÎã§."
            context = []

            # Î≤îÏ£ºÎ≥Ñ ÏÑ§Î™Ö (MVPÏö©)
            if category == "safe":
                context.append("Ï†ÑÏ≤¥Ï†ÅÏúºÎ°ú Í≥µÍ≤©Ï†ÅÏù¥Í±∞ÎÇò ÏúÑÌòëÏ†ÅÏù∏ ÌëúÌòÑÏù¥ Í∞ïÌïòÏßÄ ÏïäÏïÑ, ÏùºÎ∞òÏ†ÅÏù∏ ÎåÄÌôî Îß•ÎùΩÏóêÏÑúÎäî ÌÅ∞ Î¨∏Ï†úÎ°ú Î≥¥Ïù¥ÏßÄ ÏïäÏäµÎãàÎã§.")
                context.append("Îã§Îßå ÏÉÅÎåÄÍ∞Ä ÏòàÎØºÌïòÍ≤å Î∞õÏïÑÎì§Ïùº Ïàò ÏûàÎäî ÌëúÌòÑÏù¥ ÏûàÎã§Î©¥, Ï°∞Í∏à Îçî Ï§ëÎ¶ΩÏ†ÅÏúºÎ°ú Î∞îÍæ∏Î©¥ ÏïàÏ†ÑÌï©ÎãàÎã§.")
            elif category == "toxic":
                context.append("Î¨∏Ïû•Ïóê ÏÉÅÎåÄÎ•º ÌèâÍ∞ÄÌïòÍ±∞ÎÇò ÍπéÏïÑÎÇ¥Î¶¨Îäî ÎâòÏïôÏä§Í∞Ä Ìè¨Ìï®Îê† Ïàò ÏûàÏñ¥Ïöî.")
                context.append("ÌäπÌûà ÎπÑÍºº/Ï°∞Î°±/Îã®Ï†ïÏ†ÅÏù∏ ÌëúÌòÑÏùÄ ÏùΩÎäî ÏÇ¨ÎûåÏóêÍ≤å Í≥µÍ≤©Ï†ÅÏúºÎ°ú Ï†ÑÎã¨ÎêòÍ∏∞ Ïâ¨Ïõå Í∞àÎì±Ïù¥ Ïª§Ïßà Í∞ÄÎä•ÏÑ±Ïù¥ ÏûàÏäµÎãàÎã§.")
            elif category == "hate_speech":
                context.append("ÌäπÏ†ï ÏßëÎã®(Ï†ïÏ≤¥ÏÑ±/Î≥¥Ìò∏ÏßëÎã®)ÏùÑ Í≤®ÎÉ•Ìïú Ï∞®Î≥Ñ¬∑ÌòêÏò§Î°ú Ìï¥ÏÑùÎê† Ïó¨ÏßÄÍ∞Ä ÏûàÏñ¥ ÎØºÍ∞êÌï©ÎãàÎã§.")
                context.append("ÏùòÎèÑÏôÄ Í¥ÄÍ≥ÑÏóÜÏù¥ ‚ÄòÏùºÎ∞òÌôî‚ÄôÍ∞Ä ÏÑûÏù¥Î©¥ Ïò§Ìï¥Í∞Ä Ïª§Ïßà Ïàò ÏûàÏñ¥, ÎåÄÏÉÅ ÌëúÌòÑÏùÑ Í±∑Ïñ¥ÎÇ¥Îäî Ìé∏Ïù¥ ÏïàÏ†ÑÌï©ÎãàÎã§.")
            elif category == "threat":
                context.append("ÏÉÅÎåÄÍ∞Ä ‚ÄòÏïïÎ∞ï/ÌòëÎ∞ï‚ÄôÏúºÎ°ú ÎäêÎÇÑ Ïàò ÏûàÎäî Î¨∏Íµ¨Í∞Ä Ìè¨Ìï®ÎêòÎ©¥ Î∂ÑÏüÅ Í∞ÄÎä•ÏÑ±Ïù¥ Ïª§ÏßëÎãàÎã§.")
                context.append("Í∑úÏ†ï ÏïàÎÇ¥/Ï†àÏ∞® ÏïàÎÇ¥ ÌÜ§ÏúºÎ°ú Î∞îÍæ∏Îäî Í≤ÉÏù¥ ÏïàÏ†ÑÌïòÍ≥†, ÌîåÎû´Ìèº Ï†ïÏ±ÖÏóêÎèÑ Îçî Ïûò ÎßûÏäµÎãàÎã§.")
            elif category == "violence":
                context.append("Ìè≠Î†•Ï†Å ÌëúÌòÑÏùÄ ÎπÑÏú†ÎùºÎèÑ ÏúÑÌóò Ïã†Ìò∏Î°ú ÏùΩÌûê Ïàò ÏûàÏñ¥Ïöî.")
                context.append("Í∞êÏ†ï Ï†ÑÎã¨Ïù¥ Î™©Ï†ÅÏù¥ÎùºÎ©¥ Ìè≠Î†• ÏùÄÏú† ÎåÄÏã† Í∞êÏ†ï/ÏÉÅÌô©ÏùÑ ÏßÅÏ†ë ÏÑúÏà†ÌïòÎäî Î∞©ÏãùÏù¥ ÏïàÏ†ÑÌï©ÎãàÎã§.")
            elif category == "sexual_content":
                context.append("ÏÑ±Ï†Å ÎâòÏïôÏä§Îäî Îß•ÎùΩÏóê Îî∞Îùº Î∂àÏæåÍ∞êÏùÑ Ï§Ñ Ïàò ÏûàÏñ¥ Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
                context.append("ÌëúÌòÑÏùÑ Îã¥Î∞±ÌïòÍ≤å Ï†ïÎ¶¨ÌïòÍ≥†, Î∂àÌïÑÏöîÌïú Î¨òÏÇ¨Îäî ÌîºÌïòÎäî Í≤ÉÏùÑ Í∂åÏû•Ìï©ÎãàÎã§.")
            elif category == "defamation":
                context.append("ÌäπÏ†ï Í∞úÏù∏/Îã®Ï≤¥Î•º Îã®Ï†ïÏ†ÅÏúºÎ°ú ÎπÑÎÇúÌïòÍ±∞ÎÇò ÏÇ¨Ïã§Ï≤òÎüº ÏÑúÏà†ÌïòÎ©¥ Î™ÖÏòàÌõºÏÜê Ïù¥ÏäàÎ°ú Ïù¥Ïñ¥Ïßà Ïàò ÏûàÏñ¥Ïöî.")
                context.append("ÏÇ¨Ïã§Í¥ÄÍ≥ÑÍ∞Ä ÌôïÏù∏ÎêòÏßÄ ÏïäÏïòÎã§Î©¥ Îã®Ï†ï ÎåÄÏã† Ï∂îÏ†ï ÌëúÌòÑÏùÑ Ïì∞Îäî Í≤å ÏïàÏ†ÑÌï©ÎãàÎã§.")
            elif category == "privacy":
                context.append("Í∞úÏù∏Ï†ïÎ≥¥(Ïã§Î™Ö/Ïó∞ÎùΩÏ≤ò/Ï£ºÏÜå Îì±)Í∞Ä Ìè¨Ìï®Îê† Í≤ΩÏö∞ Ïã¨Í∞ÅÌïú ÌîÑÎùºÏù¥Î≤ÑÏãú Ïπ®Ìï¥Í∞Ä Îê† Ïàò ÏûàÏñ¥Ïöî.")
                context.append("ÎØºÍ∞ê Ï†ïÎ≥¥Îäî ÏÇ≠Ï†úÌïòÍ±∞ÎÇò ÏùµÎ™Ö Ï≤òÎ¶¨ÌïòÎäî Í≤å ÏïàÏ†ÑÌï©ÎãàÎã§.")

            # Ïã†Ìò∏ ÏöîÏïΩ(Î†àÎ≤®Îßå)
            signal_line = (
                f"ÌòÑÏû¨ ÌÜ§ Í∏∞Ï§ÄÏúºÎ°úÎäî "
                f"Í≥µÍ≤©ÏÑ±ÏùÄ {level_ko(toxicity)}, "
                f"ÏúÑÌòëÏÑ±ÏùÄ {level_ko(threat)}, "
                f"Ìè≠Î†•ÏÑ±ÏùÄ {level_ko(violence)}, "
                f"ÌòêÏò§ Í∞ÄÎä•ÏÑ±ÏùÄ {level_ko(hate)}, "
                f"ÏÑ±Ï†Å ÎâòÏïôÏä§Îäî {level_ko(sexual)} Ï†ïÎèÑÎ°ú Ìï¥ÏÑùÎê©ÎãàÎã§."
            )

            # ÌÇ§ÏõåÎìú ÌûåÌä∏
            keyword_line = ""
            if detected_keywords:
                keyword_line = f"ÎòêÌïú ÏùºÎ∂Ä ÌëúÌòÑ(ÌÇ§ÏõåÎìú)Ïù¥ Í∞ïÌïú ÌÜ§ÏúºÎ°ú Ïù∏ÏãùÎê† Ïàò ÏûàÏñ¥Ïöî: {', '.join(detected_keywords)}"

            # Í∂åÏû• Ï°∞Ïπò
            if category in ("threat", "violence", "hate_speech", "sexual_content", "privacy"):
                action = "Í∂åÏû• Ï°∞Ïπò: **Í≤åÏãú Ï†Ñ ÏàòÏ†ï/Í≤ÄÌÜ†**Î•º Í∂åÏû•Ìï©ÎãàÎã§. (ÌïÑÏöî Ïãú Ïà®ÍπÄ/Í≤ΩÍ≥†/Ï†úÌïú Ï†ïÏ±Ö Ï†ÅÏö©)"
            elif category in ("defamation",):
                action = "Í∂åÏû• Ï°∞Ïπò: **ÏÇ¨Ïã§ ÌôïÏù∏ Ï†Ñ Îã®Ï†ï ÌëúÌòÑ Í∏àÏßÄ**, Ïã§Î™Ö/Íµ¨Ï≤¥ Ï†ïÎ≥¥ Ìè¨Ìï® Ïó¨Î∂ÄÎ•º Ï†êÍ≤ÄÌïòÏÑ∏Ïöî."
            elif category in ("toxic",):
                action = "Í∂åÏû• Ï°∞Ïπò: ÏÉÅÎåÄÎ•º ÎπÑÎÇúÌïòÍ∏∞Î≥¥Îã§ **ÏÉÅÌô©¬∑ÌñâÎèô Ï§ëÏã¨ÏúºÎ°ú ÌëúÌòÑÏùÑ ÏôÑÌôî**ÌïòÎ©¥ Í≤åÏãú ÌóàÏö© Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏïÑÏßëÎãàÎã§."
            else:
                action = "Í∂åÏû• Ï°∞Ïπò: ÌòÑ Îã®Í≥ÑÏóêÏÑúÎäî Í≤åÏãú Í∞ÄÎä•ÌïòÎÇò, ÎØºÍ∞ê Ï£ºÏ†úÎäî Ï∂îÍ∞Ä Î™®ÎãàÌÑ∞ÎßÅÏùÑ Í∂åÏû•Ìï©ÎãàÎã§."

            # Î∞îÍøîÏì∞Í∏∞ Ï†úÏïà
            rewrites = rewrite_ko(category)

            # ÏµúÏ¢Ö Ïû•Î¨∏ Íµ¨ÏÑ±
            parts = [
                intro,
                "",
                "Ìï¥ÏÑù",
                *[f"- {c}" for c in context],
                "",
                "ÏúÑÌóò Ïã†Ìò∏ ÏöîÏïΩ(Ï≤¥Í∞ê Í∏∞Ï§Ä)",
                f"- {signal_line}",
            ]
            if keyword_line:
                parts += ["", "Ï∞∏Í≥†", f"- {keyword_line}"]

            # Guard Í∏∞Î∞ò Î©îÎ™® (Î∞úÌëú Îïå Ï†êÏàòÎ≥¥Îã§ ÏÑ§ÎìùÎ†•)
            if guard_violated:
                parts += ["", "ÏïàÏ†Ñ Ï†ïÏ±Ö Í¥ÄÏ†ê(Guard)", f"- Ï†ïÏ±ÖÏÉÅ ÎØºÍ∞ê Ïπ¥ÌÖåÍ≥†Î¶¨ Ïã†Ìò∏: {', '.join(guard_violated)}"]

            parts += [
                "",
                "Îçî ÏïàÏ†ÑÌïú ÌëúÌòÑ Ï†úÏïà(ÏòàÏãú)",
                *[f"- {r}" for r in rewrites],
                "",
                f"{action}",
            ]
            return "\n".join(parts)

        # English
        intro = f"Based on the analysis, this text is classified as **{category}**."
        context = []

        if category == "safe":
            context.append("No strong hostile/threatening signals are present in typical conversation contexts.")
            context.append("If the topic is sensitive, keeping a neutral tone can further reduce misunderstandings.")
        elif category == "toxic":
            context.append("The wording can be perceived as judgmental or insulting.")
            context.append("Sarcasm, ridicule, or definitive put-downs often escalate conflict even if unintended.")
        elif category == "hate_speech":
            context.append("It may be interpreted as targeting an identity/protected group, which is highly sensitive.")
            context.append("Even without intent, group generalizations can be perceived as discriminatory.")
        elif category == "threat":
            context.append("Some parts may be perceived as intimidation or coercion.")
            context.append("A neutral policy/procedure tone is safer and aligns better with platform guidelines.")
        elif category == "violence":
            context.append("Violent wording can be risky even as a metaphor.")
            context.append("Use non-violent emotional descriptions to avoid misinterpretation.")
        elif category == "sexual_content":
            context.append("Sexual implications can cause discomfort depending on context.")
            context.append("Keep it informational and avoid suggestive phrasing.")
        elif category == "defamation":
            context.append("Definitive accusations can create defamation risk.")
            context.append("Use cautious language and avoid sharing identifying details.")
        elif category == "privacy":
            context.append("Personal data exposure is risky.")
            context.append("Remove or anonymize identifying information.")

        signal_line = (
            f"Tone-level signals: toxicity {level_en(toxicity)}, threat {level_en(threat)}, "
            f"violence {level_en(violence)}, hate {level_en(hate)}, sexual {level_en(sexual)}."
        )

        keyword_line = ""
        if detected_keywords:
            keyword_line = f"Potential strong tone indicators (keywords): {', '.join(detected_keywords)}"

        if category in ("threat", "violence", "hate_speech", "sexual_content", "privacy"):
            action = "Recommended action: **revise before posting** (consider hide/warn/restrict if needed)."
        elif category == "defamation":
            action = "Recommended action: avoid definitive accusations; check for identifying details."
        elif category == "toxic":
            action = "Recommended action: soften the tone and focus on behavior/situation rather than labeling a person."
        else:
            action = "Recommended action: allow, but monitor sensitive contexts."

        rewrites = rewrite_en(category)

        parts = [
            intro,
            "",
            "Interpretation",
            *[f"- {c}" for c in context],
            "",
            "Signal summary (human-readable)",
            f"- {signal_line}",
        ]
        if keyword_line:
            parts += ["", "Note", f"- {keyword_line}"]
        if guard_violated:
            parts += ["", "Guard policy signal", f"- Potential sensitive categories: {', '.join(guard_violated)}"]

        parts += [
            "",
            "‚úÖ Safer rewrite suggestions",
            *[f"- {r}" for r in rewrites],
            "",
            f"üß≠ {action}",
        ]
        return "\n".join(parts)

    # ==================== Î©îÏù∏ Î∂ÑÏÑù ====================
    async def analyze_text(self, text: str, language: str = "auto", use_dual_model: bool = True) -> AnalysisResponse:
        import time

        start_time = time.time()

        try:
            if not language or language == "auto":
                language = self._detect_language_simple(text)

            # Ï∫êÏãú ÌûàÌä∏
            cache_key = self._make_cache_key(text, language, use_dual_model)
            cached = self._result_cache.get(cache_key)
            if isinstance(cached, dict):
                result = dict(cached)
                processing_time = (time.time() - start_time) * 1000
                result["processing_time_ms"] = round(processing_time, 2)
                result["analyzed_at"] = datetime.now().isoformat()
                result["ai_model_version"] = self.model_version
                return AnalysisResponse(**result)

            # 1) Î£∞ Í∏∞Î∞ò
            rule_result = self._rule_based_filter(text, language)

            if not self.api_key:
                logger.warning("No API key, using fallback")
                result = self._create_fallback_response(text, rule_result, language)
            elif use_dual_model:
                result = await self._dual_model_analysis(text, language, rule_result)
            else:
                result = await self._single_model_analysis(text, language, rule_result)

            # ÏãúÍ∞Ñ/Î≤ÑÏ†Ñ ÌïÑÎìú
            processing_time = (time.time() - start_time) * 1000
            result["processing_time_ms"] = round(processing_time, 2)
            result["analyzed_at"] = datetime.now().isoformat()
            result["ai_model_version"] = self.model_version

            # Ïû•Î¨∏ reasoningÏùÄ ÏÑúÎ≤ÑÏóêÏÑú Ïû¨ÏûëÏÑ±
            try:
                result["llama_reasoning"] = self._make_reasoning_longform(
                    language=language,
                    category=str(result.get("category", "safe")),
                    text=text,
                    detected_keywords=result.get("detected_keywords", []),
                    toxicity=float(result.get("toxicity_score", 0)),
                    threat=float(result.get("threat_score", 0)),
                    violence=float(result.get("violence_score", 0)),
                    hate=float(result.get("hate_speech_score", 0)),
                    sexual=float(result.get("sexual_score", 0)),
                    protected_group=bool(result.get("protected_group", False)),
                    guard_violated=result.get("guard_categories", []),
                )
            except Exception:
                pass

            # Ï∫êÏãú Ï†ÄÏû•(Î≥ÄÎèô ÌïÑÎìú Ï†úÏô∏)
            try:
                core = dict(result)
                core.pop("processing_time_ms", None)
                core.pop("analyzed_at", None)
                core.pop("ai_model_version", None)

                if len(self._result_cache) >= self._cache_max_items:
                    oldest_key = next(iter(self._result_cache.keys()), None)
                    if oldest_key:
                        self._result_cache.pop(oldest_key, None)

                self._result_cache[cache_key] = core
            except Exception:
                pass

            # protected_groupÏùÄ ÎÇ¥Î∂Ä Í≥ÑÏÇ∞Ïö©Ïù¥ÎØÄÎ°ú ÏùëÎãµ Î™®Îç∏ÏóêÎäî ÏóÜÏßÄÎßå,
            # dictÏóê ÎÇ®ÏïÑÏûàÏúºÎ©¥ PydanticÏù¥ ÏóêÎü¨Î•º ÎÇ¥Îãà Ï†úÍ±∞
            result.pop("protected_group", None)

            return AnalysisResponse(**result)

        except Exception as e:
            logger.error(f"Analysis error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

    # ==================== ÎìÄÏñº Î∂ÑÏÑù ====================
    async def _dual_model_analysis(self, text: str, language: str, rule_result: Dict) -> Dict[str, Any]:
        try:
            guard_task = self._llama_guard_check(text, language)
            llama_task = self._llama_analysis(text, language)

            guard_result, llama_result = await asyncio.gather(guard_task, llama_task, return_exceptions=True)

            if isinstance(guard_result, Exception):
                logger.error(f"Guard model failed: {guard_result}")
                guard_result = self._fallback_guard_result()

            if isinstance(llama_result, Exception):
                logger.error(f"Llama model failed: {llama_result}")
                llama_result = self._fallback_analysis(text, language)

            combined = self._combine_dual_results(rule_result, guard_result, llama_result)
            return combined

        except Exception as e:
            logger.error(f"Dual model analysis failed: {e}")
            return self._create_fallback_response(text, rule_result, language)

    # ==================== Guard ====================
    async def _llama_guard_check(self, text: str, language: str) -> Dict[str, Any]:
        try:
            safe_text = self._mask_text_for_api(text, language)

            prompt = f"""[INST] Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Violent Crimes
S2: Non-Violent Crimes
S3: Sex-Related Crimes
S4: Child Sexual Exploitation
S5: Defamation
S6: Specialized Advice
S7: Privacy
S8: Intellectual Property
S9: Indiscriminate Weapons
S10: Hate
S11: Self-Harm
S12: Sexual Content
S13: Elections
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>
User: {safe_text}
<END CONVERSATION>

Provide your safety assessment for User's message:
- First line must be 'safe' or 'unsafe'
- If unsafe, list all violated categories (S1, S2, etc.) [/INST]"""

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.models["guard"],
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.0,
                        "max_tokens": 100,
                    },
                )

            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()

                is_safe = content.lower().startswith("safe")
                violated_categories = []

                if not is_safe:
                    categories = re.findall(r"S\d+", content)
                    violated_categories = [self.guard_categories.get(cat, cat) for cat in categories]

                return {
                    "is_safe": is_safe,
                    "violated_categories": violated_categories,
                    "raw_response": content,
                    "guard_success": True,
                }

            logger.error(f"Guard API error: {response.status_code} | body: {response.text[:800]}")
            return self._fallback_guard_result()

        except Exception as e:
            logger.error(f"Guard check failed: {e}")
            return self._fallback_guard_result()

    # ==================== Llama Î∂ÑÏÑù (scam/spam Ï†úÍ±∞ Î≤ÑÏ†Ñ) ====================
    async def _llama_analysis(self, text: str, language: str) -> Dict[str, Any]:
        """Llama 3.1 ÏÉÅÏÑ∏ Î∂ÑÏÑù (JSON Í∞ïÏ†ú + ko/en Ï†ÑÏö©, scam/spam Ï†úÏô∏)"""

        lang_to_label = {"ko": "Korean", "en": "English"}
        reasoning_lang_label = lang_to_label.get(language, "English")

        safe_text = self._mask_text_for_api(text, language)

        # ÏöïÏÑ§ ÌûåÌä∏(ÎßàÏä§ÌÇπÎêú Îã®Ïñ¥Í∞Ä ÏûàÏóàÎã§Î©¥ ÌûåÌä∏ Ï†úÍ≥µ)
        hint_keywords = []
        try:
            words = self.blocked_words.get(language, [])
            text_norm = text.casefold()
            for w in words:
                if w and w.casefold() in text_norm:
                    hint_keywords.append(w)
        except Exception:
            hint_keywords = []

        system_prompt = f"""
You are an expert in analyzing toxic and harmful content for online posts/comments.

Return ONLY a single valid JSON object.
- No markdown, no extra text, no code fences.
- Do NOT include any explanations outside JSON.
- Scores must be integers 0-100 (no % sign).
- The 'reasoning' must be written in {reasoning_lang_label}.
- Do NOT translate the user's text. Analyze it as-is.
- Do NOT quote or repeat slurs/profanity.
- If the text contains [MASK], treat it as an explicit strong insult/profanity indicator.

IMPORTANT (reduce false positives):
- hate_speech_score MUST be high ONLY when the text targets an identity/protected group
  (e.g., race, nationality, ethnicity, religion, gender, sexual orientation, disability, etc.).
- If the text criticizes "some people" without identity/protected-group references,
  treat it as general toxicity/harassment, NOT hate speech.

Scoring guidance (context-first):
- toxicity_score: overall hostility/insulting tone including sarcasm/derision
- hate_speech_score: identity/protected-group based hate/discrimination
- profanity_score: explicit profanity/curse intensity
- threat_score: intimidation, implied harm, coercive consequences
- violence_score: violence encouragement/graphic violence
- sexual_score: explicit/implicit sexual content

JSON schema:
{{
  "toxicity_score": 0,
  "hate_speech_score": 0,
  "profanity_score": 0,
  "threat_score": 0,
  "violence_score": 0,
  "sexual_score": 0,
  "protected_group": false,
  "reasoning": ""
}}
""".strip()

        user_prompt = f'Text ({reasoning_lang_label}): "{safe_text}"\nDetected keywords (may be masked): {hint_keywords}'

        async def _post(payload: Dict[str, Any]) -> httpx.Response:
            async with httpx.AsyncClient(timeout=30.0) as client:
                return await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json=payload,
                )

        base_payload = {
            "model": self.models["analysis"],
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": 0.0,
            "top_p": 1.0,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0,
            "max_tokens": 300,
        }

        payload_jsonmode = dict(base_payload)
        payload_jsonmode["response_format"] = {"type": "json_object"}

        try:
            resp = await _post(payload_jsonmode)

            if resp.status_code in (400, 422):
                logger.warning(
                    f"[LLAMA] response_format not supported. retry without it. "
                    f"status={resp.status_code} body={resp.text[:800]}"
                )
                resp = await _post(base_payload)

            if resp.status_code != 200:
                logger.error(f"[LLAMA] API error: {resp.status_code} | body: {resp.text[:1200]}")
                return self._fallback_analysis(text, language)

            result = resp.json()
            content = result["choices"][0]["message"]["content"]

            json_result = self._extract_json(content)
            if not json_result:
                logger.warning(f"[LLAMA] JSON parse failed. raw(head): {content[:600]}")
                return self._fallback_analysis(text, language)

            def _to_int(v, default=0):
                try:
                    return int(v)
                except Exception:
                    return default

            return {
                "toxicity_score": _to_int(json_result.get("toxicity_score", 0)),
                "hate_speech_score": _to_int(json_result.get("hate_speech_score", 0)),
                "profanity_score": _to_int(json_result.get("profanity_score", 0)),
                "threat_score": _to_int(json_result.get("threat_score", 0)),
                "violence_score": _to_int(json_result.get("violence_score", 0)),
                "sexual_score": _to_int(json_result.get("sexual_score", 0)),
                "protected_group": bool(json_result.get("protected_group", False)),
                "reasoning": str(json_result.get("reasoning", "")),
                "llama_success": True,
            }

        except Exception as e:
            logger.error(f"[LLAMA] analysis failed: {e}")
            return self._fallback_analysis(text, language)

    # ==================== Îã®Ïùº Î™®Îç∏ ====================
    async def _single_model_analysis(self, text: str, language: str, rule_result: Dict) -> Dict[str, Any]:
        llama_result = await self._llama_analysis(text, language)

        weight_rule = 0.3
        weight_llama = 0.7

        toxicity = rule_result["rule_score"] * weight_rule + llama_result.get("toxicity_score", 0) * weight_llama

        hate_speech = llama_result.get("hate_speech_score", 0)
        profanity = llama_result.get("profanity_score", 0)
        threat = llama_result.get("threat_score", 0)
        violence = llama_result.get("violence_score", 0)
        sexual = llama_result.get("sexual_score", 0)
        protected_group = bool(llama_result.get("protected_group", False))

        is_malicious = (
            toxicity > 55.0
            or (hate_speech > 65.0 and protected_group)
            or profanity > 75.0
            or threat > 45.0
            or violence > 65.0
            or sexual > 75.0
            or rule_result.get("is_malicious_rule", False)
        )

        category = self._decide_category_mvp(
            toxicity=toxicity,
            threat=threat,
            violence=violence,
            hate=hate_speech,
            sexual=sexual,
            protected_group=protected_group,
            guard_violated=[],  # Îã®Ïùº Î™®ÎìúÏóêÏÑúÎäî guard ÎØ∏ÏÇ¨Ïö©
        )

        confidence_score = min(100.0, max(toxicity, hate_speech, profanity, threat, violence, sexual))

        return {
            "is_malicious": is_malicious,
            "toxicity_score": float(round(toxicity, 2)),
            "hate_speech_score": float(round(hate_speech, 2)),
            "profanity_score": float(round(profanity, 2)),
            "threat_score": float(round(threat, 2)),
            "violence_score": float(round(violence, 2)),
            "sexual_score": float(round(sexual, 2)),
            "confidence_score": float(round(confidence_score, 2)),
            "category": category,
            "detected_keywords": rule_result.get("detected_keywords", []),
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": llama_result.get("reasoning", ""),
            "protected_group": protected_group,  # ÎÇ¥Î∂Ä ÌõÑÏ≤òÎ¶¨Ïö©
        }

    # ==================== Î£∞ Í∏∞Î∞ò ====================
    def _rule_based_filter(self, text: str, language: str) -> Dict[str, Any]:
        detected = []
        score = 0.0

        if language not in ("ko", "en"):
            language = "en"

        words = self.blocked_words.get(language, [])
        text_norm = text.casefold()

        for word in words:
            if word.casefold() in text_norm:
                detected.append(word)
                score += 15.0

        return {
            "detected_keywords": detected,
            "rule_score": min(score, 100.0),
            "is_malicious_rule": False,  # ÌÇ§ÏõåÎìúÎßåÏúºÎ°ú ÌôïÏ†ï Í∏àÏßÄ
        }

    # ==================== ÎìÄÏñº Í≤∞Í≥º ÌÜµÌï© (scam/spam Ï†úÍ±∞) ====================
    def _combine_dual_results(self, rule_result: Dict, guard_result: Dict, llama_result: Dict) -> Dict[str, Any]:
        guard_boost = 0
        if not guard_result.get("is_safe", True):
            guard_boost = 30

        weight_rule = 0.08
        weight_guard = 0.32
        weight_llama = 0.60

        toxicity = (
            rule_result["rule_score"] * weight_rule
            + guard_boost * weight_guard
            + llama_result.get("toxicity_score", 0) * weight_llama
        )

        hate_speech = llama_result.get("hate_speech_score", 0)
        profanity = llama_result.get("profanity_score", 0)
        threat = llama_result.get("threat_score", 0)
        violence = llama_result.get("violence_score", 0)
        sexual = llama_result.get("sexual_score", 0)
        protected_group = bool(llama_result.get("protected_group", False))

        violated_cats = guard_result.get("violated_categories", [])

        # Guard Í∏∞Î∞ò Î≥¥Ï†ï
        if "hate" in violated_cats:
            hate_speech = max(hate_speech, 80)
            protected_group = True
        if "violent_crimes" in violated_cats:
            violence = max(violence, 85)
        if "sexual_content" in violated_cats:
            sexual = max(sexual, 85)

        is_malicious = (
            toxicity > 55.0
            or (hate_speech > 65.0 and protected_group)
            or profanity > 75.0
            or threat > 45.0
            or violence > 65.0
            or sexual > 75.0
            or (not guard_result.get("is_safe", True))
        )

        category = self._decide_category_mvp(
            toxicity=toxicity,
            threat=threat,
            violence=violence,
            hate=hate_speech,
            sexual=sexual,
            protected_group=protected_group,
            guard_violated=violated_cats,
        )

        confidence_score = min(100.0, max(toxicity, hate_speech, profanity, threat, violence, sexual))

        return {
            "is_malicious": is_malicious,
            "toxicity_score": float(round(toxicity, 2)),
            "hate_speech_score": float(round(hate_speech, 2)),
            "profanity_score": float(round(profanity, 2)),
            "threat_score": float(round(threat, 2)),
            "violence_score": float(round(violence, 2)),
            "sexual_score": float(round(sexual, 2)),
            "confidence_score": float(round(confidence_score, 2)),
            "category": category,
            "detected_keywords": rule_result.get("detected_keywords", []),
            "guard_result": guard_result,
            "guard_categories": violated_cats,
            "llama_reasoning": llama_result.get("reasoning", ""),
            "protected_group": protected_group,  # ÎÇ¥Î∂Ä ÌõÑÏ≤òÎ¶¨Ïö©
        }

    # ==================== fallback ====================
    def _fallback_guard_result(self) -> Dict[str, Any]:
        return {"is_safe": True, "violated_categories": [], "raw_response": "Guard unavailable", "guard_success": False}

    def _fallback_analysis(self, text: str, language: str = "en") -> Dict[str, Any]:
        # Ïû•Î¨∏ Í∏∏Ïù¥ Í∏∞Î∞ò Ï†êÏàò Í∏àÏßÄ ‚Üí Î£∞ Í∏∞Î∞òÏúºÎ°ú Î≥¥Ïàò Ï∂îÏ†ï
        try:
            rule = self._rule_based_filter(text, language)
            rule_score = float(rule.get("rule_score", 0.0))
        except Exception:
            rule_score = 0.0

        base_score = 5.0 if rule_score <= 0 else min(max(rule_score, 15.0), 60.0)

        fallback_reasoning_map = {
            "ko": "Î∂ÑÏÑù Î™®Îç∏ Ìò∏Ï∂ú/ÌååÏã±Ïóê Ïã§Ìå®ÌïòÏó¨ Î≥¥ÏàòÏ†ÅÏù∏ Í∑úÏπô Í∏∞Î∞ò Ï∂îÏ†ï Í≤∞Í≥ºÎ•º Î∞òÌôòÌï©ÎãàÎã§.",
            "en": "Model call/parse failed; returning conservative rule-based estimates.",
        }
        reasoning = fallback_reasoning_map.get(language, fallback_reasoning_map["en"])

        return {
            "toxicity_score": round(base_score, 2),
            "hate_speech_score": round(max(0.0, base_score - 10.0), 2),
            "profanity_score": round(max(0.0, base_score - 5.0), 2),
            "threat_score": round(max(0.0, base_score - 15.0), 2),
            "violence_score": round(max(0.0, base_score - 12.0), 2),
            "sexual_score": round(max(0.0, base_score - 20.0), 2),
            "protected_group": False,
            "reasoning": reasoning,
            "llama_success": False,
        }

    def _create_fallback_response(self, text: str, rule_result: Dict, language: str) -> Dict[str, Any]:
        score = float(rule_result.get("rule_score", 0.0))
        # fallback categoryÎèÑ MVP Í∏∞Ï§ÄÏúºÎ°ú
        category = self._decide_category_mvp(
            toxicity=score,
            threat=max(0, score - 30),
            violence=max(0, score - 25),
            hate=max(0, score - 20),
            sexual=max(0, score - 35),
            protected_group=False,
            guard_violated=[],
        )

        return {
            "is_malicious": rule_result.get("is_malicious_rule", False),
            "toxicity_score": score,
            "hate_speech_score": max(0, score - 20),
            "profanity_score": max(0, score - 10),
            "threat_score": max(0, score - 30),
            "violence_score": max(0, score - 25),
            "sexual_score": max(0, score - 35),
            "confidence_score": 40.0,
            "category": category,
            "detected_keywords": rule_result.get("detected_keywords", []),
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": "Fallback: Rule-based only",
            "protected_group": False,
        }

    # ==================== JSON Ï∂îÏ∂ú ====================
    def _extract_json(self, text: str) -> Optional[Dict]:
        if not text:
            return None

        s = text.strip()
        s = re.sub(r"^```(?:json)?\s*", "", s, flags=re.IGNORECASE)
        s = re.sub(r"\s*```$", "", s)

        try:
            obj = json.loads(s)
            if isinstance(obj, dict):
                return obj
        except Exception:
            pass

        start = s.find("{")
        if start == -1:
            return None

        depth = 0
        in_str = False
        esc = False
        for i in range(start, len(s)):
            ch = s[i]
            if in_str:
                if esc:
                    esc = False
                elif ch == "\\":
                    esc = True
                elif ch == '"':
                    in_str = False
                continue
            else:
                if ch == '"':
                    in_str = True
                    continue
                if ch == "{":
                    depth += 1
                elif ch == "}":
                    depth -= 1
                    if depth == 0:
                        candidate = s[start : i + 1].strip()
                        try:
                            return json.loads(candidate)
                        except Exception:
                            candidate2 = re.sub(r",\s*}", "}", candidate)
                            candidate2 = re.sub(r",\s*]", "]", candidate2)
                            try:
                                return json.loads(candidate2)
                            except Exception:
                                return None
        return None


# AI ÏÑúÎπÑÏä§ Ïù∏Ïä§ÌÑ¥Ïä§
analyzer = GroqDualModelAnalyzer()


# ==================== API ÏóîÎìúÌè¨Ïù∏Ìä∏ ====================

@app.on_event("startup")
async def startup_event():
    logger.info("=" * 60)
    logger.info("SNS Content Analyzer - Groq Dual Model (MVP v3.1.0)")
    logger.info("=" * 60)
    if analyzer.api_key:
        logger.info("‚úì Groq API configured")
        logger.info(f"  - Guard Model: {analyzer.models['guard']}")
        logger.info(f"  - Analysis Model: {analyzer.models['analysis']}")
        logger.info("  - Strategy: Parallel (Guard + Llama) + server-side longform reasoning")
    else:
        logger.warning("‚ö† No API key - fallback mode")


@app.get("/")
async def root():
    return {
        "service": "SNS Content Analyzer - Groq Dual Model (MVP)",
        "status": "running",
        "version": "3.1.0",
        "models": {"guard": analyzer.models["guard"], "analysis": analyzer.models["analysis"]},
        "strategy": "Guard filters unsafe content ‚Üí Llama scores ‚Üí Server generates longform reasoning",
        "api_configured": bool(analyzer.api_key),
        "features": [
            "Dual model analysis",
            "MVP categories (safe/toxic/hate/threat/violence/sexual + optional defamation/privacy)",
            "Longform human-like feedback (server-generated)",
        ],
    }


@app.post("/analyze/text", response_model=AnalysisResponse)
async def analyze_text(request: Request):
    """
    ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (ÎìÄÏñº Î™®Îç∏)

    - use_dual_model=True: Guard + Llama 3.1
    - use_dual_model=False: Llama 3.1 only
    """

    try:
        content_type = (request.headers.get("content-type") or "").lower()
        raw_bytes = await request.body()
        raw_text = raw_bytes.decode("utf-8", errors="ignore").strip()

        data: Any = None
        try:
            data = await request.json()
        except Exception:
            data = None

        payload: Dict[str, Any] = {}

        if isinstance(data, dict):
            payload = data
        elif isinstance(data, str):
            payload = {"text": data, "language": "auto", "use_dual_model": True}
        else:
            if "application/x-www-form-urlencoded" in content_type:
                parsed = urllib.parse.parse_qs(raw_text)
                payload = {
                    "text": (parsed.get("text", [""])[0] or "").strip(),
                    "language": (parsed.get("language", ["auto"])[0] or "auto").strip(),
                    "use_dual_model": str(parsed.get("use_dual_model", ["true"])[0]).lower() != "false",
                }
            else:
                payload = {"text": raw_text, "language": "auto", "use_dual_model": True}

        if not payload.get("text"):
            if payload.get("content"):
                payload["text"] = payload["content"]
            elif payload.get("message"):
                payload["text"] = payload["message"]

        req_obj = TextAnalysisRequest(**payload)

    except Exception as e:
        logger.error(
            f"[REQ] Invalid request body. content-type={request.headers.get('content-type')} raw(head)={raw_text[:200]}"
        )
        raise HTTPException(status_code=400, detail=f"Invalid request body: {str(e)}")

    raw_lang = (req_obj.language or "auto").strip().lower()
    detected = analyzer._detect_language_simple(req_obj.text)

    if raw_lang == "auto":
        used_lang = detected
    else:
        # ko/en Ïô∏ Í∞í Îì§Ïñ¥Ïò§Î©¥ Î≥¥Ï†ï
        if raw_lang not in ("ko", "en"):
            used_lang = detected
        else:
            used_lang = raw_lang

    logger.info(f"[LANG] raw={raw_lang} detected={detected} used={used_lang}")
    logger.info(f"Analyzing text (length: {len(req_obj.text)}, dual: {req_obj.use_dual_model})")

    result = await analyzer.analyze_text(req_obj.text, used_lang, req_obj.use_dual_model)
    return result


@app.post("/analyze/batch")
async def analyze_batch(texts: List[str] = Body(...), language: str = "auto", use_dual_model: bool = True):
    logger.info(f"Batch analysis: {len(texts)} texts")
    results = []
    for text in texts[:10]:
        try:
            result = await analyzer.analyze_text(text, language, use_dual_model)
            results.append(result)
        except Exception as e:
            logger.error(f"Failed: {e}")
            results.append(None)

    return {"total": len(results), "results": results, "dual_model": use_dual_model, "processed_at": datetime.now().isoformat()}


@app.get("/models/info")
async def models_info():
    return {
        "guard_model": {
            "name": analyzer.models["guard"],
            "purpose": "Safety filtering",
            "categories": analyzer.guard_categories,
            "speed": "~100ms",
        },
        "analysis_model": {
            "name": analyzer.models["analysis"],
            "purpose": "Detailed scoring",
            "features": ["Scoring", "Reasoning", "Multi-category (MVP)"],
            "speed": "~50ms",
        },
        "strategy": "Parallel execution + server-side longform reasoning",
    }


@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "api_configured": bool(analyzer.api_key),
        "models_ready": True,
        "timestamp": datetime.now().isoformat(),
    }


if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("SNS Content Analyzer - Groq Dual Model (MVP v3.1.0)")
    print("=" * 60)
    print("\nüöÄ Models:")
    print(f"  1. {analyzer.models['guard']} - Safety filtering")
    print(f"  2. {analyzer.models['analysis']} - Detailed analysis (MVP categories)")
    print("\n‚ö° Strategy:")
    print("  - Parallel execution (both models run simultaneously)")
    print("  - Server generates longform human-like feedback (no numeric dump)")
    print("\nüí∞ Cost: 100% FREE")
    print("\nÏÑúÎ≤Ñ ÏãúÏûë Ï§ë...\n")

    uvicorn.run(
        "main_groq_dual:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info",
    )

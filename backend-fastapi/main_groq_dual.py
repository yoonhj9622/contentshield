"""
SNS Content Analyzer - Groq Dual Model Edition
Llama-Guard-4-12b (í•„í„°ë§) + Llama-3.1-8b-instant (ë¶„ì„)
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import logging
import os
from datetime import datetime
import httpx
import json
import re
import asyncio

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="SNS Content Analyzer - Groq Dual Model",
    description="Llama Guard 4 + Llama 3.1 ë“€ì–¼ ëª¨ë¸ ì•…ì„± ì½˜í…ì¸  íƒì§€",
    version="3.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ë°ì´í„° ëª¨ë¸ ====================

class TextAnalysisRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=10000)
    language: str = Field(default="ko")
    use_dual_model: bool = Field(default=True, description="ë‘ ëª¨ë¸ ëª¨ë‘ ì‚¬ìš© ì—¬ë¶€")


class AnalysisResponse(BaseModel):
    is_malicious: bool
    toxicity_score: float
    hate_speech_score: float
    profanity_score: float
    threat_score: float
    violence_score: float
    sexual_score: float
    confidence_score: float
    category: str
    detected_keywords: List[str]
    
    # Guard ëª¨ë¸ ê²°ê³¼
    guard_result: Optional[Dict[str, Any]] = None
    guard_categories: List[str] = []
    
    # Llama 3.1 ê²°ê³¼
    llama_reasoning: Optional[str] = None
    
    ai_model_version: str
    processing_time_ms: float
    analyzed_at: str


# ==================== Groq Dual Model Analyzer ====================

class GroqDualModelAnalyzer:
    """Llama Guard 4 + Llama 3.1 ë“€ì–¼ ëª¨ë¸ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model_version = "groq-dual-llama-guard4-llama3.1"
        self.api_key = os.getenv("GROQ_API_KEY")
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"
        
        if not self.api_key:
            logger.warning("âš ï¸ GROQ_API_KEY not set")
            logger.warning("  Get FREE API key: https://console.groq.com/keys")
        else:
            logger.info("âœ“ Groq API key configured")
        
        # ëª¨ë¸ ì„¤ì •
        self.models = {
            "guard": "llama-guard-3-8b",  # Llama Guard 4ê°€ ì•„ì§ ì—†ìœ¼ë©´ 3 ì‚¬ìš©
            "analysis": "llama-3.1-8b-instant"
        }
        
        # Guard ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.guard_categories = {
            "S1": "violent_crimes",      # í­ë ¥ ë²”ì£„
            "S2": "non_violent_crimes",  # ë¹„í­ë ¥ ë²”ì£„
            "S3": "sex_related_crimes",  # ì„±ë²”ì£„
            "S4": "child_exploitation",  # ì•„ë™ ì°©ì·¨
            "S5": "defamation",          # ëª…ì˜ˆí›¼ì†
            "S6": "specialized_advice",  # ì „ë¬¸ì  ì¡°ì–¸
            "S7": "privacy",             # í”„ë¼ì´ë²„ì‹œ ì¹¨í•´
            "S8": "intellectual_property", # ì§€ì ì¬ì‚°ê¶Œ
            "S9": "indiscriminate_weapons", # ë¬´ì°¨ë³„ ë¬´ê¸°
            "S10": "hate",               # í˜ì˜¤ í‘œí˜„
            "S11": "self_harm",          # ìí•´
            "S12": "sexual_content",     # ì„±ì  ì½˜í…ì¸ 
            "S13": "elections"           # ì„ ê±° ê´€ë ¨
        }
        
        # ê·œì¹™ ê¸°ë°˜ ì°¨ë‹¨ ë‹¨ì–´
        self.blocked_words = {
            "ko": [
                "ë°”ë³´", "ë©ì²­ì´", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì”¨ë°œ", "ì§€ë„", "ë¯¸ì¹œ",
                "ì£½ì—¬", "ì£½ì¼", "ë•Œë ¤", "í˜ì˜¤", "ì°¨ë³„", "êº¼ì ¸", "ë‹¥ì³"
            ],
            "en": [
                "stupid", "idiot", "fuck", "shit", "kill", "hate", "damn"
            ]
        }
        
        logger.info("Groq Dual Model Analyzer initialized")
        logger.info(f"  - Guard Model: {self.models['guard']}")
        logger.info(f"  - Analysis Model: {self.models['analysis']}")
    
    async def analyze_text(
        self, 
        text: str, 
        language: str = "ko",
        use_dual_model: bool = True
    ) -> AnalysisResponse:
        """í…ìŠ¤íŠ¸ ë¶„ì„ (ë“€ì–¼ ëª¨ë¸)"""
        import time
        start_time = time.time()
        
        try:
            # 1. ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ (ë¹ ë¥¸ ì²´í¬)
            rule_result = self._rule_based_filter(text, language)
            
            if not self.api_key:
                logger.warning("No API key, using fallback")
                result = self._create_fallback_response(text, rule_result)
            elif use_dual_model:
                # 2. ë“€ì–¼ ëª¨ë¸ ë¶„ì„ (Guard + Llama 3.1)
                result = await self._dual_model_analysis(text, language, rule_result)
            else:
                # 3. ë‹¨ì¼ ëª¨ë¸ ë¶„ì„ (Llama 3.1ë§Œ)
                result = await self._single_model_analysis(text, language, rule_result)
            
            processing_time = (time.time() - start_time) * 1000
            result["processing_time_ms"] = round(processing_time, 2)
            result["analyzed_at"] = datetime.now().isoformat()
            result["ai_model_version"] = self.model_version
            
            return AnalysisResponse(**result)
            
        except Exception as e:
            logger.error(f"Analysis error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")
    
    async def _dual_model_analysis(
        self, 
        text: str, 
        language: str,
        rule_result: Dict
    ) -> Dict[str, Any]:
        """ë“€ì–¼ ëª¨ë¸ ë¶„ì„ (Guard + Llama 3.1 ë³‘ë ¬ ì‹¤í–‰)"""
        try:
            # ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            guard_task = self._llama_guard_check(text, language)
            llama_task = self._llama_analysis(text, language)
            
            guard_result, llama_result = await asyncio.gather(
                guard_task,
                llama_task,
                return_exceptions=True
            )
            
            # ì—ëŸ¬ ì²˜ë¦¬
            if isinstance(guard_result, Exception):
                logger.error(f"Guard model failed: {guard_result}")
                guard_result = self._fallback_guard_result()
            
            if isinstance(llama_result, Exception):
                logger.error(f"Llama model failed: {llama_result}")
                llama_result = self._fallback_analysis(text)
            
            # ê²°ê³¼ í†µí•©
            combined = self._combine_dual_results(
                rule_result,
                guard_result,
                llama_result
            )
            
            return combined
            
        except Exception as e:
            logger.error(f"Dual model analysis failed: {e}")
            return self._create_fallback_response(text, rule_result)
    
    async def _llama_guard_check(self, text: str, language: str) -> Dict[str, Any]:
        """Llama Guard 4 ì•ˆì „ì„± ê²€ì‚¬"""
        try:
            # Guard í”„ë¡¬í”„íŠ¸ (ê³µì‹ í¬ë§·)
            prompt = f"""[INST] Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Violent Crimes
S2: Non-Violent Crimes
S3: Sex-Related Crimes
S4: Child Sexual Exploitation
S5: Defamation
S6: Specialized Advice
S7: Privacy
S8: Intellectual Property
S9: Indiscriminate Weapons
S10: Hate
S11: Self-Harm
S12: Sexual Content
S13: Elections
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>
User: {text}
<END CONVERSATION>

Provide your safety assessment for User's message:
- First line must be 'safe' or 'unsafe'
- If unsafe, list all violated categories (S1, S2, etc.) [/INST]"""

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.models["guard"],
                        "messages": [
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.0,
                        "max_tokens": 100
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()
                
                # Guard ê²°ê³¼ íŒŒì‹±
                is_safe = content.lower().startswith("safe")
                violated_categories = []
                
                if not is_safe:
                    # S1, S2 ë“± ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    categories = re.findall(r'S\d+', content)
                    violated_categories = [
                        self.guard_categories.get(cat, cat) 
                        for cat in categories
                    ]
                
                logger.info(f"Guard result: {'safe' if is_safe else 'unsafe'}, categories: {violated_categories}")
                
                return {
                    "is_safe": is_safe,
                    "violated_categories": violated_categories,
                    "raw_response": content,
                    "guard_success": True
                }
            else:
                logger.error(f"Guard API error: {response.status_code}")
                return self._fallback_guard_result()
                
        except Exception as e:
            logger.error(f"Guard check failed: {e}")
            return self._fallback_guard_result()
    
    async def _llama_analysis(self, text: str, language: str) -> Dict[str, Any]:
        """Llama 3.1 ìƒì„¸ ë¶„ì„"""
        try:
            system_prompt = """You are an expert in analyzing toxic and harmful content.
Analyze the given text and provide detailed scores (0-100) for each category.

Respond in valid JSON format only, no markdown:
{
  "toxicity_score": <0-100>,
  "hate_speech_score": <0-100>,
  "profanity_score": <0-100>,
  "threat_score": <0-100>,
  "violence_score": <0-100>,
  "sexual_score": <0-100>,
  "reasoning": "<brief explanation in same language as input>"
}"""

            user_prompt = f'Analyze this text for harmful content: "{text}"'
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.models["analysis"],
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.1,
                        "max_tokens": 300
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # JSON ì¶”ì¶œ
                json_result = self._extract_json(content)
                
                if json_result:
                    logger.info(f"Llama analysis: toxicity={json_result.get('toxicity_score', 0)}")
                    
                    return {
                        "toxicity_score": json_result.get("toxicity_score", 0),
                        "hate_speech_score": json_result.get("hate_speech_score", 0),
                        "profanity_score": json_result.get("profanity_score", 0),
                        "threat_score": json_result.get("threat_score", 0),
                        "violence_score": json_result.get("violence_score", 0),
                        "sexual_score": json_result.get("sexual_score", 0),
                        "reasoning": json_result.get("reasoning", ""),
                        "llama_success": True
                    }
                else:
                    logger.warning("Failed to parse Llama response")
                    return self._fallback_analysis(text)
            else:
                logger.error(f"Llama API error: {response.status_code}")
                return self._fallback_analysis(text)
                
        except Exception as e:
            logger.error(f"Llama analysis failed: {e}")
            return self._fallback_analysis(text)
    
    async def _single_model_analysis(
        self,
        text: str,
        language: str,
        rule_result: Dict
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ë¶„ì„ (Llama 3.1ë§Œ ì‚¬ìš©)"""
        llama_result = await self._llama_analysis(text, language)
        return self._combine_results(rule_result, llama_result)
    
    def _rule_based_filter(self, text: str, language: str) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ í•„í„°ë§"""
        detected = []
        score = 0.0
        
        words = self.blocked_words.get(language, [])
        text_lower = text.lower()
        
        for word in words:
            if word in text_lower:
                detected.append(word)
                score += 25.0
        
        return {
            "detected_keywords": detected,
            "rule_score": min(score, 100.0),
            "is_malicious_rule": score > 50.0
        }
    
    def _combine_dual_results(
        self,
        rule_result: Dict,
        guard_result: Dict,
        llama_result: Dict
    ) -> Dict[str, Any]:
        """ë“€ì–¼ ëª¨ë¸ ê²°ê³¼ í†µí•©"""
        
        # Guard ê²°ê³¼ ë°˜ì˜
        guard_boost = 0
        if not guard_result.get("is_safe", True):
            guard_boost = 30  # Guardê°€ unsafe íŒì • ì‹œ ì ìˆ˜ ìƒí–¥
        
        # ê°€ì¤‘ í‰ê· 
        weight_rule = 0.15
        weight_guard = 0.35
        weight_llama = 0.50
        
        toxicity = (
            rule_result["rule_score"] * weight_rule +
            guard_boost * weight_guard +
            llama_result.get("toxicity_score", 0) * weight_llama
        )
        
        hate_speech = llama_result.get("hate_speech_score", 0)
        profanity = llama_result.get("profanity_score", 0)
        threat = llama_result.get("threat_score", 0)
        violence = llama_result.get("violence_score", 0)
        sexual = llama_result.get("sexual_score", 0)
        
        # Guardì˜ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ì ìˆ˜ ì¡°ì •
        violated_cats = guard_result.get("violated_categories", [])
        if "hate" in violated_cats:
            hate_speech = max(hate_speech, 80)
        if "violent_crimes" in violated_cats:
            violence = max(violence, 85)
        if "sexual_content" in violated_cats:
            sexual = max(sexual, 85)
        
        # ì•…ì„± ì—¬ë¶€ íŒë‹¨
        is_malicious = (
            toxicity > 50.0 or
            hate_speech > 60.0 or
            profanity > 70.0 or
            threat > 40.0 or
            violence > 60.0 or
            sexual > 70.0 or
            not guard_result.get("is_safe", True) or
            rule_result["is_malicious_rule"]
        )
        
        # ì¹´í…Œê³ ë¦¬ ê²°ì •
        if violence > 70:
            category = "violence"
        elif sexual > 70:
            category = "sexual_content"
        elif threat > 60:
            category = "threat"
        elif hate_speech > 60:
            category = "hate_speech"
        elif profanity > 70:
            category = "profanity"
        elif toxicity > 70:
            category = "highly_toxic"
        elif toxicity > 40:
            category = "moderately_toxic"
        else:
            category = "safe"
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = 95.0 if guard_result.get("guard_success") and llama_result.get("llama_success") else 70.0
        
        return {
            "is_malicious": is_malicious,
            "toxicity_score": round(toxicity, 2),
            "hate_speech_score": round(hate_speech, 2),
            "profanity_score": round(profanity, 2),
            "threat_score": round(threat, 2),
            "violence_score": round(violence, 2),
            "sexual_score": round(sexual, 2),
            "confidence_score": round(confidence, 2),
            "category": category,
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": {
                "is_safe": guard_result.get("is_safe", True),
                "violated_categories": violated_cats
            },
            "guard_categories": violated_cats,
            "llama_reasoning": llama_result.get("reasoning", "")
        }
    
    def _combine_results(self, rule_result: Dict, llama_result: Dict) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ í†µí•© (Llama 3.1ë§Œ)"""
        weight_rule = 0.3
        weight_llama = 0.7
        
        toxicity = (
            rule_result["rule_score"] * weight_rule +
            llama_result.get("toxicity_score", 0) * weight_llama
        )
        
        return {
            "is_malicious": toxicity > 50 or rule_result["is_malicious_rule"],
            "toxicity_score": round(toxicity, 2),
            "hate_speech_score": round(llama_result.get("hate_speech_score", 0), 2),
            "profanity_score": round(llama_result.get("profanity_score", 0), 2),
            "threat_score": round(llama_result.get("threat_score", 0), 2),
            "violence_score": round(llama_result.get("violence_score", 0), 2),
            "sexual_score": round(llama_result.get("sexual_score", 0), 2),
            "confidence_score": 85.0,
            "category": "toxic" if toxicity > 50 else "safe",
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": llama_result.get("reasoning", "")
        }
    
    def _fallback_guard_result(self) -> Dict[str, Any]:
        """Guard í´ë°±"""
        return {
            "is_safe": True,
            "violated_categories": [],
            "raw_response": "Guard unavailable",
            "guard_success": False
        }
    
    def _fallback_analysis(self, text: str) -> Dict[str, Any]:
        """Llama í´ë°±"""
        text_length = len(text)
        base_score = min(text_length * 2, 100)
        
        return {
            "toxicity_score": base_score,
            "hate_speech_score": max(0, base_score - 30),
            "profanity_score": max(0, base_score - 20),
            "threat_score": max(0, base_score - 40),
            "violence_score": max(0, base_score - 35),
            "sexual_score": max(0, base_score - 45),
            "reasoning": "Fallback analysis",
            "llama_success": False
        }
    
    def _create_fallback_response(self, text: str, rule_result: Dict) -> Dict[str, Any]:
        """ì™„ì „ í´ë°±"""
        score = rule_result["rule_score"]
        
        return {
            "is_malicious": rule_result["is_malicious_rule"],
            "toxicity_score": score,
            "hate_speech_score": max(0, score - 20),
            "profanity_score": max(0, score - 10),
            "threat_score": max(0, score - 30),
            "violence_score": max(0, score - 25),
            "sexual_score": max(0, score - 35),
            "confidence_score": 40.0,
            "category": "toxic" if score > 50 else "safe",
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": "Fallback: Rule-based only"
        }
    
    def _extract_json(self, text: str) -> Optional[Dict]:
        """JSON ì¶”ì¶œ"""
        try:
            json_match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            return json.loads(text)
        except:
            return None


# AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
analyzer = GroqDualModelAnalyzer()


# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘"""
    logger.info("=" * 60)
    logger.info("SNS Content Analyzer - Groq Dual Model")
    logger.info("=" * 60)
    
    if analyzer.api_key:
        logger.info("âœ“ Groq API configured")
        logger.info(f"  - Guard Model: {analyzer.models['guard']}")
        logger.info(f"  - Analysis Model: {analyzer.models['analysis']}")
        logger.info("  - Strategy: Guard filters â†’ Llama analyzes")
    else:
        logger.warning("âš  No API key - fallback mode")


@app.get("/")
async def root():
    """API ìƒíƒœ"""
    return {
        "service": "SNS Content Analyzer - Groq Dual Model",
        "status": "running",
        "version": "3.0.0",
        "models": {
            "guard": analyzer.models["guard"],
            "analysis": analyzer.models["analysis"]
        },
        "strategy": "Guard filters unsafe content â†’ Llama provides detailed analysis",
        "api_configured": bool(analyzer.api_key),
        "cost": "100% FREE",
        "features": [
            "Dual model analysis",
            "13 safety categories (Llama Guard)",
            "Detailed scoring (Llama 3.1)",
            "Parallel execution"
        ]
    }


@app.post("/analyze/text", response_model=AnalysisResponse)
async def analyze_text(request: TextAnalysisRequest):
    """
    í…ìŠ¤íŠ¸ ë¶„ì„ (ë“€ì–¼ ëª¨ë¸)
    
    - **use_dual_model=True**: Guard + Llama 3.1 (ë” ì •í™•, ì•½ê°„ ëŠë¦¼)
    - **use_dual_model=False**: Llama 3.1ë§Œ (ë¹ ë¦„)
    """
    logger.info(f"Analyzing text (length: {len(request.text)}, dual: {request.use_dual_model})")
    result = await analyzer.analyze_text(
        request.text, 
        request.language,
        request.use_dual_model
    )
    return result


@app.post("/analyze/batch")
async def analyze_batch(
    texts: List[str], 
    language: str = "ko",
    use_dual_model: bool = True
):
    """ëŒ€ëŸ‰ ë¶„ì„"""
    logger.info(f"Batch analysis: {len(texts)} texts")
    
    results = []
    for text in texts[:10]:
        try:
            result = await analyzer.analyze_text(text, language, use_dual_model)
            results.append(result)
        except Exception as e:
            logger.error(f"Failed: {e}")
            results.append(None)
    
    return {
        "total": len(results),
        "results": results,
        "dual_model": use_dual_model,
        "processed_at": datetime.now().isoformat()
    }


@app.get("/models/info")
async def models_info():
    """ëª¨ë¸ ì •ë³´"""
    return {
        "guard_model": {
            "name": analyzer.models["guard"],
            "purpose": "Safety filtering",
            "categories": analyzer.guard_categories,
            "speed": "~100ms"
        },
        "analysis_model": {
            "name": analyzer.models["analysis"],
            "purpose": "Detailed analysis",
            "features": ["Scoring", "Reasoning", "Multi-category"],
            "speed": "~50ms"
        },
        "strategy": "Parallel execution for speed"
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "api_configured": bool(analyzer.api_key),
        "models_ready": True,
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("SNS Content Analyzer - Groq Dual Model Edition")
    print("=" * 60)
    print("\nğŸš€ Models:")
    print(f"  1. {analyzer.models['guard']} - Safety filtering")
    print(f"  2. {analyzer.models['analysis']} - Detailed analysis")
    print("\nâš¡ Strategy:")
    print("  - Parallel execution (both models run simultaneously)")
    print("  - Guard: 13 safety categories")
    print("  - Llama 3.1: Detailed scoring + reasoning")
    print("\nğŸ’° Cost: 100% FREE")
    print("  - Rate limit: 30 req/min, 14,400 req/day")
    print("\nğŸ”‘ Setup:")
    print("  export GROQ_API_KEY=your_key")
    print("  python main_groq_dual.py")
    print("\nì„œë²„ ì‹œì‘ ì¤‘...\n")
    
    uvicorn.run(
        "main_groq_dual:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )

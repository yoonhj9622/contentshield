"""
SNS Content Analyzer - Groq Dual Model Edition
Llama-Guard-4-12b (ÌïÑÌÑ∞ÎßÅ) + Llama-3.1-8b-instant (Î∂ÑÏÑù)
+ AI Writing Assistant Í∏∞Îä• Ï∂îÍ∞Ä
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import logging
import os
from datetime import datetime
import httpx
import json
import re
import asyncio
from dotenv import load_dotenv  # ‚ú® Ï∂îÍ∞Ä

# ‚ú® .env ÌååÏùº Î°úÎìú
load_dotenv()

# API ÌÇ§ Î°úÎìú ÌôïÏù∏
api_key_loaded = bool(os.getenv("GROQ_API_KEY"))
print(f"GROQ_API_KEY loaded: {api_key_loaded}")

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI Ïï± ÏÉùÏÑ±
app = FastAPI(
    title="SNS Content Analyzer - Groq Dual Model + AI Assistant",
    description="Llama Guard 4 + Llama 3.1 ÎìÄÏñº Î™®Îç∏ ÏïÖÏÑ± ÏΩòÌÖêÏ∏† ÌÉêÏßÄ + AI ÏûëÏÑ± Î≥¥Ï°∞",
    version="3.1.0"
)

# CORS ÏÑ§Ï†ï
app.add_middleware(
    CORSMiddleware,
    # ÌòÑÏû¨ (Î™®Îì† ÎèÑÎ©îÏù∏ ÌóàÏö© - Í∞úÎ∞úÏö©)
    allow_origins=["*"],
    # Î∞∞Ìè¨Ï†Ñ
    #allow_origins=["http://localhost:3000", "https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ ====================

class TextAnalysisRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=10000)
    language: str = Field(default="ko")
    use_dual_model: bool = Field(default=True, description="Îëê Î™®Îç∏ Î™®Îëê ÏÇ¨Ïö© Ïó¨Î∂Ä")
    custom_blocked_words: List[str] = Field(default=[], description="ÏÇ¨Ïö©Ïûê Ï†ïÏùò Ï∞®Îã® Îã®Ïñ¥")  # Ï∂îÍ∞Ä!

class AnalysisResponse(BaseModel):
    is_malicious: bool
    is_blocked: bool = False  # Ï∂îÍ∞Ä! ÏÇ¨Ïö©Ïûê Ï∞®Îã® Îã®Ïñ¥ Ìè¨Ìï® Ïó¨Î∂Ä
    blocked_words_found: List[str] = []  # Ï∂îÍ∞Ä! Î∞úÍ≤¨Îêú ÏÇ¨Ïö©Ïûê Ï∞®Îã® Îã®Ïñ¥
    status: str = "clean"  # Ï∂îÍ∞Ä! "clean", "malicious", "blocked"
    toxicity_score: float
    hate_speech_score: float
    profanity_score: float
    threat_score: float
    violence_score: float
    sexual_score: float
    confidence_score: float
    category: str
    detected_keywords: List[str]
    
    # Guard Î™®Îç∏ Í≤∞Í≥º
    guard_result: Optional[Dict[str, Any]] = None
    guard_categories: List[str] = []
    
    # Llama 3.1 Í≤∞Í≥º
    llama_reasoning: Optional[str] = None
    
    ai_model_version: str
    processing_time_ms: float
    analyzed_at: str


# ==================== üÜï AI Assistant Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ ====================

class AssistantAnalyzeRequest(BaseModel):
    """ÏõêÎ≥∏ ÌÖçÏä§Ìä∏ Î∂ÑÏÑù ÏöîÏ≤≠"""
    text: str = Field(..., min_length=1, max_length=10000)
    language: str = Field(default="ko")


class AssistantImproveRequest(BaseModel):
    """ÌÖçÏä§Ìä∏ Í∞úÏÑ† ÏöîÏ≤≠"""
    text: str = Field(..., min_length=1, max_length=10000)
    tone: str = Field(default="polite", description="polite, neutral, friendly, formal, casual")
    language: str = Field(default="ko")
    instruction: Optional[str] = Field(default=None, description="Ï∂îÍ∞Ä ÏßÄÏãúÏÇ¨Ìï≠")


class AssistantReplyRequest(BaseModel):
    """ÎåìÍ∏Ä ÎãµÎ≥Ä ÏÉùÏÑ± ÏöîÏ≤≠"""
    original_comment: str = Field(..., min_length=1, max_length=1000)
    context: Optional[str] = Field(default=None, description="ÏòÅÏÉÅ/Í≤åÏãúÍ∏Ä ÎÇ¥Ïö©")
    reply_type: str = Field(default="constructive", description="constructive, grateful, apologetic, defensive")
    language: str = Field(default="ko")


class AssistantTemplateRequest(BaseModel):
    """ÏÉÅÌô©Î≥Ñ ÌÖúÌîåÎ¶ø ÏÉùÏÑ± ÏöîÏ≤≠"""
    situation: str = Field(..., description="promotion, announcement, apology, explanation, feedback_request")
    topic: Optional[str] = Field(default=None, description="Ï£ºÏ†ú/ÏÉÅÌô© ÏÑ§Î™Ö")
    tone: str = Field(default="professional")
    language: str = Field(default="ko")


class QuickAnalysis(BaseModel):
    """Í∞ÑÎã® Î∂ÑÏÑù Í≤∞Í≥º"""
    emotion_tone: str  # "Í∏çÏ†ïÏ†Å", "Ï§ëÎ¶ΩÏ†Å", "Î∂ÄÏ†ïÏ†Å"
    risk_level: str    # "ÏïàÏ†Ñ", "Ï£ºÏùò", "ÏúÑÌóò"
    has_profanity: bool
    has_aggression: bool
    misunderstanding_risk: str  # "ÏóÜÏùå", "ÎÇÆÏùå", "ÏûàÏùå", "ÎÜíÏùå"


class SuggestionOption(BaseModel):
    """AI Ï†úÏïà ÏòµÏÖò"""
    version: int
    text: str
    tone: str
    reasoning: str
    confidence: float


class AssistantResponse(BaseModel):
    """AI Assistant ÌÜµÌï© ÏùëÎãµ"""
    success: bool
    analysis: Optional[QuickAnalysis] = None
    suggestions: List[SuggestionOption] = []
    processing_time_ms: float
    model_used: str


# ==================== Í∏∞Ï°¥ Groq Dual Model Analyzer (Ïú†ÏßÄ) ====================

class GroqDualModelAnalyzer:
    """Llama Guard 4 + Llama 3.1 ÎìÄÏñº Î™®Îç∏ Î∂ÑÏÑù ÏÑúÎπÑÏä§"""
    
    def __init__(self):
        self.model_version = "groq-dual-llama-guard4-llama3.1"
        self.api_key = os.getenv("GROQ_API_KEY")
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è GROQ_API_KEY not set")
            logger.warning("  Get FREE API key: https://console.groq.com/keys")
        else:
            logger.info("‚úì Groq API key configured")
        
        # Î™®Îç∏ ÏÑ§Ï†ï
        self.models = {
            "guard": "llama-guard-3-8b",
            "analysis": "llama-3.1-8b-instant"
        }
        
        # Guard Ïπ¥ÌÖåÍ≥†Î¶¨ Îß§Ìïë
        self.guard_categories = {
            "S1": "violent_crimes",
            "S2": "non_violent_crimes",
            "S3": "sex_related_crimes",
            "S4": "child_exploitation",
            "S5": "defamation",
            "S6": "specialized_advice",
            "S7": "privacy",
            "S8": "intellectual_property",
            "S9": "indiscriminate_weapons",
            "S10": "hate",
            "S11": "self_harm",
            "S12": "sexual_content",
            "S13": "elections"
        }
        
        # Í∑úÏπô Í∏∞Î∞ò Ï∞®Îã® Îã®Ïñ¥
        self.blocked_words = {
            "ko": [
                "Î∞îÎ≥¥", "Î©çÏ≤≠Ïù¥", "Î≥ëÏã†", "Í∞úÏÉàÎÅº", "Ïî®Î∞ú", "ÏßÄÎûÑ", "ÎØ∏Ïπú",
                "Ï£ΩÏó¨", "Ï£ΩÏùº", "ÎïåÎ†§", "ÌòêÏò§", "Ï∞®Î≥Ñ", "Í∫ºÏ†∏", "Îã•Ï≥ê","Í∞úÏûêÏãù","ÏñëÏïÑÏπò"
            ],
            "en": [
                "stupid", "idiot", "fuck", "shit", "kill", "hate", "damn"
            ]
        }
        
        logger.info("Groq Dual Model Analyzer initialized")
        logger.info(f"  - Guard Model: {self.models['guard']}")
        logger.info(f"  - Analysis Model: {self.models['analysis']}")
    
    async def analyze_text(
        self, 
        text: str, 
        language: str = "ko",
        use_dual_model: bool = True,
        custom_blocked_words: List[str] = None  # Ï∂îÍ∞Ä!
    ) -> AnalysisResponse:
        """ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (ÎìÄÏñº Î™®Îç∏)"""
        import time
        start_time = time.time()
        
        try:
            # 1. Í∑úÏπô Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (ÏÇ¨Ïö©Ïûê Ï∞®Îã® Îã®Ïñ¥ Ìè¨Ìï®)
            rule_result = self._rule_based_filter(text, language, custom_blocked_words or [])
            
            if not self.api_key:
                logger.warning("No API key, using fallback")
                result = self._create_fallback_response(text, rule_result)
            elif use_dual_model:
                # 2. ÎìÄÏñº Î™®Îç∏ Î∂ÑÏÑù
                result = await self._dual_model_analysis(text, language, rule_result)
            else:
                # 3. Îã®Ïùº Î™®Îç∏ Î∂ÑÏÑù
                result = await self._single_model_analysis(text, language, rule_result)
            
            # ÏÇ¨Ïö©Ïûê Ï∞®Îã® Îã®Ïñ¥ Ï≤òÎ¶¨ Ï∂îÍ∞Ä
            result["is_blocked"] = rule_result.get("is_blocked_by_user", False)
            result["blocked_words_found"] = rule_result.get("user_blocked_words_found", [])
            
            # status Í≤∞Ï†ï
            if result["is_blocked"]:
                result["status"] = "blocked"
            elif result["is_malicious"]:
                result["status"] = "malicious"
            else:
                result["status"] = "clean"
            
            processing_time = (time.time() - start_time) * 1000
            result["processing_time_ms"] = round(processing_time, 2)
            result["analyzed_at"] = datetime.now().isoformat()
            result["ai_model_version"] = self.model_version
            
            return AnalysisResponse(**result)
        
        except Exception as e:
            logger.error(f"Analysis error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")
    
    async def _dual_model_analysis(
        self, 
        text: str, 
        language: str,
        rule_result: Dict
    ) -> Dict[str, Any]:
        """ÎìÄÏñº Î™®Îç∏ Î∂ÑÏÑù (Guard + Llama 3.1 Î≥ëÎ†¨ Ïã§Ìñâ)"""
        try:
            guard_task = self._llama_guard_check(text, language)
            llama_task = self._llama_analysis(text, language)
            
            guard_result, llama_result = await asyncio.gather(
                guard_task,
                llama_task,
                return_exceptions=True
            )
            
            if isinstance(guard_result, Exception):
                logger.error(f"Guard model failed: {guard_result}")
                guard_result = self._fallback_guard_result()
            
            if isinstance(llama_result, Exception):
                logger.error(f"Llama model failed: {llama_result}")
                llama_result = self._fallback_analysis(text)
            
            combined = self._combine_dual_results(
                rule_result,
                guard_result,
                llama_result
            )
            
            return combined
            
        except Exception as e:
            logger.error(f"Dual model analysis failed: {e}")
            return self._create_fallback_response(text, rule_result)
    
    async def _llama_guard_check(self, text: str, language: str) -> Dict[str, Any]:
        """Llama Guard 4 ÏïàÏ†ÑÏÑ± Í≤ÄÏÇ¨"""
        try:
            prompt = f"""[INST] Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Violent Crimes
S2: Non-Violent Crimes
S3: Sex-Related Crimes
S4: Child Sexual Exploitation
S5: Defamation
S6: Specialized Advice
S7: Privacy
S8: Intellectual Property
S9: Indiscriminate Weapons
S10: Hate
S11: Self-Harm
S12: Sexual Content
S13: Elections
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>
User: {text}
<END CONVERSATION>

Provide your safety assessment for User's message:
- First line must be 'safe' or 'unsafe'
- If unsafe, list all violated categories (S1, S2, etc.) [/INST]"""

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.models["guard"],
                        "messages": [
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.0,
                        "max_tokens": 100
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()
                
                is_safe = content.lower().startswith("safe")
                violated_categories = []
                
                if not is_safe:
                    categories = re.findall(r'S\d+', content)
                    violated_categories = [
                        self.guard_categories.get(cat, cat) 
                        for cat in categories
                    ]
                
                logger.info(f"Guard result: {'safe' if is_safe else 'unsafe'}, categories: {violated_categories}")
                
                return {
                    "is_safe": is_safe,
                    "violated_categories": violated_categories,
                    "raw_response": content,
                    "guard_success": True
                }
            else:
                logger.error(f"Guard API error: {response.status_code}")
                return self._fallback_guard_result()
                
        except Exception as e:
            logger.error(f"Guard check failed: {e}")
            return self._fallback_guard_result()
    
    async def _llama_analysis(self, text: str, language: str) -> Dict[str, Any]:
        """Llama 3.1 ÏÉÅÏÑ∏ Î∂ÑÏÑù"""
        try:
            system_prompt = """You are an expert in analyzing toxic and harmful content.
Analyze the given text and provide detailed scores (0-100) for each category.

Respond in valid JSON format only, no markdown:
{
  "toxicity_score": <0-100>,
  "hate_speech_score": <0-100>,
  "profanity_score": <0-100>,
  "threat_score": <0-100>,
  "violence_score": <0-100>,
  "sexual_score": <0-100>,
  "reasoning": "<brief explanation in same language as input>"
}"""

            user_prompt = f'Analyze this text for harmful content: "{text}"'
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.models["analysis"],
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.1,
                        "max_tokens": 300
                    }
                )
            
            if response.status_code == 200:
                # [Rate Limit Logging]
                remaining_tokens = response.headers.get("x-ratelimit-remaining-tokens", "unknown")
                remaining_requests = response.headers.get("x-ratelimit-remaining-requests", "unknown")
                logger.info(f"‚ö° Groq Rate Limit Info: Remaining Tokens={remaining_tokens}, Requests={remaining_requests}")

                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                json_result = self._extract_json(content)
                
                if json_result:
                    logger.info(f"Llama analysis: toxicity={json_result.get('toxicity_score', 0)}")
                    
                    return {
                        "toxicity_score": json_result.get("toxicity_score", 0),
                        "hate_speech_score": json_result.get("hate_speech_score", 0),
                        "profanity_score": json_result.get("profanity_score", 0),
                        "threat_score": json_result.get("threat_score", 0),
                        "violence_score": json_result.get("violence_score", 0),
                        "sexual_score": json_result.get("sexual_score", 0),
                        "reasoning": json_result.get("reasoning", ""),
                        "llama_success": True
                    }
                else:
                    logger.warning("Failed to parse Llama response")
                    return self._fallback_analysis(text)
            else:
                logger.error(f"Llama API error: {response.status_code}")
                return self._fallback_analysis(text)
                
        except Exception as e:
            logger.error(f"Llama analysis failed: {e}")
            return self._fallback_analysis(text)
    
    async def _single_model_analysis(
        self,
        text: str,
        language: str,
        rule_result: Dict
    ) -> Dict[str, Any]:
        """Îã®Ïùº Î™®Îç∏ Î∂ÑÏÑù (Llama 3.1Îßå ÏÇ¨Ïö©)"""
        llama_result = await self._llama_analysis(text, language)
        return self._combine_results(rule_result, llama_result)
    
    def _rule_based_filter(self, text: str, language: str, custom_blocked_words: List[str] = None) -> Dict[str, Any]:
        """Í∑úÏπô Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (ÏÇ¨Ïö©Ïûê Ï∞®Îã® Îã®Ïñ¥ Ìè¨Ìï®)"""
        detected = []
        user_blocked_found = []
        score = 0.0
        
        # Í∏∞Î≥∏ Ï∞®Îã® Îã®Ïñ¥ Ï≤¥ÌÅ¨
        words = self.blocked_words.get(language, [])
        text_lower = text.lower()
        
        for word in words:
            if word in text_lower:
                detected.append(word)
                score += 25.0
        
        # ÏÇ¨Ïö©Ïûê Ï†ïÏùò Ï∞®Îã® Îã®Ïñ¥ Ï≤¥ÌÅ¨
        if custom_blocked_words:
            for word in custom_blocked_words:
                if word.lower() in text_lower:
                    user_blocked_found.append(word)
                    # ÏÇ¨Ïö©Ïûê Ï∞®Îã® Îã®Ïñ¥Îäî Î≥ÑÎèÑÎ°ú Ï≤òÎ¶¨ (Ï†êÏàòÏóê Ï∂îÍ∞ÄÌïòÏßÄ ÏïäÏùå)
        
        return {
            "detected_keywords": detected,
            "rule_score": min(score, 100.0),
            "is_malicious_rule": score > 50.0,
            "is_blocked_by_user": len(user_blocked_found) > 0,  # Ï∂îÍ∞Ä!
            "user_blocked_words_found": user_blocked_found  # Ï∂îÍ∞Ä!
        }
    
    def _combine_dual_results(
        self,
        rule_result: Dict,
        guard_result: Dict,
        llama_result: Dict
    ) -> Dict[str, Any]:
        """ÎìÄÏñº Î™®Îç∏ Í≤∞Í≥º ÌÜµÌï©"""
        
        guard_boost = 0
        if not guard_result.get("is_safe", True):
            guard_boost = 30
        
        weight_rule = 0.15
        weight_guard = 0.35
        weight_llama = 0.50
        
        toxicity = (
            rule_result["rule_score"] * weight_rule +
            guard_boost * weight_guard +
            llama_result.get("toxicity_score", 0) * weight_llama
        )
        
        hate_speech = llama_result.get("hate_speech_score", 0)
        profanity = llama_result.get("profanity_score", 0)
        threat = llama_result.get("threat_score", 0)
        violence = llama_result.get("violence_score", 0)
        sexual = llama_result.get("sexual_score", 0)
        
        violated_cats = guard_result.get("violated_categories", [])
        if "hate" in violated_cats:
            hate_speech = max(hate_speech, 80)
        if "violent_crimes" in violated_cats:
            violence = max(violence, 85)
        if "sexual_content" in violated_cats:
            sexual = max(sexual, 85)
        
        is_malicious = (
            toxicity > 0 or  # STRICT POLICY: Any score > 0 is malicious
            hate_speech > 60.0 or
            profanity > 70.0 or
            threat > 40.0 or
            violence > 60.0 or
            sexual > 70.0 or
            not guard_result.get("is_safe", True) or
            rule_result["is_malicious_rule"]
        )
        
        if violence > 70:
            category = "violence"
        elif sexual > 70:
            category = "sexual_content"
        elif threat > 60:
            category = "threat"
        elif hate_speech > 60:
            category = "hate_speech"
        elif profanity > 70:
            category = "profanity"
        elif toxicity > 70:
            category = "highly_toxic"
        elif toxicity > 0:  # STRICT POLICY
            category = "moderately_toxic"
        else:
            category = "safe"
        
        confidence = 95.0 if guard_result.get("guard_success") and llama_result.get("llama_success") else 70.0
        
        return {
            "is_malicious": is_malicious,
            "toxicity_score": round(toxicity, 2),
            "hate_speech_score": round(hate_speech, 2),
            "profanity_score": round(profanity, 2),
            "threat_score": round(threat, 2),
            "violence_score": round(violence, 2),
            "sexual_score": round(sexual, 2),
            "confidence_score": round(confidence, 2),
            "category": category,
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": {
                "is_safe": guard_result.get("is_safe", True),
                "violated_categories": violated_cats
            },
            "guard_categories": violated_cats,
            "llama_reasoning": llama_result.get("reasoning", "")
        }
    
    def _combine_results(self, rule_result: Dict, llama_result: Dict) -> Dict[str, Any]:
        """Îã®Ïùº Î™®Îç∏ Í≤∞Í≥º ÌÜµÌï© (Llama 3.1Îßå)"""
        weight_rule = 0.3
        weight_llama = 0.7
        
        toxicity = (
            rule_result["rule_score"] * weight_rule +
            llama_result.get("toxicity_score", 0) * weight_llama
        )
        
        return {
            "is_malicious": toxicity > 50 or rule_result["is_malicious_rule"],
            "toxicity_score": round(toxicity, 2),
            "hate_speech_score": round(llama_result.get("hate_speech_score", 0), 2),
            "profanity_score": round(llama_result.get("profanity_score", 0), 2),
            "threat_score": round(llama_result.get("threat_score", 0), 2),
            "violence_score": round(llama_result.get("violence_score", 0), 2),
            "sexual_score": round(llama_result.get("sexual_score", 0), 2),
            "confidence_score": 85.0,
            "category": "toxic" if toxicity > 50 else "safe",
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": llama_result.get("reasoning", "")
        }
    
    def _fallback_guard_result(self) -> Dict[str, Any]:
        """Guard Ìè¥Î∞±"""
        return {
            "is_safe": True,
            "violated_categories": [],
            "raw_response": "Guard unavailable",
            "guard_success": False
        }
    
    def _fallback_analysis(self, text: str) -> Dict[str, Any]:
        """Llama Ìè¥Î∞±"""
        # API Ïò§Î•ò Ïãú 'ÏïàÏ†Ñ'ÏúºÎ°ú Ï≤òÎ¶¨ (Í∏∏Ïù¥ Í∏∞Î∞ò ÌåêÏ†ï Ï†úÍ±∞)
        return {
            "toxicity_score": 0,
            "hate_speech_score": 0,
            "profanity_score": 0,
            "threat_score": 0,
            "violence_score": 0,
            "sexual_score": 0,
            "reasoning": "Fallback analysis (API Unavailable) - Assumed Safe",
            "llama_success": False
        }
    
    def _create_fallback_response(self, text: str, rule_result: Dict) -> Dict[str, Any]:
        """ÏôÑÏ†Ñ Ìè¥Î∞±"""
        score = rule_result["rule_score"]
        
        return {
            "is_malicious": rule_result["is_malicious_rule"],
            "toxicity_score": score,
            "hate_speech_score": max(0, score - 20),
            "profanity_score": max(0, score - 10),
            "threat_score": max(0, score - 30),
            "violence_score": max(0, score - 25),
            "sexual_score": max(0, score - 35),
            "confidence_score": 40.0,
            "category": "toxic" if score > 50 else "safe",
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": "Fallback: Rule-based only"
        }
    
    def _extract_json(self, text: str) -> Optional[Dict]:
        """JSON Ï∂îÏ∂ú (Groq ÏùëÎãµ Ï†ÑÏö©)"""
        try:
            # 1. Markdown ÏΩîÎìú Î∏îÎ°ù Ï†úÍ±∞
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'\s*```', '', text)
            text = text.strip()

            # 2. ÏßÅÏ†ë ÌååÏã± ÏãúÎèÑ
            try:
                result = json.loads(text)
                logger.info(f"‚úÖ JSON ÏßÅÏ†ë ÌååÏã± ÏÑ±Í≥µ: {len(result.get('suggestions', []))}Í∞ú Ï†úÏïà")
                return result
            except json.JSONDecodeError as e:
                logger.warning(f"‚ö†Ô∏è ÏßÅÏ†ë ÌååÏã± Ïã§Ìå®: {e}")
        
            # 3. Í∞ÄÏû• ÌÅ∞ JSON Í∞ùÏ≤¥ Ï∞æÍ∏∞ (Ï§ëÏ≤© Íµ¨Ï°∞ ÏßÄÏõê)
            max_json = None
            max_length = 0
        
            # Î™®Îì† { ÏúÑÏπò Ï∞æÍ∏∞
            for i in range(len(text)):
                if text[i] == '{':
                    # Ïù¥ ÏúÑÏπòÏóêÏÑú ÏãúÏûëÌïòÎäî ÏôÑÏ†ÑÌïú JSON Ï∞æÍ∏∞
                    depth = 0
                    for j in range(i, len(text)):
                        if text[j] == '{':
                            depth += 1
                        elif text[j] == '}':
                            depth -= 1
                            if depth == 0:
                                # ÏôÑÏ†ÑÌïú JSON Î∞úÍ≤¨
                                json_str = text[i:j+1]
                                try:
                                    parsed = json.loads(json_str)
                                    if len(json_str) > max_length:
                                        max_json = parsed
                                        max_length = len(json_str)
                                except:
                                    pass
                                break
        
            if max_json:
                logger.info(f"‚úÖ Ï§ëÍ¥ÑÌò∏ Îß§Ïπ≠ÏúºÎ°ú ÌååÏã± ÏÑ±Í≥µ: {len(max_json.get('suggestions', []))}Í∞ú Ï†úÏïà")
                return max_json
        
            logger.error(f"‚ùå JSON ÌååÏã± Ïã§Ìå® - Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Í∏∏Ïù¥: {len(text)}")
            logger.error(f"‚ùå ÌÖçÏä§Ìä∏ ÏãúÏûë Î∂ÄÎ∂Ñ: {text[:500]}")
            return None
        
        except Exception as e:
            logger.error(f"‚ùå JSON ÌååÏã± ÏòàÏô∏: {e}")
            return None

# ==================== üÜï AI Writing Assistant Service ====================

class AIWritingAssistant:
    """AI ÏûëÏÑ± Î≥¥Ï°∞ ÏÑúÎπÑÏä§ (Llama 3.1 Í∏∞Î∞ò)"""
    
    def __init__(self, analyzer: GroqDualModelAnalyzer):
        self.analyzer = analyzer
        self.api_key = analyzer.api_key
        self.base_url = analyzer.base_url
        self.model = analyzer.models["analysis"]  # Llama-3.1-8b-instant
        
        # ÌÜ§ Ïï§ Îß§ÎÑà ÌïúÍµ≠Ïñ¥ Îß§Ìïë
        self.tone_mapping = {
            "polite": "Í≥µÏÜêÌïòÍ≥† Ï†ïÏ§ëÌïú",
            "neutral": "Ï§ëÎ¶ΩÏ†ÅÏù¥Í≥† Í∞ùÍ¥ÄÏ†ÅÏù∏",
            "friendly": "ÏπúÍ∑ºÌïòÍ≥† Îî∞ÎúªÌïú",
            "formal": "Í≤©ÏãùÏûàÍ≥† Ï†ÑÎ¨∏Ï†ÅÏù∏",
            "casual": "Ìé∏ÏïàÌïòÍ≥† ÏûêÏó∞Ïä§Îü¨Ïö¥"
        }
        
        # ÏÉÅÌô©Î≥Ñ ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø
        self.situation_templates = {
            "promotion": "ÌôçÎ≥¥/ÎßàÏºÄÌåÖ Í≤åÏãúÍ∏Ä",
            "announcement": "Ìå¨ Í≥µÏßÄ/ÏïàÎÇ¥ Î©îÏãúÏßÄ",
            "apology": "ÏÇ¨Í≥º Î∞è Ìï¥Î™Ö",
            "explanation": "ÏÉÅÌô© ÏÑ§Î™Ö",
            "feedback_request": "Í±¥ÏÑ§Ï†Å ÌîºÎìúÎ∞± ÏöîÏ≤≠"
        }
        
        logger.info("AI Writing Assistant initialized")
    
    async def quick_analyze(
        self, 
        text: str, 
        language: str = "ko"
    ) -> QuickAnalysis:
        """Îπ†Î•∏ Í∞êÏ†ï/ÏúÑÌóòÎèÑ Î∂ÑÏÑù"""
        try:
            # Í∏∞Ï°¥ analyzer ÌôúÏö© (Guard + Llama 3.1)
            analysis_result = await self.analyzer.analyze_text(text, language, use_dual_model=True)
            
            # Í∞êÏ†ï ÌÜ§ ÌåêÎ≥Ñ
            if analysis_result.toxicity_score > 60:
                emotion_tone = "Î∂ÄÏ†ïÏ†Å"
            elif analysis_result.toxicity_score < 30:
                emotion_tone = "Í∏çÏ†ïÏ†Å"
            else:
                emotion_tone = "Ï§ëÎ¶ΩÏ†Å"
            
            # ÏúÑÌóòÎèÑ ÌåêÎ≥Ñ
            if analysis_result.is_malicious or analysis_result.toxicity_score > 70:
                risk_level = "ÏúÑÌóò"
            elif analysis_result.toxicity_score > 40:
                risk_level = "Ï£ºÏùò"
            else:
                risk_level = "ÏïàÏ†Ñ"
            
            # Ïò§Ìï¥ Í∞ÄÎä•ÏÑ± ÌåêÎ≥Ñ
            if analysis_result.toxicity_score > 50:
                misunderstanding_risk = "ÎÜíÏùå"
            elif analysis_result.toxicity_score > 30:
                misunderstanding_risk = "ÏûàÏùå"
            elif analysis_result.toxicity_score > 15:
                misunderstanding_risk = "ÎÇÆÏùå"
            else:
                misunderstanding_risk = "ÏóÜÏùå"
            
            return QuickAnalysis(
                emotion_tone=emotion_tone,
                risk_level=risk_level,
                has_profanity=analysis_result.profanity_score > 60,
                has_aggression=analysis_result.threat_score > 50 or analysis_result.violence_score > 50,
                misunderstanding_risk=misunderstanding_risk
            )
            
        except Exception as e:
            logger.error(f"Quick analysis failed: {e}")
            # Ìè¥Î∞±
            return QuickAnalysis(
                emotion_tone="Ï§ëÎ¶ΩÏ†Å",
                risk_level="ÏïàÏ†Ñ",
                has_profanity=False,
                has_aggression=False,
                misunderstanding_risk="ÏóÜÏùå"
            )
    
    async def improve_text(
        self,
        text: str,
        tone: str = "polite",
        language: str = "ko",
        instruction: Optional[str] = None
    ) -> List[SuggestionOption]:
        """ÌÖçÏä§Ìä∏ Í∞úÏÑ† (3Í∞ÄÏßÄ Î≤ÑÏ†Ñ ÏÉùÏÑ±)"""
        try:
            # ‚ú® Ïù¥ Î°úÍ∑∏ Ï∂îÍ∞Ä
            logger.info(f"üîÑ Starting text improvement: text='{text[:30]}...', tone={tone}")
            tone_ko = self.tone_mapping.get(tone, "Í≥µÏÜêÌïòÍ≥† Ï†ïÏ§ëÌïú")
            
            # ÌîÑÎ°¨ÌîÑÌä∏ ÏûëÏÑ±
            system_prompt = f"""ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ ÏΩòÌÖêÏ∏† ÏóêÎîîÌÑ∞ÏûÖÎãàÎã§. 
ÏÇ¨Ïö©ÏûêÏùò ÌÖçÏä§Ìä∏Î•º {tone_ko} ÌÜ§ÏúºÎ°ú Í∞úÏÑ†ÌïòÏó¨ 3Í∞ÄÏßÄ Îã§Î•∏ Î≤ÑÏ†ÑÏùÑ Ï†úÏïàÌïòÏÑ∏Ïöî.

ÏöîÍµ¨ÏÇ¨Ìï≠:
1. ÏõêÎ¨∏Ïùò ÌïµÏã¨ ÏùòÎØ∏Îäî Ïú†ÏßÄ
2. Ïò§Ìï¥Ïùò ÏÜåÏßÄÍ∞Ä ÏóÜÎèÑÎ°ù Î™ÖÌôïÌïòÍ≤å ÌëúÌòÑ
3. ÏöïÏÑ§, Í≥µÍ≤©Ï†Å ÌëúÌòÑ Ï†úÍ±∞
4. 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏùÄ Í∞ÅÍ∞Å Îã§Î•∏ Í∞ïÎèÑ/Ïä§ÌÉÄÏùºÎ°ú ÏûëÏÑ±
5. Ïú†ÌäúÎ∏å ÎåìÍ∏Ä/Ïª§ÎÆ§ÎãàÌã∞ Í≤åÏãúÍ∏ÄÏóê Ï†ÅÌï©Ìïú Í∏∏Ïù¥ (2-5Ï§Ñ)

ÏùëÎãµ ÌòïÏãù (JSONÎßå):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "Í∞úÏÑ†Îêú ÌÖçÏä§Ìä∏ Î≤ÑÏ†Ñ 1 (Í∞ÄÏû• Í≥µÏÜêÌï®)",
      "tone": "Îß§Ïö∞ Í≥µÏÜê",
      "reasoning": "Í∞úÏÑ† Ïù¥Ïú† ÏÑ§Î™Ö",
      "confidence": 0.95
    }},
    {{
      "version": 2,
      "text": "Í∞úÏÑ†Îêú ÌÖçÏä§Ìä∏ Î≤ÑÏ†Ñ 2 (Ï§ëÍ∞Ñ)",
      "tone": "Ï§ëÎ¶ΩÏ†Å",
      "reasoning": "Í∞úÏÑ† Ïù¥Ïú† ÏÑ§Î™Ö",
      "confidence": 0.90
    }},
    {{
      "version": 3,
      "text": "Í∞úÏÑ†Îêú ÌÖçÏä§Ìä∏ Î≤ÑÏ†Ñ 3 (ÏπúÍ∑ºÌï®)",
      "tone": "ÏπúÍ∑ºÌï®",
      "reasoning": "Í∞úÏÑ† Ïù¥Ïú† ÏÑ§Î™Ö",
      "confidence": 0.88
    }}
  ]
}}"""

            user_prompt = f"""ÏõêÎ≥∏ ÌÖçÏä§Ìä∏: "{text}"
{'Ï∂îÍ∞Ä ÏßÄÏãúÏÇ¨Ìï≠: ' + instruction if instruction else ''}

ÏúÑ ÌÖçÏä§Ìä∏Î•º {tone_ko} ÌÜ§ÏúºÎ°ú 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏúºÎ°ú Í∞úÏÑ†Ìï¥Ï£ºÏÑ∏Ïöî."""

            async with httpx.AsyncClient(timeout=30.0) as client:
                # ‚ú® Ïù¥ Î°úÍ∑∏ Ï∂îÍ∞Ä
                logger.info(f"üì§ Sending request to Groq API...")
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 1500
                    }
                )
                # ‚ú® Ïù¥ Î°úÍ∑∏ Ï∂îÍ∞Ä
                logger.info(f"üì• Groq API response: status={response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                # ‚ú® Ïù¥ Î°úÍ∑∏ Ï∂îÍ∞Ä
                # Ï†ÑÏ≤¥ ÏùëÎãµÏùÑ ÌååÏùºÎ°ú Ï†ÄÏû• (ÎîîÎ≤ÑÍπÖÏö©)
                logger.info(f"üìù Groq response length: {len(content)} characters")
                # Ï†ÑÏ≤¥ ÎÇ¥Ïö©ÏùÄ ÌååÏã±Îßå ÌïòÍ≥† Î°úÍ∑∏Îäî Ïïà Ìï® (ÎÑàÎ¨¥ Í∏∏Ïñ¥ÏÑú)
                
                # JSON ÌååÏã±
                json_result = self.analyzer._extract_json(content)
                
                if json_result and "suggestions" in json_result:
                    suggestions = []
                    for item in json_result["suggestions"]:
                        suggestions.append(SuggestionOption(
                            version=item.get("version", 1),
                            text=item.get("text", ""),
                            tone=item.get("tone", tone),
                            reasoning=item.get("reasoning", ""),
                            confidence=item.get("confidence", 0.85)
                        ))
                    
                    logger.info(f"Generated {len(suggestions)} improved versions")
                    return suggestions
                else:
                    logger.warning("Failed to parse improvement response")
                    return self._fallback_improvement(text, tone)
            else:
                logger.error(f"Improvement API error: {response.status_code}")
                return self._fallback_improvement(text, tone)
                
        except Exception as e:
            logger.error(f"Text improvement failed: {e}")
            return self._fallback_improvement(text, tone)
    
    async def generate_reply(
        self,
        original_comment: str,
        context: Optional[str] = None,
        reply_type: str = "constructive",
        language: str = "ko"
    ) -> List[SuggestionOption]:
        """ÎåìÍ∏Ä ÎãµÎ≥Ä ÏÉùÏÑ± (3Í∞ÄÏßÄ Î≤ÑÏ†Ñ)"""
        try:
            # ÎãµÎ≥Ä Ïú†Ìòï Îß§Ìïë
            reply_types_ko = {
                "constructive": "Í±¥ÏÑ§Ï†ÅÏù¥Í≥† Î∞úÏ†ÑÏ†ÅÏù∏",
                "grateful": "Í∞êÏÇ¨ÌïòÍ≥† Í≤∏ÏÜêÌïú",
                "apologetic": "ÏÇ¨Í≥ºÌïòÍ≥† Ìï¥Î™ÖÌïòÎäî",
                "defensive": "Î∞©Ïñ¥Ï†ÅÏù¥ÏßÄÎßå ÏòàÏùòÏûàÎäî"
            }
            
            reply_tone = reply_types_ko.get(reply_type, "Í±¥ÏÑ§Ï†ÅÏù¥Í≥† Î∞úÏ†ÑÏ†ÅÏù∏")
            
            system_prompt = f"""ÎãπÏã†ÏùÄ Ïú†ÌäúÎ∏å ÌÅ¨Î¶¨ÏóêÏù¥ÌÑ∞Ïùò Ïª§ÎÆ§ÎãàÌã∞ Îß§ÎãàÏ†ÄÏûÖÎãàÎã§.
ÏïÖÏÑ± ÎåìÍ∏ÄÏù¥ÎÇò ÎπÑÌåêÏ†Å ÎåìÍ∏ÄÏóê ÎåÄÌï¥ {reply_tone} ÎãµÎ≥ÄÏùÑ 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏúºÎ°ú ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.

ÏõêÏπô:
1. Ï†àÎåÄ ÏöïÏÑ§Ïù¥ÎÇò Í≥µÍ≤©Ï†Å ÌëúÌòÑ ÏÇ¨Ïö© Í∏àÏßÄ
2. Ìå¨Îì§Í≥ºÏùò Í¥ÄÍ≥Ñ Ïú†ÏßÄÎ•º ÏµúÏö∞ÏÑ†ÏúºÎ°ú
3. Î≤ïÏ†Å Î¶¨Ïä§ÌÅ¨Í∞Ä ÏûàÎäî ÌëúÌòÑ ÌöåÌîº
4. Î∏åÎûúÎìú Ïù¥ÎØ∏ÏßÄ Î≥¥Ìò∏
5. Í∞Å Î≤ÑÏ†ÑÏùÄ Îã§Î•∏ Í∞ïÎèÑ/Ï†ëÍ∑ºÎ≤ï ÏÇ¨Ïö©

ÏùëÎãµ ÌòïÏãù (JSONÎßå):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "ÎãµÎ≥Ä Î≤ÑÏ†Ñ 1 (Í∞ÄÏû• Í≥µÏÜêÌïòÍ≥† Í≤∏ÏÜê)",
      "tone": "Îß§Ïö∞ Í≥µÏÜê",
      "reasoning": "Ïù¥ ÎãµÎ≥ÄÏùÑ ÏÑ†ÌÉùÌïú Ïù¥Ïú†",
      "confidence": 0.92
    }},
    {{
      "version": 2,
      "text": "ÎãµÎ≥Ä Î≤ÑÏ†Ñ 2 (Ï§ëÎ¶ΩÏ†Å)",
      "tone": "Ï§ëÎ¶ΩÏ†Å",
      "reasoning": "Ïù¥ ÎãµÎ≥ÄÏùÑ ÏÑ†ÌÉùÌïú Ïù¥Ïú†",
      "confidence": 0.88
    }},
    {{
      "version": 3,
      "text": "ÎãµÎ≥Ä Î≤ÑÏ†Ñ 3 (Î≤ïÏ†Å Í≤ΩÍ≥† Ìè¨Ìï®)",
      "tone": "Îã®Ìò∏ÌïòÏßÄÎßå ÏòàÏùòÏûàÏùå",
      "reasoning": "Ïù¥ ÎãµÎ≥ÄÏùÑ ÏÑ†ÌÉùÌïú Ïù¥Ïú†",
      "confidence": 0.85
    }}
  ]
}}"""

            context_text = f"\nÏòÅÏÉÅ/Í≤åÏãúÍ∏Ä ÎÇ¥Ïö©: {context}" if context else ""
            
            user_prompt = f"""ÏõêÎ≥∏ ÎåìÍ∏Ä: "{original_comment}"{context_text}

ÏúÑ ÎåìÍ∏ÄÏóê ÎåÄÌïú {reply_tone} ÎãµÎ≥ÄÏùÑ 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏúºÎ°ú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî."""

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 1500
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                logger.info(f"üìù Groq response length: {len(content)} characters")
                
                json_result = self.analyzer._extract_json(content)
                
                if json_result and "suggestions" in json_result:
                    suggestions = []
                    for item in json_result["suggestions"]:
                        suggestions.append(SuggestionOption(
                            version=item.get("version", 1),
                            text=item.get("text", ""),
                            tone=item.get("tone", reply_type),
                            reasoning=item.get("reasoning", ""),
                            confidence=item.get("confidence", 0.85)
                        ))
                    
                    logger.info(f"Generated {len(suggestions)} reply versions")
                    return suggestions
                else:
                    logger.warning("Failed to parse reply response")
                    return self._fallback_reply(original_comment, reply_type)
            else:
                logger.error(f"Reply API error: {response.status_code}")
                return self._fallback_reply(original_comment, reply_type)
                
        except Exception as e:
            logger.error(f"Reply generation failed: {e}")
            return self._fallback_reply(original_comment, reply_type)
    
    async def generate_template(
        self,
        situation: str,
        topic: Optional[str] = None,
        tone: str = "professional",
        language: str = "ko"
    ) -> List[SuggestionOption]:
        """ÏÉÅÌô©Î≥Ñ ÌÖúÌîåÎ¶ø ÏÉùÏÑ± (3Í∞ÄÏßÄ Î≤ÑÏ†Ñ)"""
        try:
            situation_ko = self.situation_templates.get(situation, "ÏùºÎ∞ò Í≤åÏãúÍ∏Ä")
            tone_ko = self.tone_mapping.get(tone, "Ï†ÑÎ¨∏Ï†ÅÏù∏")
            
            system_prompt = f"""ÎãπÏã†ÏùÄ ÏÜåÏÖú ÎØ∏ÎîîÏñ¥ ÏΩòÌÖêÏ∏† Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.
"{situation_ko}" ÏÉÅÌô©Ïóê ÎßûÎäî Í≤åÏãúÍ∏Ä/ÎåìÍ∏Ä ÌÖúÌîåÎ¶øÏùÑ {tone_ko} ÌÜ§ÏúºÎ°ú 3Í∞ÄÏßÄ Î≤ÑÏ†Ñ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.

ÏöîÍµ¨ÏÇ¨Ìï≠:
1. Ïú†ÌäúÎ∏å Ïª§ÎÆ§ÎãàÌã∞ Í≤åÏãúÍ∏Ä ÎòêÎäî ÎåìÍ∏ÄÎ°ú Ï†ÅÌï©
2. 3-7Ï§Ñ Í∏∏Ïù¥ (ÎÑàÎ¨¥ Í∏∏ÏßÄ ÏïäÍ≤å)
3. Ïù¥Î™®ÏßÄ ÏÇ¨Ïö© Í∞ÄÎä• (Ï†ÅÏ†àÌûà)
4. Í∞Å Î≤ÑÏ†ÑÏùÄ Îã§Î•∏ Ï†ëÍ∑ºÎ≤ï/Í∏∏Ïù¥ ÏÇ¨Ïö©
5. Î≤ïÏ†Å Î¶¨Ïä§ÌÅ¨ ÏóÜÎäî ÏïàÏ†ÑÌïú ÌëúÌòÑ

ÏùëÎãµ ÌòïÏãù (JSONÎßå):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "ÌÖúÌîåÎ¶ø Î≤ÑÏ†Ñ 1 (Í∞ÑÍ≤∞ÌïòÍ≥† ÌïµÏã¨Ï†Å)",
      "tone": "Í∞ÑÍ≤∞",
      "reasoning": "Ïù¥ ÌÖúÌîåÎ¶øÏùò ÌäπÏßï",
      "confidence": 0.90
    }},
    {{
      "version": 2,
      "text": "ÌÖúÌîåÎ¶ø Î≤ÑÏ†Ñ 2 (Ï§ëÍ∞Ñ Í∏∏Ïù¥, Í∞êÏ†ï ÌëúÌòÑ)",
      "tone": "Í∞êÏ†ïÏ†Å",
      "reasoning": "Ïù¥ ÌÖúÌîåÎ¶øÏùò ÌäπÏßï",
      "confidence": 0.88
    }},
    {{
      "version": 3,
      "text": "ÌÖúÌîåÎ¶ø Î≤ÑÏ†Ñ 3 (ÏÉÅÏÑ∏ÌïòÍ≥† Ï†ÑÎ¨∏Ï†Å)",
      "tone": "Ï†ÑÎ¨∏Ï†Å",
      "reasoning": "Ïù¥ ÌÖúÌîåÎ¶øÏùò ÌäπÏßï",
      "confidence": 0.85
    }}
  ]
}}"""

            topic_text = f"\nÏ£ºÏ†ú/ÏÉÅÌô©: {topic}" if topic else ""
            
            user_prompt = f"""ÏÉÅÌô©: {situation_ko}{topic_text}

ÏúÑ ÏÉÅÌô©Ïóê ÎßûÎäî {tone_ko} ÌÜ§Ïùò ÌÖúÌîåÎ¶øÏùÑ 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏúºÎ°ú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî."""

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.8,
                        "max_tokens": 1500
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                logger.info(f"üìù Groq response length: {len(content)} characters")
                
                json_result = self.analyzer._extract_json(content)
                
                if json_result and "suggestions" in json_result:
                    suggestions = []
                    for item in json_result["suggestions"]:
                        suggestions.append(SuggestionOption(
                            version=item.get("version", 1),
                            text=item.get("text", ""),
                            tone=item.get("tone", tone),
                            reasoning=item.get("reasoning", ""),
                            confidence=item.get("confidence", 0.85)
                        ))
                    
                    logger.info(f"Generated {len(suggestions)} template versions")
                    return suggestions
                else:
                    logger.warning("Failed to parse template response")
                    return self._fallback_template(situation, tone)
            else:
                logger.error(f"Template API error: {response.status_code}")
                return self._fallback_template(situation, tone)
                
        except Exception as e:
            logger.error(f"Template generation failed: {e}")
            return self._fallback_template(situation, tone)
    
    def _fallback_improvement(self, text: str, tone: str) -> List[SuggestionOption]:
        """ÌÖçÏä§Ìä∏ Í∞úÏÑ† Ìè¥Î∞±"""
        return [
            SuggestionOption(
                version=1,
                text=f"{text} (Îçî Í≥µÏÜêÌïú ÌëúÌòÑÏúºÎ°ú ÏàòÏ†ï ÌïÑÏöî)",
                tone=tone,
                reasoning="API Ïò§Î•òÎ°ú Ïù∏Ìïú Í∏∞Î≥∏ Ï†úÏïà",
                confidence=0.5
            ),
            SuggestionOption(
                version=2,
                text=f"{text} (Ï§ëÎ¶ΩÏ†Å ÌëúÌòÑÏúºÎ°ú ÏàòÏ†ï ÌïÑÏöî)",
                tone="neutral",
                reasoning="API Ïò§Î•òÎ°ú Ïù∏Ìïú Í∏∞Î≥∏ Ï†úÏïà",
                confidence=0.5
            ),
            SuggestionOption(
                version=3,
                text=f"{text} (ÏπúÍ∑ºÌïú ÌëúÌòÑÏúºÎ°ú ÏàòÏ†ï ÌïÑÏöî)",
                tone="friendly",
                reasoning="API Ïò§Î•òÎ°ú Ïù∏Ìïú Í∏∞Î≥∏ Ï†úÏïà",
                confidence=0.5
            )
        ]
    
    def _fallback_reply(self, comment: str, reply_type: str) -> List[SuggestionOption]:
        """ÎãµÎ≥Ä ÏÉùÏÑ± Ìè¥Î∞±"""
        return [
            SuggestionOption(
                version=1,
                text="ÏÜåÏ§ëÌïú ÏùòÍ≤¨ Í∞êÏÇ¨Ìï©ÎãàÎã§. Îçî ÎÇòÏùÄ ÏΩòÌÖêÏ∏†Î°ú Î≥¥ÎãµÌïòÍ≤†ÏäµÎãàÎã§.",
                tone="grateful",
                reasoning="Í∏∞Î≥∏ Í∞êÏÇ¨ ÎãµÎ≥Ä",
                confidence=0.6
            ),
            SuggestionOption(
                version=2,
                text="ÌîºÎìúÎ∞± Í∞êÏÇ¨ÎìúÎ¶ΩÎãàÎã§. Ïñ¥Îñ§ Î∂ÄÎ∂ÑÏùÑ Í∞úÏÑ†ÌïòÎ©¥ Ï¢ãÏùÑÏßÄ Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏïåÎ†§Ï£ºÏãúÎ©¥ ÌÅ∞ ÎèÑÏõÄÏù¥ Îê©ÎãàÎã§.",
                tone="constructive",
                reasoning="Í±¥ÏÑ§Ï†Å ÌîºÎìúÎ∞± ÏöîÏ≤≠",
                confidence=0.6
            ),
            SuggestionOption(
                version=3,
                text="ÏùòÍ≤¨ Ï£ºÏÖîÏÑú Í∞êÏÇ¨Ìï©ÎãàÎã§. ÏïûÏúºÎ°ú Îçî Ïã†Ï§ëÌûà ÏΩòÌÖêÏ∏†Î•º Ï†úÏûëÌïòÍ≤†ÏäµÎãàÎã§.",
                tone="apologetic",
                reasoning="ÏÇ¨Í≥ºÏôÄ Í∞úÏÑ† ÏùòÏßÄ",
                confidence=0.6
            )
        ]
    
    def _fallback_template(self, situation: str, tone: str) -> List[SuggestionOption]:
        """ÌÖúÌîåÎ¶ø ÏÉùÏÑ± Ìè¥Î∞±"""
        templates = {
            "promotion": "ÏÉàÎ°úÏö¥ ÏΩòÌÖêÏ∏†Î•º Ï§ÄÎπÑÌñàÏäµÎãàÎã§! ÎßéÏùÄ Í¥ÄÏã¨ Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§ üôè",
            "announcement": "ÏïàÎÖïÌïòÏÑ∏Ïöî! Ï§ëÏöîÌïú Í≥µÏßÄ ÏÇ¨Ìï≠ÏùÑ Ï†ÑÎã¨ÎìúÎ¶ΩÎãàÎã§.",
            "apology": "Î∂àÌé∏ÏùÑ ÎìúÎ†§ ÏßÑÏã¨ÏúºÎ°ú ÏÇ¨Í≥ºÎìúÎ¶ΩÎãàÎã§. Îçî ÎÇòÏùÄ Î™®ÏäµÏúºÎ°ú Ï∞æÏïÑÎµôÍ≤†ÏäµÎãàÎã§.",
            "feedback_request": "Ïó¨Îü¨Î∂ÑÏùò ÏÜåÏ§ëÌïú ÏùòÍ≤¨ÏùÑ Îì£Í≥† Ïã∂ÏäµÎãàÎã§. ÎåìÍ∏ÄÎ°ú ÏùòÍ≤¨ ÎÇ®Í≤®Ï£ºÏÑ∏Ïöî!"
        }
        
        base_text = templates.get(situation, "Í≤åÏãúÍ∏Ä ÎÇ¥Ïö©")
        
        return [
            SuggestionOption(
                version=1,
                text=base_text,
                tone=tone,
                reasoning="Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø",
                confidence=0.6
            ),
            SuggestionOption(
                version=2,
                text=f"{base_text} (ÏÉÅÏÑ∏ Î≤ÑÏ†Ñ)",
                tone=tone,
                reasoning="Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø ÌôïÏû•",
                confidence=0.6
            ),
            SuggestionOption(
                version=3,
                text=f"{base_text} (Í∞ÑÍ≤∞ Î≤ÑÏ†Ñ)",
                tone=tone,
                reasoning="Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø Ï∂ïÏïΩ",
                confidence=0.6
            )
        ]


# AI ÏÑúÎπÑÏä§ Ïù∏Ïä§ÌÑ¥Ïä§
analyzer = GroqDualModelAnalyzer()
writing_assistant = AIWritingAssistant(analyzer)


# ==================== Í∏∞Ï°¥ API ÏóîÎìúÌè¨Ïù∏Ìä∏ (Ïú†ÏßÄ) ====================

@app.on_event("startup")
async def startup_event():
    """ÏÑúÎ≤Ñ ÏãúÏûë"""
    logger.info("=" * 60)
    logger.info("SNS Content Analyzer - Groq Dual Model + AI Assistant")
    logger.info("=" * 60)
    
    if analyzer.api_key:
        logger.info("‚úì Groq API configured")
        logger.info(f"  - Guard Model: {analyzer.models['guard']}")
        logger.info(f"  - Analysis Model: {analyzer.models['analysis']}")
        logger.info("  - AI Assistant: Enabled")
    else:
        logger.warning("‚ö† No API key - fallback mode")


@app.get("/")
async def root():
    """API ÏÉÅÌÉú"""
    return {
        "service": "SNS Content Analyzer - Groq Dual Model + AI Assistant",
        "status": "running",
        "version": "3.1.0",
        "models": {
            "guard": analyzer.models["guard"],
            "analysis": analyzer.models["analysis"]
        },
        "features": [
            "Dual model analysis",
            "13 safety categories",
            "AI Writing Assistant",
            "Text improvement",
            "Reply generation",
            "Template creation"
        ],
        "endpoints": {
            "content_analysis": "/analyze/text",
            "ai_assistant_analyze": "/api/assistant/analyze",
            "ai_assistant_improve": "/api/assistant/improve",
            "ai_assistant_reply": "/api/assistant/reply",
            "ai_assistant_template": "/api/assistant/template"
        }
    }


@app.post("/analyze/text", response_model=AnalysisResponse)
async def analyze_text(request: TextAnalysisRequest):
    """ÌÖçÏä§Ìä∏ Î∂ÑÏÑù (ÎìÄÏñº Î™®Îç∏)"""
    logger.info(f"Analyzing text (length: {len(request.text)}, dual: {request.use_dual_model})")
    result = await analyzer.analyze_text(
        request.text, 
        request.language,
        request.use_dual_model,
        request.custom_blocked_words
    )
    return result


@app.post("/analyze/batch")
async def analyze_batch(
    texts: List[str], 
    language: str = "ko",
    use_dual_model: bool = True
):
    """ÎåÄÎüâ Î∂ÑÏÑù"""
    logger.info(f"Batch analysis: {len(texts)} texts")
    
    results = []
    for text in texts[:10]:
        try:
            result = await analyzer.analyze_text(text, language, use_dual_model)
            results.append(result)
        except Exception as e:
            logger.error(f"Failed: {e}")
            results.append(None)
    
    
    return {
        "total": len(results),
        "results": results,
        "dual_model": use_dual_model,
        "processed_at": datetime.now().isoformat()
    }


# ==================== YouTube Crawler ====================

class YoutubeCrawlRequest(BaseModel):
    url: str

@app.post("/crawl/youtube")
async def crawl_youtube(request: YoutubeCrawlRequest):
    """Ïú†ÌäúÎ∏å ÎåìÍ∏Ä ÏàòÏßë (youtube-comment-downloader ÏÇ¨Ïö©)"""
    logger.info(f"Crawling YouTube comments for: {request.url}")
    
    try:
        from youtube_comment_downloader import YoutubeCommentDownloader
        downloader = YoutubeCommentDownloader()
        
        comments = []
        # sort_by=1 (ÏµúÏã†Ïàú), limit=100 (ÏµúÎåÄ 100Í∞úÎßå ÏàòÏßëÌïòÏó¨ ÌÖåÏä§Ìä∏)
        generator = downloader.get_comments_from_url(request.url, sort_by=1)
        
        count = 0
        for comment in generator:
            # if count >= 500:
            #     break
                

            comments.append({
                "external_id": comment.get('cid', ''),
                "author": comment.get('author', 'Unknown'),
                "text": comment.get('text', ''),
                "publish_date": comment.get('time', ''),
                "author_id": comment.get('channel', ''),
                "like_count": comment.get('votes', 0)
            })
            count += 1
            
        logger.info(f"Crawled {len(comments)} comments")
        
        return {
            "status": "success",
            "video_url": request.url,
            "count": len(comments),
            "comments": comments
        }
        
    except Exception as e:
        logger.error(f"Crawling failed: {e}")
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}")


# ==================== üÜï AI Assistant ÏóîÎìúÌè¨Ïù∏Ìä∏ ====================

@app.post("/api/assistant/analyze", response_model=AssistantResponse)
async def assistant_analyze(request: AssistantAnalyzeRequest):
    """
    AI Assistant - ÏõêÎ≥∏ ÌÖçÏä§Ìä∏ Î∂ÑÏÑù
    
    Í∞êÏ†ï ÌÜ§, ÏúÑÌóòÎèÑ, Ïò§Ìï¥ Í∞ÄÎä•ÏÑ± Îì±ÏùÑ Îπ†Î•¥Í≤å Î∂ÑÏÑù
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant analyzing: {request.text[:50]}...")
        
        analysis = await writing_assistant.quick_analyze(
            request.text,
            request.language
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=analysis,
            suggestions=[],
            processing_time_ms=round(processing_time, 2),
            model_used="llama-guard-3-8b + llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant analyze failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/assistant/improve", response_model=AssistantResponse)
async def assistant_improve(request: AssistantImproveRequest):
    """
    AI Assistant - ÌÖçÏä§Ìä∏ Í∞úÏÑ†
    
    ÏõêÎ≥∏ ÌÖçÏä§Ìä∏Î•º ÏßÄÏ†ïÎêú ÌÜ§ÏúºÎ°ú Í∞úÏÑ†ÌïòÏó¨ 3Í∞ÄÏßÄ Î≤ÑÏ†Ñ Ï†úÏïà
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant improving text (tone: {request.tone})")
        
        # 1. Îπ†Î•∏ Î∂ÑÏÑù
        analysis = await writing_assistant.quick_analyze(
            request.text,
            request.language
        )
        
        # 2. ÌÖçÏä§Ìä∏ Í∞úÏÑ†
        suggestions = await writing_assistant.improve_text(
            request.text,
            request.tone,
            request.language,
            request.instruction
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=analysis,
            suggestions=suggestions,
            processing_time_ms=round(processing_time, 2),
            model_used="llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant improve failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/assistant/reply", response_model=AssistantResponse)
async def assistant_reply(request: AssistantReplyRequest):
    """
    AI Assistant - ÎåìÍ∏Ä ÎãµÎ≥Ä ÏÉùÏÑ±
    
    ÏõêÎ≥∏ ÎåìÍ∏ÄÏóê ÎåÄÌïú Ï†ÅÏ†àÌïú ÎãµÎ≥ÄÏùÑ 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏúºÎ°ú ÏÉùÏÑ±
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant generating reply (type: {request.reply_type})")
        
        # 1. ÎåìÍ∏Ä Î∂ÑÏÑù
        analysis = await writing_assistant.quick_analyze(
            request.original_comment,
            request.language
        )
        
        # 2. ÎãµÎ≥Ä ÏÉùÏÑ±
        suggestions = await writing_assistant.generate_reply(
            request.original_comment,
            request.context,
            request.reply_type,
            request.language
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=analysis,
            suggestions=suggestions,
            processing_time_ms=round(processing_time, 2),
            model_used="llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant reply failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/assistant/template", response_model=AssistantResponse)
async def assistant_template(request: AssistantTemplateRequest):
    """
    AI Assistant - ÏÉÅÌô©Î≥Ñ ÌÖúÌîåÎ¶ø ÏÉùÏÑ±
    
    ÌäπÏ†ï ÏÉÅÌô©(ÌôçÎ≥¥, Í≥µÏßÄ, ÏÇ¨Í≥º Îì±)Ïóê ÎßûÎäî ÌÖúÌîåÎ¶øÏùÑ 3Í∞ÄÏßÄ Î≤ÑÏ†ÑÏúºÎ°ú ÏÉùÏÑ±
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant generating template (situation: {request.situation})")
        
        # ÌÖúÌîåÎ¶ø ÏÉùÏÑ±
        suggestions = await writing_assistant.generate_template(
            request.situation,
            request.topic,
            request.tone,
            request.language
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=None,  # ÌÖúÌîåÎ¶ø ÏÉùÏÑ±ÏùÄ Î∂ÑÏÑù Î∂àÌïÑÏöî
            suggestions=suggestions,
            processing_time_ms=round(processing_time, 2),
            model_used="llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant template failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/models/info")
async def models_info():
    """Î™®Îç∏ Ï†ïÎ≥¥"""
    return {
        "guard_model": {
            "name": analyzer.models["guard"],
            "purpose": "Safety filtering",
            "categories": analyzer.guard_categories,
            "speed": "~100ms"
        },
        "analysis_model": {
            "name": analyzer.models["analysis"],
            "purpose": "Detailed analysis + AI Assistant",
            "features": [
                "Scoring", 
                "Reasoning", 
                "Text improvement",
                "Reply generation",
                "Template creation"
            ],
            "speed": "~200ms"
        },
        "assistant_features": {
            "tones": list(writing_assistant.tone_mapping.keys()),
            "situations": list(writing_assistant.situation_templates.keys()),
            "reply_types": ["constructive", "grateful", "apologetic", "defensive"]
        }
    }


@app.get("/health")
async def health_check():
    """Ìó¨Ïä§ Ï≤¥ÌÅ¨"""
    return {
        "status": "healthy",
        "api_configured": bool(analyzer.api_key),
        "models_ready": True,
        "ai_assistant_ready": True,
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("SNS Content Analyzer - Groq Dual Model + AI Assistant")
    print("=" * 60)
    print("\nüöÄ Models:")
    print(f"  1. {analyzer.models['guard']} - Safety filtering")
    print(f"  2. {analyzer.models['analysis']} - Analysis + AI Assistant")
    print("\n‚ú® AI Assistant Features:")
    print("  - Text improvement (3 versions)")
    print("  - Reply generation (3 versions)")
    print("  - Template creation (3 versions)")
    print("  - Quick emotion/risk analysis")
    print("\nüí∞ Cost: 100% FREE")
    print("  - Rate limit: 30 req/min")
    print("\nüîë Setup:")
    print("  export GROQ_API_KEY=your_key")
    print("  python main_groq_dual.py")
    print("\nÏÑúÎ≤Ñ ÏãúÏûë Ï§ë...\n")
    
    uvicorn.run(
        "main_groq_dual:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
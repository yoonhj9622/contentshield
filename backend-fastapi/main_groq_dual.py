"""
SNS Content Analyzer - Groq Dual Model Edition
Llama-Guard-4-12b (í•„í„°ë§) + Llama-3.1-8b-instant (ë¶„ì„)
+ AI Writing Assistant ê¸°ëŠ¥ ì¶”ê°€
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union
import logging
import os
from datetime import datetime
import httpx
import json
import re
import asyncio
from dotenv import load_dotenv  # âœ¨ ì¶”ê°€

# âœ¨ .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API í‚¤ ë¡œë“œ í™•ì¸
api_key_loaded = bool(os.getenv("GROQ_API_KEY"))
print(f"GROQ_API_KEY loaded: {api_key_loaded}")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="SNS Content Analyzer - Groq Dual Model + AI Assistant",
    description="Llama Guard 4 + Llama 3.1 ë“€ì–¼ ëª¨ë¸ ì•…ì„± ì½˜í…ì¸  íƒì§€ + AI ì‘ì„± ë³´ì¡°",
    version="3.1.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    # í˜„ì¬ (ëª¨ë“  ë„ë©”ì¸ í—ˆìš© - ê°œë°œìš©)
    allow_origins=["*"],
    # ë°°í¬ì „
    #allow_origins=["http://localhost:3000", "https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== ê¸°ì¡´ ë°ì´í„° ëª¨ë¸ ====================

class TextAnalysisRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=10000)
    language: str = Field(default="ko")
    use_dual_model: bool = Field(default=True, description="ë‘ ëª¨ë¸ ëª¨ë‘ ì‚¬ìš© ì—¬ë¶€")
    custom_blocked_words: List[str] = Field(default=[], description="ì‚¬ìš©ì ì •ì˜ ì°¨ë‹¨ ë‹¨ì–´")  # ì¶”ê°€!

class AnalysisResponse(BaseModel):
    is_malicious: bool
    is_blocked: bool = False  # ì¶”ê°€! ì‚¬ìš©ì ì°¨ë‹¨ ë‹¨ì–´ í¬í•¨ ì—¬ë¶€
    blocked_words_found: List[str] = []  # ì¶”ê°€! ë°œê²¬ëœ ì‚¬ìš©ì ì°¨ë‹¨ ë‹¨ì–´
    status: str = "clean"  # ì¶”ê°€! "clean", "malicious", "blocked"
    toxicity_score: float
    hate_speech_score: float
    profanity_score: float
    threat_score: float
    violence_score: float
    sexual_score: float
    confidence_score: float
    category: str
    detected_keywords: List[str]
    
    # Guard ëª¨ë¸ ê²°ê³¼
    guard_result: Optional[Dict[str, Any]] = None
    guard_categories: List[str] = []
    
    # Llama 3.1 ê²°ê³¼
    llama_reasoning: Optional[str] = None
    
    ai_model_version: str
    processing_time_ms: float
    analyzed_at: str


# ==================== ğŸ†• AI Assistant ë°ì´í„° ëª¨ë¸ ====================

class AssistantAnalyzeRequest(BaseModel):
    """ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­"""
    text: str = Field(..., min_length=1, max_length=10000)
    language: str = Field(default="ko")


class AssistantImproveRequest(BaseModel):
    """í…ìŠ¤íŠ¸ ê°œì„  ìš”ì²­"""
    text: str = Field(..., min_length=1, max_length=10000)
    tone: str = Field(default="polite", description="polite, neutral, friendly, formal, casual")
    language: str = Field(default="ko")
    instruction: Optional[str] = Field(default=None, description="ì¶”ê°€ ì§€ì‹œì‚¬í•­")


class AssistantReplyRequest(BaseModel):
    """ëŒ“ê¸€ ë‹µë³€ ìƒì„± ìš”ì²­"""
    original_comment: str = Field(..., min_length=1, max_length=1000)
    context: Optional[str] = Field(default=None, description="ì˜ìƒ/ê²Œì‹œê¸€ ë‚´ìš©")
    reply_type: str = Field(default="constructive", description="constructive, grateful, apologetic, defensive")
    language: str = Field(default="ko")


class AssistantTemplateRequest(BaseModel):
    """ìƒí™©ë³„ í…œí”Œë¦¿ ìƒì„± ìš”ì²­"""
    situation: str = Field(..., description="promotion, announcement, apology, explanation, feedback_request")
    topic: Optional[str] = Field(default=None, description="ì£¼ì œ/ìƒí™© ì„¤ëª…")
    tone: str = Field(default="professional")
    language: str = Field(default="ko")


class QuickAnalysis(BaseModel):
    """ê°„ë‹¨ ë¶„ì„ ê²°ê³¼"""
    emotion_tone: str  # "ê¸ì •ì ", "ì¤‘ë¦½ì ", "ë¶€ì •ì "
    risk_level: str    # "ì•ˆì „", "ì£¼ì˜", "ìœ„í—˜"
    has_profanity: bool
    has_aggression: bool
    misunderstanding_risk: str  # "ì—†ìŒ", "ë‚®ìŒ", "ìˆìŒ", "ë†’ìŒ"


class SuggestionOption(BaseModel):
    """AI ì œì•ˆ ì˜µì…˜"""
    version: Union[int, str]
    text: str
    tone: str
    reasoning: str
    confidence: float


class AssistantResponse(BaseModel):
    """AI Assistant í†µí•© ì‘ë‹µ"""
    success: bool
    analysis: Optional[QuickAnalysis] = None
    suggestions: List[SuggestionOption] = []
    processing_time_ms: float
    model_used: str


# ==================== ê¸°ì¡´ Groq Dual Model Analyzer (ìœ ì§€) ====================

class GroqDualModelAnalyzer:
    """Llama Guard 4 + Llama 3.1 ë“€ì–¼ ëª¨ë¸ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model_version = "groq-dual-llama-guard4-llama3.1"
        self.api_key = os.getenv("GROQ_API_KEY")
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"
        
        if not self.api_key:
            logger.warning("âš ï¸ GROQ_API_KEY not set")
            logger.warning("  Get FREE API key: https://console.groq.com/keys")
        else:
            logger.info("âœ“ Groq API key configured")
        
        # ëª¨ë¸ ì„¤ì •
        self.models = {
            "guard": "llama-guard-3-8b",
            "analysis": "llama-3.1-8b-instant"
        }
        
        # Guard ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        self.guard_categories = {
            "S1": "violent_crimes",
            "S2": "non_violent_crimes",
            "S3": "sex_related_crimes",
            "S4": "child_exploitation",
            "S5": "defamation",
            "S6": "specialized_advice",
            "S7": "privacy",
            "S8": "intellectual_property",
            "S9": "indiscriminate_weapons",
            "S10": "hate",
            "S11": "self_harm",
            "S12": "sexual_content",
            "S13": "elections"
        }
        
        # ê·œì¹™ ê¸°ë°˜ ì°¨ë‹¨ ë‹¨ì–´
        self.blocked_words = {
            "ko": [
                "ë°”ë³´", "ë©ì²­ì´", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì”¨ë°œ", "ì§€ë„", "ë¯¸ì¹œ",
                "ì£½ì—¬", "ì£½ì¼", "ë•Œë ¤", "í˜ì˜¤", "ì°¨ë³„", "êº¼ì ¸", "ë‹¥ì³","ê°œìì‹","ì–‘ì•„ì¹˜"
            ],
            "en": [
                "stupid", "idiot", "fuck", "shit", "kill", "hate", "damn"
            ]
        }
        
        logger.info("Groq Dual Model Analyzer initialized")
        logger.info(f"  - Guard Model: {self.models['guard']}")
        logger.info(f"  - Analysis Model: {self.models['analysis']}")
    
    async def analyze_text(
        self, 
        text: str, 
        language: str = "ko",
        use_dual_model: bool = True,
        custom_blocked_words: List[str] = None  # ì¶”ê°€!
    ) -> AnalysisResponse:
        """í…ìŠ¤íŠ¸ ë¶„ì„ (ë“€ì–¼ ëª¨ë¸)"""
        import time
        start_time = time.time()
        
        try:
            # 1. ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ (ì‚¬ìš©ì ì°¨ë‹¨ ë‹¨ì–´ í¬í•¨)
            rule_result = self._rule_based_filter(text, language, custom_blocked_words or [])
            
            if not self.api_key:
                logger.warning("No API key, using fallback")
                result = self._create_fallback_response(text, rule_result)
            elif use_dual_model:
                # 2. ë“€ì–¼ ëª¨ë¸ ë¶„ì„
                result = await self._dual_model_analysis(text, language, rule_result)
            else:
                # 3. ë‹¨ì¼ ëª¨ë¸ ë¶„ì„
                result = await self._single_model_analysis(text, language, rule_result)
            
            # ì‚¬ìš©ì ì°¨ë‹¨ ë‹¨ì–´ ì²˜ë¦¬ ì¶”ê°€
            result["is_blocked"] = rule_result.get("is_blocked_by_user", False)
            result["blocked_words_found"] = rule_result.get("user_blocked_words_found", [])
            
            # status ê²°ì •
            if result["is_blocked"]:
                result["status"] = "blocked"
            elif result["is_malicious"]:
                result["status"] = "malicious"
            else:
                result["status"] = "clean"
            
            processing_time = (time.time() - start_time) * 1000
            result["processing_time_ms"] = round(processing_time, 2)
            result["analyzed_at"] = datetime.now().isoformat()
            result["ai_model_version"] = self.model_version
            
            return AnalysisResponse(**result)
        
        except Exception as e:
            logger.error(f"Analysis error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")
    
    async def _dual_model_analysis(
        self, 
        text: str, 
        language: str,
        rule_result: Dict
    ) -> Dict[str, Any]:
        """ë“€ì–¼ ëª¨ë¸ ë¶„ì„ (Guard + Llama 3.1 ë³‘ë ¬ ì‹¤í–‰)"""
        try:
            guard_task = self._llama_guard_check(text, language)
            llama_task = self._llama_analysis(text, language)
            
            guard_result, llama_result = await asyncio.gather(
                guard_task,
                llama_task,
                return_exceptions=True
            )
            
            if isinstance(guard_result, Exception):
                logger.error(f"Guard model failed: {guard_result}")
                guard_result = self._fallback_guard_result()
            
            if isinstance(llama_result, Exception):
                logger.error(f"Llama model failed: {llama_result}")
                llama_result = self._fallback_analysis(text)
            
            combined = self._combine_dual_results(
                rule_result,
                guard_result,
                llama_result
            )
            
            return combined
            
        except Exception as e:
            logger.error(f"Dual model analysis failed: {e}")
            return self._create_fallback_response(text, rule_result)
    
    async def _llama_guard_check(self, text: str, language: str) -> Dict[str, Any]:
        """Llama Guard 4 ì•ˆì „ì„± ê²€ì‚¬"""
        try:
            prompt = f"""[INST] Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
S1: Violent Crimes
S2: Non-Violent Crimes
S3: Sex-Related Crimes
S4: Child Sexual Exploitation
S5: Defamation
S6: Specialized Advice
S7: Privacy
S8: Intellectual Property
S9: Indiscriminate Weapons
S10: Hate
S11: Self-Harm
S12: Sexual Content
S13: Elections
<END UNSAFE CONTENT CATEGORIES>

<BEGIN CONVERSATION>
User: {text}
<END CONVERSATION>

Provide your safety assessment for User's message:
- First line must be 'safe' or 'unsafe'
- If unsafe, list all violated categories (S1, S2, etc.) [/INST]"""

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.models["guard"],
                        "messages": [
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": 0.0,
                        "max_tokens": 100
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"].strip()
                
                is_safe = content.lower().startswith("safe")
                violated_categories = []
                
                if not is_safe:
                    categories = re.findall(r'S\d+', content)
                    violated_categories = [
                        self.guard_categories.get(cat, cat) 
                        for cat in categories
                    ]
                
                logger.info(f"Guard result: {'safe' if is_safe else 'unsafe'}, categories: {violated_categories}")
                
                return {
                    "is_safe": is_safe,
                    "violated_categories": violated_categories,
                    "raw_response": content,
                    "guard_success": True
                }
            else:
                logger.error(f"Guard API error: {response.status_code}")
                return self._fallback_guard_result()
                
        except Exception as e:
            logger.error(f"Guard check failed: {e}")
            return self._fallback_guard_result()
    
    async def _llama_analysis(self, text: str, language: str) -> Dict[str, Any]:
        """Llama 3.1 ìƒì„¸ ë¶„ì„"""
        try:
            # ì–¸ì–´ ê°ì§€ (íŒŒë¼ë¯¸í„°ê°€ ì—†ê±°ë‚˜ ë¶ˆí™•ì‹¤í•  ë•Œ)
            lang = language
            if not lang or lang not in ("ko", "en"):
                if re.search(r"[ê°€-í£]", text):
                    lang = "ko"
                else:
                    lang = "en"
            
            if lang == "ko":
                system_prompt = """You are an expert in analyzing toxic and harmful content.
Analyze the given text and provide detailed scores (0-100) for each category.

Respond in valid JSON format only, no markdown:
{
  "toxicity_score": <0-100>,
  "hate_speech_score": <0-100>,
  "profanity_score": <0-100>,
  "threat_score": <0-100>,
  "violence_score": <0-100>,
  "sexual_score": <0-100>,
  "reasoning": "<brief explanation in KOREAN>"
}"""
                user_prompt = f'ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ìœ í•´ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: "{text}"'
            else:
                system_prompt = """You are an expert in analyzing toxic and harmful content.
Analyze the given text and provide detailed scores (0-100) for each category.

Respond in valid JSON format only, no markdown:
{
  "toxicity_score": <0-100>,
  "hate_speech_score": <0-100>,
  "profanity_score": <0-100>,
  "threat_score": <0-100>,
  "violence_score": <0-100>,
  "sexual_score": <0-100>,
  "reasoning": "<brief explanation in English>"
}"""
                user_prompt = f'Analyze this text for harmful content: "{text}"'
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.models["analysis"],
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.1,
                        "max_tokens": 300
                    }
                )
            
            if response.status_code == 200:
                # [Rate Limit Logging]
                remaining_tokens = response.headers.get("x-ratelimit-remaining-tokens", "unknown")
                remaining_requests = response.headers.get("x-ratelimit-remaining-requests", "unknown")
                logger.info(f"âš¡ Groq Rate Limit Info: Remaining Tokens={remaining_tokens}, Requests={remaining_requests}")

                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                json_result = self._extract_json(content)
                
                if json_result:
                    logger.info(f"Llama analysis: toxicity={json_result.get('toxicity_score', 0)}")
                    
                    return {
                        "toxicity_score": json_result.get("toxicity_score", 0),
                        "hate_speech_score": json_result.get("hate_speech_score", 0),
                        "profanity_score": json_result.get("profanity_score", 0),
                        "threat_score": json_result.get("threat_score", 0),
                        "violence_score": json_result.get("violence_score", 0),
                        "sexual_score": json_result.get("sexual_score", 0),
                        "reasoning": json_result.get("reasoning", ""),
                        "llama_success": True
                    }
                else:
                    logger.warning("Failed to parse Llama response")
                    return self._fallback_analysis(text)
            else:
                logger.error(f"Llama API error: {response.status_code}")
                return self._fallback_analysis(text)
                
        except Exception as e:
            logger.error(f"Llama analysis failed: {e}")
            return self._fallback_analysis(text)
    
    async def _single_model_analysis(
        self,
        text: str,
        language: str,
        rule_result: Dict
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ë¶„ì„ (Llama 3.1ë§Œ ì‚¬ìš©)"""
        llama_result = await self._llama_analysis(text, language)
        return self._combine_results(rule_result, llama_result)
    
    def _rule_based_filter(self, text: str, language: str, custom_blocked_words: List[str] = None) -> Dict[str, Any]:
        """ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ (ì‚¬ìš©ì ì°¨ë‹¨ ë‹¨ì–´ í¬í•¨)"""
        detected = []
        user_blocked_found = []
        score = 0.0
        
        # ê¸°ë³¸ ì°¨ë‹¨ ë‹¨ì–´ ì²´í¬
        words = self.blocked_words.get(language, [])
        text_lower = text.lower()
        
        for word in words:
            if word in text_lower:
                detected.append(word)
                score += 25.0
        
        # ì‚¬ìš©ì ì •ì˜ ì°¨ë‹¨ ë‹¨ì–´ ì²´í¬
        if custom_blocked_words:
            for word in custom_blocked_words:
                if word.lower() in text_lower:
                    user_blocked_found.append(word)
                    # ì‚¬ìš©ì ì°¨ë‹¨ ë‹¨ì–´ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬ (ì ìˆ˜ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
        
        return {
            "detected_keywords": detected,
            "rule_score": min(score, 100.0),
            "is_malicious_rule": score > 50.0,
            "is_blocked_by_user": len(user_blocked_found) > 0,  # ì¶”ê°€!
            "user_blocked_words_found": user_blocked_found  # ì¶”ê°€!
        }
    
    def _combine_dual_results(
        self,
        rule_result: Dict,
        guard_result: Dict,
        llama_result: Dict
    ) -> Dict[str, Any]:
        """ë“€ì–¼ ëª¨ë¸ ê²°ê³¼ í†µí•©"""
        
        guard_boost = 0
        if not guard_result.get("is_safe", True):
            guard_boost = 30
        
        weight_rule = 0.15
        weight_guard = 0.35
        weight_llama = 0.50
        
        toxicity = (
            rule_result["rule_score"] * weight_rule +
            guard_boost * weight_guard +
            llama_result.get("toxicity_score", 0) * weight_llama
        )
        
        hate_speech = llama_result.get("hate_speech_score", 0)
        profanity = llama_result.get("profanity_score", 0)
        threat = llama_result.get("threat_score", 0)
        violence = llama_result.get("violence_score", 0)
        sexual = llama_result.get("sexual_score", 0)
        
        violated_cats = guard_result.get("violated_categories", [])
        if "hate" in violated_cats:
            hate_speech = max(hate_speech, 80)
        if "violent_crimes" in violated_cats:
            violence = max(violence, 85)
        if "sexual_content" in violated_cats:
            sexual = max(sexual, 85)
        
        is_malicious = (
            toxicity > 0 or  # STRICT POLICY: Any score > 0 is malicious
            hate_speech > 60.0 or
            profanity > 70.0 or
            threat > 40.0 or
            violence > 60.0 or
            sexual > 70.0 or
            not guard_result.get("is_safe", True) or
            rule_result["is_malicious_rule"]
        )
        
        if violence > 70:
            category = "violence"
        elif sexual > 70:
            category = "sexual_content"
        elif threat > 60:
            category = "threat"
        elif hate_speech > 60:
            category = "hate_speech"
        elif profanity > 70:
            category = "profanity"
        elif toxicity > 70:
            category = "highly_toxic"
        elif toxicity > 0:  # STRICT POLICY
            category = "moderately_toxic"
        else:
            category = "safe"
        
        confidence = 95.0 if guard_result.get("guard_success") and llama_result.get("llama_success") else 70.0
        
        return {
            "is_malicious": is_malicious,
            "toxicity_score": round(toxicity, 2),
            "hate_speech_score": round(hate_speech, 2),
            "profanity_score": round(profanity, 2),
            "threat_score": round(threat, 2),
            "violence_score": round(violence, 2),
            "sexual_score": round(sexual, 2),
            "confidence_score": round(confidence, 2),
            "category": category,
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": {
                "is_safe": guard_result.get("is_safe", True),
                "violated_categories": violated_cats
            },
            "guard_categories": violated_cats,
            "llama_reasoning": llama_result.get("reasoning", "")
        }
    
    def _combine_results(self, rule_result: Dict, llama_result: Dict) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ ê²°ê³¼ í†µí•© (Llama 3.1ë§Œ)"""
        weight_rule = 0.3
        weight_llama = 0.7
        
        toxicity = (
            rule_result["rule_score"] * weight_rule +
            llama_result.get("toxicity_score", 0) * weight_llama
        )
        
        return {
            "is_malicious": toxicity > 50 or rule_result["is_malicious_rule"],
            "toxicity_score": round(toxicity, 2),
            "hate_speech_score": round(llama_result.get("hate_speech_score", 0), 2),
            "profanity_score": round(llama_result.get("profanity_score", 0), 2),
            "threat_score": round(llama_result.get("threat_score", 0), 2),
            "violence_score": round(llama_result.get("violence_score", 0), 2),
            "sexual_score": round(llama_result.get("sexual_score", 0), 2),
            "confidence_score": 85.0,
            "category": "toxic" if toxicity > 50 else "safe",
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": llama_result.get("reasoning", "")
        }
    
    def _fallback_guard_result(self) -> Dict[str, Any]:
        """Guard í´ë°±"""
        return {
            "is_safe": True,
            "violated_categories": [],
            "raw_response": "Guard unavailable",
            "guard_success": False
        }
    
    def _fallback_analysis(self, text: str) -> Dict[str, Any]:
        """Llama í´ë°±"""
        # API ì˜¤ë¥˜ ì‹œ 'ì•ˆì „'ìœ¼ë¡œ ì²˜ë¦¬ (ê¸¸ì´ ê¸°ë°˜ íŒì • ì œê±°)
        return {
            "toxicity_score": 0,
            "hate_speech_score": 0,
            "profanity_score": 0,
            "threat_score": 0,
            "violence_score": 0,
            "sexual_score": 0,
            "reasoning": "Fallback analysis (API Unavailable) - Assumed Safe",
            "llama_success": False
        }
    
    def _create_fallback_response(self, text: str, rule_result: Dict) -> Dict[str, Any]:
        """ì™„ì „ í´ë°±"""
        score = rule_result["rule_score"]
        
        return {
            "is_malicious": rule_result["is_malicious_rule"],
            "toxicity_score": score,
            "hate_speech_score": max(0, score - 20),
            "profanity_score": max(0, score - 10),
            "threat_score": max(0, score - 30),
            "violence_score": max(0, score - 25),
            "sexual_score": max(0, score - 35),
            "confidence_score": 40.0,
            "category": "toxic" if score > 50 else "safe",
            "detected_keywords": rule_result["detected_keywords"],
            "guard_result": None,
            "guard_categories": [],
            "llama_reasoning": "Fallback: Rule-based only"
        }
    
    def _extract_json(self, text: str) -> Optional[Dict]:
        """JSON ì¶”ì¶œ (Groq ì‘ë‹µ ì „ìš©)"""
        try:
            # 1. Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'\s*```', '', text)
            text = text.strip()

            # 2. ì§ì ‘ íŒŒì‹± ì‹œë„
            try:
                result = json.loads(text)
                logger.info(f"âœ… JSON ì§ì ‘ íŒŒì‹± ì„±ê³µ: {len(result.get('suggestions', []))}ê°œ ì œì•ˆ")
                return result
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ ì§ì ‘ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
            # 3. ê°€ì¥ í° JSON ê°ì²´ ì°¾ê¸° (ì¤‘ì²© êµ¬ì¡° ì§€ì›)
            max_json = None
            max_length = 0
        
            # ëª¨ë“  { ìœ„ì¹˜ ì°¾ê¸°
            for i in range(len(text)):
                if text[i] == '{':
                    # ì´ ìœ„ì¹˜ì—ì„œ ì‹œì‘í•˜ëŠ” ì™„ì „í•œ JSON ì°¾ê¸°
                    depth = 0
                    for j in range(i, len(text)):
                        if text[j] == '{':
                            depth += 1
                        elif text[j] == '}':
                            depth -= 1
                            if depth == 0:
                                # ì™„ì „í•œ JSON ë°œê²¬
                                json_str = text[i:j+1]
                                try:
                                    parsed = json.loads(json_str)
                                    if len(json_str) > max_length:
                                        max_json = parsed
                                        max_length = len(json_str)
                                except:
                                    pass
                                break
        
            if max_json:
                logger.info(f"âœ… ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ íŒŒì‹± ì„±ê³µ: {len(max_json.get('suggestions', []))}ê°œ ì œì•ˆ")
                return max_json
        
            logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨ - ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì‹œì‘ ë¶€ë¶„: {text[:500]}")
            return None
        
        except Exception as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì˜ˆì™¸: {e}")
            return None

#ì†Œì˜ë‹˜
# ==================== ğŸ†• AI Writing Assistant Service ====================

class AIWritingAssistant:
    """AI ì‘ì„± ë³´ì¡° ì„œë¹„ìŠ¤ (Llama 3.1 ê¸°ë°˜)"""
    
    def __init__(self, analyzer: GroqDualModelAnalyzer):
        self.analyzer = analyzer
        self.api_key = analyzer.api_key
        self.base_url = analyzer.base_url
        self.model = analyzer.models["analysis"]  # Llama-3.1-8b-instant
        
        # í†¤ ì•¤ ë§¤ë„ˆ í•œêµ­ì–´ ë§¤í•‘
        self.tone_mapping = {
            "polite": "ê³µì†í•˜ê³  ì •ì¤‘í•œ",
            "neutral": "ì¤‘ë¦½ì ì´ê³  ê°ê´€ì ì¸",
            "friendly": "ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ",
            "formal": "ê²©ì‹ìˆê³  ì „ë¬¸ì ì¸",
            "casual": "í¸ì•ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´"
        }

        # ìƒí™©ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.situation_templates = {
            "promotion": "í™ë³´/ë§ˆì¼€íŒ… ê²Œì‹œê¸€",
            "announcement": "íŒ¬ ê³µì§€/ì•ˆë‚´ ë©”ì‹œì§€",
            "apology": "ì‚¬ê³¼ ë° í•´ëª…",
            "explanation": "ìƒí™© ì„¤ëª…",
            "feedback_request": "ê±´ì„¤ì  í”¼ë“œë°± ìš”ì²­"
        }

        # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€: ì˜ì–´ ì¶œë ¥ ì§€ì›ì„ ìœ„í•œ EN ë§¤í•‘/ì–¸ì–´ê°ì§€ ìœ í‹¸ ì¶”ê°€
        self.tone_mapping_en = {
            "polite": "polite and respectful",
            "neutral": "neutral and objective",
            "friendly": "friendly and warm",
            "formal": "formal and professional",
            "casual": "casual and natural"
        }

        self.situation_templates_en = {
            "promotion": "a promotional/marketing post",
            "announcement": "an announcement / community notice",
            "apology": "an apology / clarification",
            "explanation": "an explanation",
            "feedback_request": "a constructive feedback request"
        }

        def _detect_language_simple(text: str) -> str:
            if not text:
                return "en"
            if re.search(r"[ê°€-í£]", text):
                return "ko"
            return "en"

        self._detect_language_simple = _detect_language_simple
        # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€
        
        logger.info("AI Writing Assistant initialized")

    # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€: language íŒŒë¼ë¯¸í„°ê°€ ko/enì´ ì•„ë‹ˆê±°ë‚˜ ëˆ„ë½ë˜ë©´ ì…ë ¥ í…ìŠ¤íŠ¸ë¡œ ìë™ ê°ì§€
    def _resolve_language(self, text: str, language: str = "ko") -> str:
        lang = (language or "").strip().lower()
        if lang not in ("ko", "en"):
            lang = self._detect_language_simple(text)
        return lang
    # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€
    
    async def quick_analyze(
        self, 
        text: str, 
        language: str = "ko"
    ) -> QuickAnalysis:
        """ë¹ ë¥¸ ê°ì •/ìœ„í—˜ë„ ë¶„ì„"""
        try:
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€: quick_analyzeë„ ì…ë ¥ ê¸°ë°˜ ì–¸ì–´ ìë™ ë³´ì •
            language = self._resolve_language(text, language)
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€

            # ê¸°ì¡´ analyzer í™œìš© (Guard + Llama 3.1)
            analysis_result = await self.analyzer.analyze_text(text, language, use_dual_model=True)
            
            # ê°ì • í†¤ íŒë³„
            if analysis_result.toxicity_score > 60:
                emotion_tone = "ë¶€ì •ì " if language == "ko" else "Negative"
            elif analysis_result.toxicity_score < 30:
                emotion_tone = "ê¸ì •ì " if language == "ko" else "Positive"
            else:
                emotion_tone = "ì¤‘ë¦½ì " if language == "ko" else "Neutral"
            
            # ìœ„í—˜ë„ íŒë³„
            if analysis_result.is_malicious or analysis_result.toxicity_score > 70:
                risk_level = "ìœ„í—˜" if language == "ko" else "High"
            elif analysis_result.toxicity_score > 40:
                risk_level = "ì£¼ì˜" if language == "ko" else "Medium"
            else:
                risk_level = "ì•ˆì „" if language == "ko" else "Low"
            
            # ì˜¤í•´ ê°€ëŠ¥ì„± íŒë³„
            if analysis_result.toxicity_score > 50:
                misunderstanding_risk = "ë†’ìŒ" if language == "ko" else "High"
            elif analysis_result.toxicity_score > 30:
                misunderstanding_risk = "ìˆìŒ" if language == "ko" else "Some"
            elif analysis_result.toxicity_score > 15:
                misunderstanding_risk = "ë‚®ìŒ" if language == "ko" else "Low"
            else:
                misunderstanding_risk = "ì—†ìŒ" if language == "ko" else "None"
            
            return QuickAnalysis(
                emotion_tone=emotion_tone,
                risk_level=risk_level,
                has_profanity=analysis_result.profanity_score > 60,
                has_aggression=analysis_result.threat_score > 50 or analysis_result.violence_score > 50,
                misunderstanding_risk=misunderstanding_risk
            )
            
        except Exception as e:
            logger.error(f"Quick analysis failed: {e}")
            # í´ë°±
            return QuickAnalysis(
                emotion_tone="ì¤‘ë¦½ì ",
                risk_level="ì•ˆì „",
                has_profanity=False,
                has_aggression=False,
                misunderstanding_risk="ì—†ìŒ"
            )
    
    async def improve_text(
        self,
        text: str,
        tone: str = "polite",
        language: str = "ko",
        instruction: Optional[str] = None
    ) -> List[SuggestionOption]:
        """í…ìŠ¤íŠ¸ ê°œì„  (3ê°€ì§€ ë²„ì „ ìƒì„±)"""
        try:
            logger.info(f"ğŸ”„ Starting text improvement: text='{text[:30]}...', tone={tone}")

            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€: ì…ë ¥ ê¸°ë°˜ ì–¸ì–´ ìë™ ê°ì§€ + í”„ë¡¬í”„íŠ¸ë¥¼ ko/enë¡œ ë¶„ê¸°
            language = self._resolve_language(text, language)

            if language == "ko":
                tone_label = self.tone_mapping.get(tone, "ê³µì†í•˜ê³  ì •ì¤‘í•œ")

                system_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì½˜í…ì¸  ì—ë””í„°ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ í…ìŠ¤íŠ¸ë¥¼ {tone_label} í†¤ìœ¼ë¡œ ê°œì„ í•˜ì—¬ 3ê°€ì§€ ë‹¤ë¥¸ ë²„ì „ì„ ì œì•ˆí•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ì›ë¬¸ì˜ í•µì‹¬ ì˜ë¯¸ëŠ” ìœ ì§€
2. ì˜¤í•´ì˜ ì†Œì§€ê°€ ì—†ë„ë¡ ëª…í™•í•˜ê²Œ í‘œí˜„
3. ìš•ì„¤, ê³µê²©ì  í‘œí˜„ ì œê±°
4. 3ê°€ì§€ ë²„ì „ì€ ê°ê° ë‹¤ë¥¸ ê°•ë„/ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±
5. ìœ íŠœë¸Œ ëŒ“ê¸€/ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ì— ì í•©í•œ ê¸¸ì´ (2-5ì¤„)

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "ê°œì„ ëœ í…ìŠ¤íŠ¸ ë²„ì „ 1 (ê°€ì¥ ê³µì†í•¨)",
      "tone": "ë§¤ìš° ê³µì†",
      "reasoning": "ê°œì„  ì´ìœ  ì„¤ëª…",
      "confidence": 0.95
    }},
    {{
      "version": 2,
      "text": "ê°œì„ ëœ í…ìŠ¤íŠ¸ ë²„ì „ 2 (ì¤‘ê°„)",
      "tone": "ì¤‘ë¦½ì ",
      "reasoning": "ê°œì„  ì´ìœ  ì„¤ëª…",
      "confidence": 0.90
    }},
    {{
      "version": 3,
      "text": "ê°œì„ ëœ í…ìŠ¤íŠ¸ ë²„ì „ 3 (ì¹œê·¼í•¨)",
      "tone": "ì¹œê·¼í•¨",
      "reasoning": "ê°œì„  ì´ìœ  ì„¤ëª…",
      "confidence": 0.88
    }}
  ]
}}"""

                user_prompt = f"""ì›ë³¸ í…ìŠ¤íŠ¸: "{text}"
{'ì¶”ê°€ ì§€ì‹œì‚¬í•­: ' + instruction if instruction else ''}

ìœ„ í…ìŠ¤íŠ¸ë¥¼ {tone_label} í†¤ìœ¼ë¡œ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”."""
            else:
                tone_label = self.tone_mapping_en.get(tone, "polite and respectful")

                system_prompt = f"""You are a professional content editor.
Rewrite the user's text in a {tone_label} tone and propose 3 distinct versions.

Requirements:
1. Keep the original meaning
2. Make it clear and reduce misunderstanding
3. Remove profanity, insults, and aggressive wording
4. Each version should differ in style/intensity
5. Suitable length for YouTube comments/community posts (2-5 lines)

Return JSON ONLY (no markdown, no extra text):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "Improved version 1 (most polite)",
      "tone": "Very polite",
      "reasoning": "Why this is better",
      "confidence": 0.95
    }},
    {{
      "version": 2,
      "text": "Improved version 2 (neutral)",
      "tone": "Neutral",
      "reasoning": "Why this is better",
      "confidence": 0.90
    }},
    {{
      "version": 3,
      "text": "Improved version 3 (friendly)",
      "tone": "Friendly",
      "reasoning": "Why this is better",
      "confidence": 0.88
    }}
  ]
}}"""

                user_prompt = f"""Original text: "{text}"
{('Additional instruction: ' + instruction) if instruction else ''}

Please rewrite the text into 3 versions in a {tone_label} tone."""
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€

            async with httpx.AsyncClient(timeout=30.0) as client:
                logger.info(f"ğŸ“¤ Sending request to Groq API...")
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 1500
                    }
                )
                logger.info(f"ğŸ“¥ Groq API response: status={response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                logger.info(f"ğŸ“ Groq response length: {len(content)} characters")
                
                json_result = self.analyzer._extract_json(content)
                
                if json_result and "suggestions" in json_result:
                    suggestions = []
                    for item in json_result["suggestions"]:
                        # í…ìŠ¤íŠ¸ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ (í†¤ ì •ë³´ ì œê±°)
                        text_val = item.get("text", "")
                        
                        # ë²„ì „ì— í†¤ ì •ë³´ ì¶”ê°€ (ì‚¬ìš©ì ìš”ì²­)
                        tone_val = item.get("tone", "")
                        version_val = item.get("version", 1)
                        if tone_val:
                            version_val = f"{version_val} ({tone_val})"

                        suggestions.append(SuggestionOption(
                            version=version_val,
                            text=text_val,
                            tone=item.get("tone", tone),
                            reasoning=item.get("reasoning", ""),
                            confidence=item.get("confidence", 0.85)
                        ))
                    logger.info(f"Generated {len(suggestions)} improved versions")
                    return suggestions
                else:
                    logger.warning("Failed to parse improvement response")
                    return self._fallback_improvement(text, tone)
            else:
                logger.error(f"Improvement API error: {response.status_code}")
                return self._fallback_improvement(text, tone)
                
        except Exception as e:
            logger.error(f"Text improvement failed: {e}")
            return self._fallback_improvement(text, tone)
    
    async def generate_reply(
        self,
        original_comment: str,
        context: Optional[str] = None,
        reply_type: str = "constructive",
        language: str = "ko"
    ) -> List[SuggestionOption]:
        """ëŒ“ê¸€ ë‹µë³€ ìƒì„± (3ê°€ì§€ ë²„ì „)"""
        try:
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€: ì…ë ¥ ê¸°ë°˜ ì–¸ì–´ ìë™ ê°ì§€ + í”„ë¡¬í”„íŠ¸ ko/en ë¶„ê¸°
            language = self._resolve_language(original_comment, language)

            if language == "ko":
                reply_types_ko = {
                    "constructive": "ê±´ì„¤ì ì´ê³  ë°œì „ì ì¸",
                    "grateful": "ê°ì‚¬í•˜ê³  ê²¸ì†í•œ",
                    "apologetic": "ì‚¬ê³¼í•˜ê³  í•´ëª…í•˜ëŠ”",
                    "defensive": "ë°©ì–´ì ì´ì§€ë§Œ ì˜ˆì˜ìˆëŠ”"
                }
                reply_tone = reply_types_ko.get(reply_type, "ê±´ì„¤ì ì´ê³  ë°œì „ì ì¸")

                system_prompt = f"""ë‹¹ì‹ ì€ ìœ íŠœë¸Œ í¬ë¦¬ì—ì´í„°ì˜ ì»¤ë®¤ë‹ˆí‹° ë§¤ë‹ˆì €ì…ë‹ˆë‹¤.
ì•…ì„± ëŒ“ê¸€ì´ë‚˜ ë¹„íŒì  ëŒ“ê¸€ì— ëŒ€í•´ {reply_tone} ë‹µë³€ì„ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.

ì›ì¹™:
1. ì ˆëŒ€ ìš•ì„¤ì´ë‚˜ ê³µê²©ì  í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
2. íŒ¬ë“¤ê³¼ì˜ ê´€ê³„ ìœ ì§€ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ
3. ë²•ì  ë¦¬ìŠ¤í¬ê°€ ìˆëŠ” í‘œí˜„ íšŒí”¼
4. ë¸Œëœë“œ ì´ë¯¸ì§€ ë³´í˜¸
5. ê° ë²„ì „ì€ ë‹¤ë¥¸ ê°•ë„/ì ‘ê·¼ë²• ì‚¬ìš©

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "ë‹µë³€ ë²„ì „ 1 (ê°€ì¥ ê³µì†í•˜ê³  ê²¸ì†)",
      "tone": "ë§¤ìš° ê³µì†",
      "reasoning": "ì´ ë‹µë³€ì„ ì„ íƒí•œ ì´ìœ ",
      "confidence": 0.92
    }},
    {{
      "version": 2,
      "text": "ë‹µë³€ ë²„ì „ 2 (ì¤‘ë¦½ì )",
      "tone": "ì¤‘ë¦½ì ",
      "reasoning": "ì´ ë‹µë³€ì„ ì„ íƒí•œ ì´ìœ ",
      "confidence": 0.88
    }},
    {{
      "version": 3,
      "text": "ë‹µë³€ ë²„ì „ 3 (ë‹¨í˜¸í•˜ì§€ë§Œ ì˜ˆì˜ìˆìŒ)",
      "tone": "ë‹¨í˜¸í•˜ì§€ë§Œ ì˜ˆì˜ìˆìŒ",
      "reasoning": "ì´ ë‹µë³€ì„ ì„ íƒí•œ ì´ìœ ",
      "confidence": 0.85
    }}
  ]
}}"""

                context_text = f"\nì˜ìƒ/ê²Œì‹œê¸€ ë‚´ìš©: {context}" if context else ""
                user_prompt = f"""ì›ë³¸ ëŒ“ê¸€: "{original_comment}"{context_text}

ìœ„ ëŒ“ê¸€ì— ëŒ€í•œ {reply_tone} ë‹µë³€ì„ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."""
            else:
                reply_types_en = {
                    "constructive": "constructive and solution-oriented",
                    "grateful": "grateful and humble",
                    "apologetic": "apologetic and clarifying",
                    "defensive": "firm but polite"
                }
                reply_tone = reply_types_en.get(reply_type, "constructive and solution-oriented")

                system_prompt = f"""You are a community manager for a YouTube creator.
Generate 3 reply options to a critical/negative comment in a {reply_tone} style.

Rules:
1. Never use profanity or insults
2. Prioritize maintaining a healthy relationship with viewers
3. Avoid legal-risky statements
4. Protect brand image
5. Each version should differ in approach/intensity

Return JSON ONLY (no markdown, no extra text):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "Reply version 1 (most polite)",
      "tone": "Very polite",
      "reasoning": "Why this reply works",
      "confidence": 0.92
    }},
    {{
      "version": 2,
      "text": "Reply version 2 (neutral)",
      "tone": "Neutral",
      "reasoning": "Why this reply works",
      "confidence": 0.88
    }},
    {{
      "version": 3,
      "text": "Reply version 3 (firm but polite)",
      "tone": "Firm but polite",
      "reasoning": "Why this reply works",
      "confidence": 0.85
    }}
  ]
}}"""

                context_text = f"\nContext (video/post): {context}" if context else ""
                user_prompt = f"""Original comment: "{original_comment}"{context_text}

Please generate 3 reply options in a {reply_tone} style."""
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 1500
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                logger.info(f"ğŸ“ Groq response length: {len(content)} characters")
                
                json_result = self.analyzer._extract_json(content)
                
                if json_result and "suggestions" in json_result:
                    suggestions = []
                    for item in json_result["suggestions"]:
                        tone_val = item.get("tone", reply_type)
                        version_val = item.get("version", 1)
                        if tone_val:
                            version_val = f"{version_val} ({tone_val})"

                        suggestions.append(SuggestionOption(
                            version=version_val,
                            text=item.get("text", ""),
                            tone=tone_val,
                            reasoning=item.get("reasoning", ""),
                            confidence=item.get("confidence", 0.85)
                        ))
                    
                    logger.info(f"Generated {len(suggestions)} reply versions")
                    return suggestions
                else:
                    logger.warning("Failed to parse reply response")
                    return self._fallback_reply(original_comment, reply_type)
            else:
                logger.error(f"Reply API error: {response.status_code}")
                return self._fallback_reply(original_comment, reply_type)
                
        except Exception as e:
            logger.error(f"Reply generation failed: {e}")
            return self._fallback_reply(original_comment, reply_type)
    
    async def generate_template(
        self,
        situation: str,
        topic: Optional[str] = None,
        tone: str = "professional",
        language: str = "ko"
    ) -> List[SuggestionOption]:
        """ìƒí™©ë³„ í…œí”Œë¦¿ ìƒì„± (3ê°€ì§€ ë²„ì „)"""
        try:
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€: topic/ìƒí™© í…ìŠ¤íŠ¸ë¡œë„ ì–¸ì–´ ê°ì§€(í•œêµ­ì–´ ì£¼ì œë©´ í•œêµ­ì–´ë¡œ)
            sample_text = (topic or "") + " " + (situation or "")
            language = self._resolve_language(sample_text, language)

            if language == "ko":
                situation_ko = self.situation_templates.get(situation, "ì¼ë°˜ ê²Œì‹œê¸€")
                tone_ko = self.tone_mapping.get(tone, "ì „ë¬¸ì ì¸")
                
                system_prompt = f"""ë‹¹ì‹ ì€ ì†Œì…œ ë¯¸ë””ì–´ ì½˜í…ì¸  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
"{situation_ko}" ìƒí™©ì— ë§ëŠ” ê²Œì‹œê¸€/ëŒ“ê¸€ í…œí”Œë¦¿ì„ {tone_ko} í†¤ìœ¼ë¡œ 3ê°€ì§€ ë²„ì „ ìƒì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ìœ íŠœë¸Œ ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ ë˜ëŠ” ëŒ“ê¸€ë¡œ ì í•©
2. 3-7ì¤„ ê¸¸ì´ (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ)
3. ì´ëª¨ì§€ ì‚¬ìš© ê°€ëŠ¥ (ì ì ˆíˆ)
4. ê° ë²„ì „ì€ ë‹¤ë¥¸ ì ‘ê·¼ë²•/ê¸¸ì´ ì‚¬ìš©
5. ë²•ì  ë¦¬ìŠ¤í¬ ì—†ëŠ” ì•ˆì „í•œ í‘œí˜„

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "í…œí”Œë¦¿ ë²„ì „ 1 (ê°„ê²°í•˜ê³  í•µì‹¬ì )",
      "tone": "ê°„ê²°",
      "reasoning": "ì´ í…œí”Œë¦¿ì˜ íŠ¹ì§•",
      "confidence": 0.90
    }},
    {{
      "version": 2,
      "text": "í…œí”Œë¦¿ ë²„ì „ 2 (ì¤‘ê°„ ê¸¸ì´, ê°ì • í‘œí˜„)",
      "tone": "ê°ì •ì ",
      "reasoning": "ì´ í…œí”Œë¦¿ì˜ íŠ¹ì§•",
      "confidence": 0.88
    }},
    {{
      "version": 3,
      "text": "í…œí”Œë¦¿ ë²„ì „ 3 (ìƒì„¸í•˜ê³  ì „ë¬¸ì )",
      "tone": "ì „ë¬¸ì ",
      "reasoning": "ì´ í…œí”Œë¦¿ì˜ íŠ¹ì§•",
      "confidence": 0.85
    }}
  ]
}}"""

                topic_text = f"\nì£¼ì œ/ìƒí™©: {topic}" if topic else ""
                user_prompt = f"""ìƒí™©: {situation_ko}{topic_text}

ìœ„ ìƒí™©ì— ë§ëŠ” {tone_ko} í†¤ì˜ í…œí”Œë¦¿ì„ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”."""
            else:
                situation_en = self.situation_templates_en.get(situation, "a general post")
                tone_en = self.tone_mapping_en.get(tone, "professional")

                system_prompt = f"""You are a social media content specialist.
Create 3 template versions for {situation_en} in a {tone_en} tone.

Requirements:
1. Suitable for YouTube community posts or comments
2. Length: 3-7 lines
3. Emojis allowed (use appropriately)
4. Each version should differ in approach/length
5. Safe wording with low legal risk

Return JSON ONLY (no markdown, no extra text):
{{
  "suggestions": [
    {{
      "version": 1,
      "text": "Template version 1 (concise)",
      "tone": "Concise",
      "reasoning": "Why this works",
      "confidence": 0.90
    }},
    {{
      "version": 2,
      "text": "Template version 2 (balanced)",
      "tone": "Balanced",
      "reasoning": "Why this works",
      "confidence": 0.88
    }},
    {{
      "version": 3,
      "text": "Template version 3 (detailed)",
      "tone": "Detailed",
      "reasoning": "Why this works",
      "confidence": 0.85
    }}
  ]
}}"""

                topic_text = f"\nTopic/context: {topic}" if topic else ""
                user_prompt = f"""Situation: {situation_en}{topic_text}

Please generate 3 template versions in a {tone_en} tone."""
            # ì¥ì†Œì˜~ì—¬ê¸°ê¹Œì§€
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    self.base_url,
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "temperature": 0.8,
                        "max_tokens": 1500
                    }
                )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                logger.info(f"ğŸ“ Groq response length: {len(content)} characters")
                
                json_result = self.analyzer._extract_json(content)
                
                if json_result and "suggestions" in json_result:
                    suggestions = []
                    for item in json_result["suggestions"]:
                        tone_val = item.get("tone", tone)
                        version_val = item.get("version", 1)
                        if tone_val:
                            version_val = f"{version_val} ({tone_val})"

                        suggestions.append(SuggestionOption(
                            version=version_val,
                            text=item.get("text", ""),
                            tone=tone_val,
                            reasoning=item.get("reasoning", ""),
                            confidence=item.get("confidence", 0.85)
                        ))
                    
                    logger.info(f"Generated {len(suggestions)} template versions")
                    return suggestions
                else:
                    logger.warning("Failed to parse template response")
                    return self._fallback_template(situation, tone)
            else:
                logger.error(f"Template API error: {response.status_code}")
                return self._fallback_template(situation, tone)
                
        except Exception as e:
            logger.error(f"Template generation failed: {e}")
            return self._fallback_template(situation, tone)

    
    def _fallback_improvement(self, text: str, tone: str) -> List[SuggestionOption]:
        """í…ìŠ¤íŠ¸ ê°œì„  í´ë°±"""
        return [
            SuggestionOption(
                version=1,
                text=f"{text} (ë” ê³µì†í•œ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)",
                tone=tone,
                reasoning="API ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì œì•ˆ",
                confidence=0.5
            ),
            SuggestionOption(
                version=2,
                text=f"{text} (ì¤‘ë¦½ì  í‘œí˜„ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)",
                tone="neutral",
                reasoning="API ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì œì•ˆ",
                confidence=0.5
            ),
            SuggestionOption(
                version=3,
                text=f"{text} (ì¹œê·¼í•œ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì • í•„ìš”)",
                tone="friendly",
                reasoning="API ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì œì•ˆ",
                confidence=0.5
            )
        ]
    
    def _fallback_reply(self, comment: str, reply_type: str) -> List[SuggestionOption]:
        """ë‹µë³€ ìƒì„± í´ë°±"""
        return [
            SuggestionOption(
                version=1,
                text="ì†Œì¤‘í•œ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤. ë” ë‚˜ì€ ì½˜í…ì¸ ë¡œ ë³´ë‹µí•˜ê² ìŠµë‹ˆë‹¤.",
                tone="grateful",
                reasoning="ê¸°ë³¸ ê°ì‚¬ ë‹µë³€",
                confidence=0.6
            ),
            SuggestionOption(
                version=2,
                text="í”¼ë“œë°± ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ì–´ë–¤ ë¶€ë¶„ì„ ê°œì„ í•˜ë©´ ì¢‹ì„ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ í° ë„ì›€ì´ ë©ë‹ˆë‹¤.",
                tone="constructive",
                reasoning="ê±´ì„¤ì  í”¼ë“œë°± ìš”ì²­",
                confidence=0.6
            ),
            SuggestionOption(
                version=3,
                text="ì˜ê²¬ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì•ìœ¼ë¡œ ë” ì‹ ì¤‘íˆ ì½˜í…ì¸ ë¥¼ ì œì‘í•˜ê² ìŠµë‹ˆë‹¤.",
                tone="apologetic",
                reasoning="ì‚¬ê³¼ì™€ ê°œì„  ì˜ì§€",
                confidence=0.6
            )
        ]
    
    def _fallback_template(self, situation: str, tone: str) -> List[SuggestionOption]:
        """í…œí”Œë¦¿ ìƒì„± í´ë°±"""
        templates = {
            "promotion": "ìƒˆë¡œìš´ ì½˜í…ì¸ ë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤! ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤ ğŸ™",
            "announcement": "ì•ˆë…•í•˜ì„¸ìš”! ì¤‘ìš”í•œ ê³µì§€ ì‚¬í•­ì„ ì „ë‹¬ë“œë¦½ë‹ˆë‹¤.",
            "apology": "ë¶ˆí¸ì„ ë“œë ¤ ì§„ì‹¬ìœ¼ë¡œ ì‚¬ê³¼ë“œë¦½ë‹ˆë‹¤. ë” ë‚˜ì€ ëª¨ìŠµìœ¼ë¡œ ì°¾ì•„ëµ™ê² ìŠµë‹ˆë‹¤.",
            "feedback_request": "ì—¬ëŸ¬ë¶„ì˜ ì†Œì¤‘í•œ ì˜ê²¬ì„ ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤. ëŒ“ê¸€ë¡œ ì˜ê²¬ ë‚¨ê²¨ì£¼ì„¸ìš”!"
        }
        
        base_text = templates.get(situation, "ê²Œì‹œê¸€ ë‚´ìš©")
        
        return [
            SuggestionOption(
                version=1,
                text=base_text,
                tone=tone,
                reasoning="ê¸°ë³¸ í…œí”Œë¦¿",
                confidence=0.6
            ),
            SuggestionOption(
                version=2,
                text=f"{base_text} (ìƒì„¸ ë²„ì „)",
                tone=tone,
                reasoning="ê¸°ë³¸ í…œí”Œë¦¿ í™•ì¥",
                confidence=0.6
            ),
            SuggestionOption(
                version=3,
                text=f"{base_text} (ê°„ê²° ë²„ì „)",
                tone=tone,
                reasoning="ê¸°ë³¸ í…œí”Œë¦¿ ì¶•ì•½",
                confidence=0.6
            )
        ]


# AI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
analyzer = GroqDualModelAnalyzer()
writing_assistant = AIWritingAssistant(analyzer)


# ==================== ê¸°ì¡´ API ì—”ë“œí¬ì¸íŠ¸ (ìœ ì§€) ====================

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘"""
    logger.info("=" * 60)
    logger.info("SNS Content Analyzer - Groq Dual Model + AI Assistant")
    logger.info("=" * 60)
    
    if analyzer.api_key:
        logger.info("âœ“ Groq API configured")
        logger.info(f"  - Guard Model: {analyzer.models['guard']}")
        logger.info(f"  - Analysis Model: {analyzer.models['analysis']}")
        logger.info("  - AI Assistant: Enabled")
    else:
        logger.warning("âš  No API key - fallback mode")


@app.get("/")
async def root():
    """API ìƒíƒœ"""
    return {
        "service": "SNS Content Analyzer - Groq Dual Model + AI Assistant",
        "status": "running",
        "version": "3.1.0",
        "models": {
            "guard": analyzer.models["guard"],
            "analysis": analyzer.models["analysis"]
        },
        "features": [
            "Dual model analysis",
            "13 safety categories",
            "AI Writing Assistant",
            "Text improvement",
            "Reply generation",
            "Template creation"
        ],
        "endpoints": {
            "content_analysis": "/analyze/text",
            "ai_assistant_analyze": "/api/assistant/analyze",
            "ai_assistant_improve": "/api/assistant/improve",
            "ai_assistant_reply": "/api/assistant/reply",
            "ai_assistant_template": "/api/assistant/template"
        }
    }


@app.post("/analyze/text", response_model=AnalysisResponse)
async def analyze_text(request: TextAnalysisRequest):
    """í…ìŠ¤íŠ¸ ë¶„ì„ (ë“€ì–¼ ëª¨ë¸)"""
    logger.info(f"Analyzing text (length: {len(request.text)}, dual: {request.use_dual_model})")
    result = await analyzer.analyze_text(
        request.text, 
        request.language,
        request.use_dual_model,
        request.custom_blocked_words
    )
    return result


@app.post("/analyze/batch")
async def analyze_batch(
    texts: List[str], 
    language: str = "ko",
    use_dual_model: bool = True
):
    """ëŒ€ëŸ‰ ë¶„ì„"""
    logger.info(f"Batch analysis: {len(texts)} texts")
    
    results = []
    for text in texts[:10]:
        try:
            result = await analyzer.analyze_text(text, language, use_dual_model)
            results.append(result)
        except Exception as e:
            logger.error(f"Failed: {e}")
            results.append(None)
    
    
    return {
        "total": len(results),
        "results": results,
        "dual_model": use_dual_model,
        "processed_at": datetime.now().isoformat()
    }


# ==================== YouTube Crawler ====================

class YoutubeCrawlRequest(BaseModel):
    url: str

@app.post("/crawl/youtube")
async def crawl_youtube(request: YoutubeCrawlRequest):
    """ìœ íŠœë¸Œ ëŒ“ê¸€ ìˆ˜ì§‘ (youtube-comment-downloader ì‚¬ìš©)"""
    logger.info(f"Crawling YouTube comments for: {request.url}")
    
    try:
        from youtube_comment_downloader import YoutubeCommentDownloader
        downloader = YoutubeCommentDownloader()
        
        comments = []
        # sort_by=1 (ìµœì‹ ìˆœ), limit=100 (ìµœëŒ€ 100ê°œë§Œ ìˆ˜ì§‘í•˜ì—¬ í…ŒìŠ¤íŠ¸)
        generator = downloader.get_comments_from_url(request.url, sort_by=1)
        
        count = 0
        for comment in generator:
            # if count >= 500:
            #     break
                

            comments.append({
                "external_id": comment.get('cid', ''),
                "author": comment.get('author', 'Unknown'),
                "text": comment.get('text', ''),
                "publish_date": comment.get('time', ''),
                "author_id": comment.get('channel', ''),
                "like_count": comment.get('votes', 0)
            })
            count += 1
            
        logger.info(f"Crawled {len(comments)} comments")
        
        return {
            "status": "success",
            "video_url": request.url,
            "count": len(comments),
            "comments": comments
        }
        
    except Exception as e:
        logger.error(f"Crawling failed: {e}")
        raise HTTPException(status_code=500, detail=f"Crawling failed: {str(e)}")


# ==================== ğŸ†• AI Assistant ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/api/assistant/analyze", response_model=AssistantResponse)
async def assistant_analyze(request: AssistantAnalyzeRequest):
    """
    AI Assistant - ì›ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„
    
    ê°ì • í†¤, ìœ„í—˜ë„, ì˜¤í•´ ê°€ëŠ¥ì„± ë“±ì„ ë¹ ë¥´ê²Œ ë¶„ì„
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant analyzing: {request.text[:50]}...")
        
        analysis = await writing_assistant.quick_analyze(
            request.text,
            request.language
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=analysis,
            suggestions=[],
            processing_time_ms=round(processing_time, 2),
            model_used="llama-guard-3-8b + llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant analyze failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/assistant/improve", response_model=AssistantResponse)
async def assistant_improve(request: AssistantImproveRequest):
    """
    AI Assistant - í…ìŠ¤íŠ¸ ê°œì„ 
    
    ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í†¤ìœ¼ë¡œ ê°œì„ í•˜ì—¬ 3ê°€ì§€ ë²„ì „ ì œì•ˆ
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant improving text (tone: {request.tone})")
        
        # 1. ë¹ ë¥¸ ë¶„ì„
        analysis = await writing_assistant.quick_analyze(
            request.text,
            request.language
        )
        
        # 2. í…ìŠ¤íŠ¸ ê°œì„ 
        suggestions = await writing_assistant.improve_text(
            request.text,
            request.tone,
            request.language,
            request.instruction
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=analysis,
            suggestions=suggestions,
            processing_time_ms=round(processing_time, 2),
            model_used="llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant improve failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/assistant/reply", response_model=AssistantResponse)
async def assistant_reply(request: AssistantReplyRequest):
    """
    AI Assistant - ëŒ“ê¸€ ë‹µë³€ ìƒì„±
    
    ì›ë³¸ ëŒ“ê¸€ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ìƒì„±
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant generating reply (type: {request.reply_type})")
        
        # 1. ëŒ“ê¸€ ë¶„ì„
        analysis = await writing_assistant.quick_analyze(
            request.original_comment,
            request.language
        )
        
        # 2. ë‹µë³€ ìƒì„±
        suggestions = await writing_assistant.generate_reply(
            request.original_comment,
            request.context,
            request.reply_type,
            request.language
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=analysis,
            suggestions=suggestions,
            processing_time_ms=round(processing_time, 2),
            model_used="llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant reply failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/assistant/template", response_model=AssistantResponse)
async def assistant_template(request: AssistantTemplateRequest):
    """
    AI Assistant - ìƒí™©ë³„ í…œí”Œë¦¿ ìƒì„±
    
    íŠ¹ì • ìƒí™©(í™ë³´, ê³µì§€, ì‚¬ê³¼ ë“±)ì— ë§ëŠ” í…œí”Œë¦¿ì„ 3ê°€ì§€ ë²„ì „ìœ¼ë¡œ ìƒì„±
    """
    import time
    start_time = time.time()
    
    try:
        logger.info(f"Assistant generating template (situation: {request.situation})")
        
        # í…œí”Œë¦¿ ìƒì„±
        suggestions = await writing_assistant.generate_template(
            request.situation,
            request.topic,
            request.tone,
            request.language
        )
        
        processing_time = (time.time() - start_time) * 1000
        
        return AssistantResponse(
            success=True,
            analysis=None,  # í…œí”Œë¦¿ ìƒì„±ì€ ë¶„ì„ ë¶ˆí•„ìš”
            suggestions=suggestions,
            processing_time_ms=round(processing_time, 2),
            model_used="llama-3.1-8b-instant"
        )
        
    except Exception as e:
        logger.error(f"Assistant template failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/models/info")
async def models_info():
    """ëª¨ë¸ ì •ë³´"""
    return {
        "guard_model": {
            "name": analyzer.models["guard"],
            "purpose": "Safety filtering",
            "categories": analyzer.guard_categories,
            "speed": "~100ms"
        },
        "analysis_model": {
            "name": analyzer.models["analysis"],
            "purpose": "Detailed analysis + AI Assistant",
            "features": [
                "Scoring", 
                "Reasoning", 
                "Text improvement",
                "Reply generation",
                "Template creation"
            ],
            "speed": "~200ms"
        },
        "assistant_features": {
            "tones": list(writing_assistant.tone_mapping.keys()),
            "situations": list(writing_assistant.situation_templates.keys()),
            "reply_types": ["constructive", "grateful", "apologetic", "defensive"]
        }
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "api_configured": bool(analyzer.api_key),
        "models_ready": True,
        "ai_assistant_ready": True,
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("SNS Content Analyzer - Groq Dual Model + AI Assistant")
    print("=" * 60)
    print("\nğŸš€ Models:")
    print(f"  1. {analyzer.models['guard']} - Safety filtering")
    print(f"  2. {analyzer.models['analysis']} - Analysis + AI Assistant")
    print("\nâœ¨ AI Assistant Features:")
    print("  - Text improvement (3 versions)")
    print("  - Reply generation (3 versions)")
    print("  - Template creation (3 versions)")
    print("  - Quick emotion/risk analysis")
    print("\nğŸ’° Cost: 100% FREE")
    print("  - Rate limit: 30 req/min")
    print("\nğŸ”‘ Setup:")
    print("  export GROQ_API_KEY=your_key")
    print("  python main_groq_dual.py")
    print("\nì„œë²„ ì‹œì‘ ì¤‘...\n")
    
    uvicorn.run(
        "main_groq_dual:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )